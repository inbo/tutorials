[{"content":"Introduction Welcome to the tutorials website of the Research Institute for Nature and Forest (INBO). It contains a collection of guides, tutorials and further reading material on the installation, use and development of (research) software at our institute.\nGetting started When this is your first time checking this website, welcome! You are always free to look around a bit, but if you are looking for something specific, the website provides 3 options to search for relevant topics:\n Categories: A (limited) set of general categories to classify the tutorials. For example, installation for support during installation of software or r for tutorials about the R programming language. Click on a category to get the relevant tutorials. Tags: Check the cloud of tags listed on each page of the website and pick an item that would be relevant for your topic. By clicking on a specific tag, you will get an overview of the tutorials with this tag Search: Type any word in the search form and you will get a dropdown list of the pages which contain the word you are looking for.  Website menu Apart from the navigation options explained in the previous section, pages are divided into 3 main section accessible with the menu:\n Tutorials: The main bulk of pages, providing tutorials on a variety of topics. Articles: Provides useful background information about topics, links to literature, etc. Installation: Installation instructions for both system administrators and users  The absence of a more granular menu is a deliberate decision to overcome the requirement to recreate a website menu when new tutorials are written. By using categories and tags, more flexibility is provided. The tags list, categories overview and search options are automatically updated when a new tutorial is added to the website.\nWant to contribute? Great news! By sharing information, we can effectively help each other. Moreover, a tutorial is useful for your future self as well ;-). Anything that could be useful to your colleagues? Instead of keeping a note or document on your own computer or drive, make it a tutorial and share it. Notice that also links/references to other resources are useful to add, as this will enable others to find these reources.\nCheck the create tutorial page for more information on how to contribute to this website!\nQuestions? If you have any questions, do not hesitate to contact the website maintainers (main contributors listed here), provide a new issue if you already have a Github account or just ask the IT helpdesk for more information.\n","href":"/tutorials/","title":"Home"},{"content":"At the Research Institute for Nature and Forest (INBO), we are eager to sustain, promote and develop open-source software that is relevant to biodiversity researchers! This page lists R and Python packages which INBO developed or made a significant contribution to. Several of these packages continue being developed.\nPlease, feel invited to try out packages! If you encounter a problem or if you have a suggestion, we encourage you to post an issue on the package’s code repository. You can also directly contribute improvements with a pull request.\nThe package hyperlinks below refer to the package’s documentation website, if available. When there is no documentation website, often one or more vignettes are available within the package, describing the package’s purpose and demonstrating its use.\nThe following table gives a quick overview:\n   Research stage Related INBO packages     Study design grts   Retrieve data: general inbodb   Retrieve data: environmental wateRinfo, pydov, watina   Retrieve data: biological pyinaturalist, uvabits, etn, n2khab, forrescalc   Store data git2rdata   Validate data pywhip   Analyze data: graphs inboggvegan   Analyze data: models inlatools, multimput, effectclass, niche_vlaanderen, gwloggeR   Analyze data: indices LSVI   Publish INBOmd, INBOtheme   Miscellaneous (mixed content) inborutils    Study design  R package grts: draw a sample from a sampling frame with the Generalized Random Tessellation Stratified (GRTS) sampling strategy.  Retrieve data General  R package [inbodb]((https://inbo.github.io/inbodb): connect to and retrieve data from databases on the INBO server, with dedicated functions to query some of these databases.  Environmental data  R package wateRinfo: facilitates access to waterinfo.be, a website managed by the Flanders Environment Agency (VMM) and Flanders Hydraulics Research. The website provides access to real-time water and weather related environmental variables for Flanders (Belgium), such as rainfall, air pressure, discharge, and water level. The package provides functions to search for stations and variables, and download time series. Python package pydov: to query and download data from Databank Ondergrond Vlaanderen (DOV). DOV aggregates data about soil, subsoil and groundwater of Flanders and makes them publicly available. Interactive and human-readable extraction and querying of the data is provided by a web application, whereas the focus of this package is to support machine-based extraction and conversion of the data. R package watina: provides functions to query and process data from the Watina database (mainly groundwater data).  Biological data  Python package pyinaturalist: Python client for the iNaturalist APIs. R package uvabits: provides an R interface to the UvA-BiTS database, which stores bird movement data collected with UvA-BiTS GPS trackers. The package provides functionality to download data and metadata, calculate some metrics, and load the data into a query-optimized SQLite database for local analysis. It also allows to download the data in a format that can easily be uploaded to Movebank, a free online database for animal tracking data. R package etn: provides functionality to access and process data from the European Tracking Network (ETN) database hosted by the Flanders Marine Institute (VLIZ) as part of the Flemish contribution to LifeWatch. R package n2khab: provides preprocessed reference data (including checklists, spatial habitat distribution, administrative \u0026amp; environmental layers, GRTSmaster_habitats) and preprocessing functions, supporting reproducible and transparent analyses on Flemish Natura 2000 (n2k) habitats (hab) and regionally important biotopes (RIBs). R package forrescalc: provides aggregated values on dendrometry, regeneration and vegetation of the Flemish forest reserve monitoring network, and functions to derive these data starting from individual tree measurements in Fieldmap.  Store data  R package git2rdata: an R package for writing and reading dataframes as plain text files. Important information is stored in a metadata file, which allows to maintain the classes of variables. git2rdata is ideal for storing R dataframes as plain text files under version control, as it strives to minimize row based diffs between two consecutive commits. The package is intended to facilitate a reproducible and traceable workflow.  Validate data  Python package pywhip: a package to validate data against whip specifications, a human and machine-readable syntax to express specifications for data.  Analyze data Make graphs  R package inboggvegan: provides R functions for multivariate plots. More specifically, extended biplot and screeplot functionality is offered for the vegan package.  Fit models and make model predictions  R package inlatools: provides a set of functions which can be useful to diagnose INLA models: calculating Pearson residuals, simulation based checks for over- or underdispersion, simulation based checks for the distribution, visualising the effect of the variance or precision on random effects (random intercept, first order random walk, second order random walk). The functions can be useful to choose sensible priors and diagnose the fitted model. R package multimput: an R package that assists with analysing datasets with missing values using multiple imputation. R package effectclass: an R package to classify and visualize modelled effects by comparing their confidence interval with thresholds. Python package niche_vlaanderen: Python package to run the NICHE Vlaanderen model. Based on calculated abiotic properties of the location, NICHE Vlaanderen determines whether certain vegetation types can develop. An additional flooding module allows the user to test whether the predicted vegetations are compatible with a particular flooding regime. The package is a redevelopment of an existing ArcGIS plugin in Python, without external non-open source dependencies. R package gwloggeR: an R package to detect anomalous observations in timeseries of groundwater loggerdata (water pressure and air pressure). Additive outliers, temporal changes and level shifts are detected.  Calculate indices  R package LSVI: bundles a number of functions to support researchers in determining the local conservation status (‘LSVI’) of Natura 2000 habitats in Flanders. Several functions retrieve the criteria and/or associated species lists for determining the LSVI. A specific function allows to calculate the LSVI. The package is written in Dutch.  Publish your workflow and discuss your results  R package INBOmd: provides several styles for rmarkdown files and several templates to generate reports, presentations and posters. The styles are based on the corporate identity of INBO and the Flemish government. All templates are based on bookdown, which is an extension of rmarkdown. bookdown is taylored towards writing books and technical documentation. R package INBOtheme: contains ggplot2 themes for INBO, the Flemish government and Elsevier journals. The documentation website includes a set of example figures for each available theme.  Last but not least: miscellaneous!  R package inborutils: provides a collection of useful R utilities and snippets that we consider recyclable for multiple projects. The functions are either out of scope or just not mature enough to include as extensions to existing packages.  ","href":"/tutorials/articles/inbo_software/","title":"Software by INBO: packages for environmentalists and ecologists!"},{"content":"  McElreath (2015): Statistical Rethinking is an introduction to applied Bayesian data analysis, aimed at PhD students and researchers in the natural and social sciences. This audience has had some calculus and linear algebra, and one or two joyless undergraduate courses in statistics. I've been teaching applied statistics to this audience for about a decade now, and this book has evolved from that experience. The book teaches generalized linear multilevel modeling (GLMMs) from a Bayesian perspective, relying on a simple logical interpretation of Bayesian probability and maximum entropy. The book covers the basics of regression through multilevel models, as well as touching on measurement error, missing data, and Gaussian process models for spatial and network autocorrelation. This is not a traditional mathematical statistics book. Instead the approach is computational, using complete R code examples, aimed at developing skilled and skeptical scientists. Theory is explained through simulation exercises, using R code. And modeling examples are fully worked, with R code displayed within the main text. Mathematical depth is given in optional {\u0026quot;}overthinking{\u0026quot;} boxes throughout.\n  Kass et al. (2016): The authors propose a set of 10 simple rules for effective statistical practice\n  Quinn \u0026amp; Keough (2002): An essential textbook for any student or researcher in biology needing to design experiments, sample programs or analyse the resulting data. The text begins with a revision of estimation and hypothesis testing methods, covering both classical and Bayesian philosophies, before advancing to the analysis of linear and generalized linear models. Topics covered include linear and logistic regression, simple and complex ANOVA models (for factorial, nested, block, split-plot and repeated measures and covariance designs), and log-linear models. Multivariate techniques, including classification and ordination, are then introduced. Special emphasis is placed on checking assumptions, exploratory data analysis and presentation of results. The main analyses are illustrated with many examples from published papers and there is an extensive reference list to both the statistical and biological literature. The book is supported by a website that provides all data sets, questions for each chapter and links to software.\n  James et al. (2013): An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform. Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.\n  Emden (2008): The typical biology student is “hardwired” to be wary of any tasks involving the application of mathematics and statistical analyses, but the plain fact is much of biology requires interpretation of experimental data through the use of statistical methods. This unique textbook aims to demystify statistical formulae for the average biology student. Written in a lively and engaging style, Statistics for Terrified Biologists draws on the author's 30 years of lecturing experience. One of the foremost entomologists of his generation, van Emden has an extensive track record for successfully teaching statistical methods to even the most guarded of biology students. For the first time basic methods are presented using straightforward, jargon-free language. Students are taught to use simple formulae accurately to interpret what is being measured with each test and statistic, while at the same time learning to recognize overall patterns and guiding principles. Complemented by simple illustrations and useful case studies, this is an ideal statistics resource tool for undergraduate biology and environmental science students who lack confidence in their mathematical abilities.\n  Agresti (2002): The use of statistical methods for categorical data has increased dramatically, particularly for applications in the biomedical and social sciences. Responding to new developments in the field as well as to the needs of a new generation of professionals and students, this new edition of the classic Categorical Data Analysis offers a comprehensive introduction to the most important methods for categorical data analysis. Designed for statisticians and biostatisticians as well as scientists and graduate students practicing statistics, Categorical Data Analysis, Second Edition summarizes the latest methods for univariate and correlated multivariate categorical responses. Readers will find a unified generalized linear models approach that connects logistic regression and Poisson and negative binomial regression for discrete data with normal regression for continuous data.\n  van Belle (2008): This book contains chapters titled:\n Begin with a Basic Formula for Sample Size–Lehr's Equation Calculating Sample Size Using the Coefficient of Variation Ignore the Finite Population Correction in Calculating Sample Size for a Survey The Range of the Observations Provides Bounds for the Standard Deviation * Do not Formulate a Study Solely in Terms of Effect Size Overlapping Confidence Intervals do not Imply Nonsignificance Sample Size Calculation for the Poisson Distribution Sample Size Calculation for Poisson Distribution with Background Rate Sample Size Calculation for the Binomial Distribution When Unequal Sample Sizes Matter; When They Don't * Determining Sample Size when there are Different Costs Associated with the Two Samples Use the Rule of Threes for 95% Upper Bounds when there Have Been No Events Sample Size Calculations Should be Based on the Way the Data will be Analyzed    Grolemund \u0026amp; Wickham (2016): This is the website for {\u0026quot;}R for Data Science{\u0026quot;}. This book will teach you how to do data science with R: You'll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you'll learn how to clean data and draw plots—and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You'll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You'll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.\n  Baddeley et al. (2015): Spatial Point Patterns: Methodology and Applications with R shows scientific researchers and applied statisticians from a wide range of fields how to analyze their spatial point pattern data. Making the techniques accessible to non-mathematicians, the authors draw on their 25 years of software development experiences, methodological research, and broad scientific collaborations to deliver a book that clearly and succinctly explains concepts and addresses real scientific questions. Practical Advice on Data Analysis and Guidance on the Validity and Applicability of Methods The first part of the book gives an introduction to R software, advice about collecting data, information about handling and manipulating data, and an accessible introduction to the basic concepts of point processes. The second part presents tools for exploratory data analysis, including non-parametric estimation of intensity, correlation, and spacing properties. The third part discusses model-fitting and statistical inference for point patterns. The final part describes point patterns with additional {\u0026quot;}structure,{\u0026quot;} such as complicated marks, space-time observations, three- and higher-dimensional spaces, replicated observations, and point patterns constrained to a network of lines. Easily Analyze Your Own Data Throughout the book, the authors use their spatstat package, which is free, open-source code written in the R language. This package provides a wide range of capabilities for spatial point pattern data, from basic data handling to advanced analytic tools. The book focuses on practical needs from the user's perspective, offering answers to the most frequently asked questions in each chapter.\n  Hobbs \u0026amp; Hooten (2015): Bayesian modeling has become an indispensable tool for ecological research because it is uniquely suited to deal with complexity in a statistically coherent way. This textbook provides a comprehensive and accessible introduction to the latest Bayesian methods—in language ecologists can understand. Unlike other books on the subject, this one emphasizes the principles behind the computations, giving ecologists a big-picture understanding of how to implement this powerful statistical approach. Bayesian Models is an essential primer for non-statisticians. It begins with a definition of probability and develops a step-by-step sequence of connected ideas, including basic distribution theory, network diagrams, hierarchical models, Markov chain Monte Carlo, and inference from single and multiple models. This unique book places less emphasis on computer coding, favoring instead a concise presentation of the mathematical statistics needed to understand how and why Bayesian analysis works. It also explains how to write out properly formulated hierarchical Bayesian models and use them in computing, research papers, and proposals. This primer enables ecologists to understand the statistical principles behind Bayesian modeling and apply them to research, teaching, policy, and management.\n Presents the mathematical and statistical foundations of Bayesian modeling in language accessible to non-statisticians Covers basic distribution theory, network diagrams, hierarchical models, Markov chain Monte Carlo, and more - Deemphasizes computer coding in favor of basic principles Explains how to write out properly factored statistical expressions representing Bayesian models    Zuur et al. (2017): In Volume I we explain how to apply linear regression models, generalised linear models (GLM), and generalised linear mixed-effects models (GLMM) to spatial, temporal, and spatial-temporal data. The models that will be employed use the Gaussian and gamma distributions for continuous data, the Poisson and negative binomial distributions for count data, the Bernoulli distribution for absence–presence data, and the binomial distribution for proportional data.In Volume II we apply zero-inflated models and generalised additive (mixed-effects) models to spatial and spatial-temporal data. We also discuss models with more exotic distributions like the generalised Poisson distribution to deal with underdispersion and the beta distribution to analyse proportional data.\n  Zuur et al. (2010):\n While teaching statistics to ecologists, the lead authors of this paper have noticed common statistical problems. If a randomsample of theirwork (including scientific papers) produced before doing these courses were selected, half would probably contain violations of the underlying assumptions of the statistical techniquesemployed. Some violations have little impact on the results or ecological conclusions; yet others increase type I or type II errors, potentially resulting in wrong ecological conclusions. Most of these violations can be avoided by applying better data exploration. These problems are especially trouble- somein applied ecology, wheremanagement and policy decisions are often at stake. Here, we provide a protocol for data exploration; discuss current tools to detect outliers, heterogeneity of variance, collinearity, dependence of observations, problems with interactions, double zeros in multivariate analysis, zero inflation in generalized linear modelling, and the correct type of relationships between dependent and independent variables; and provide advice on how to address these problems when they arise. We also address misconceptions about normality, and provide advice on data transformations. Data exploration avoids type I and type II errors, among other problems, thereby reducing the chance ofmaking wrong ecological conclusions and poor recommendations. It is therefore essential for good quality management and policy based on statistical analyses. Key-words:    Kelleher \u0026amp; Wagener (2011): Our ability to visualize scientific data has evolved significantly over the last 40 years. However, this advancement does not necessarily alleviate many common pitfalls in visualization for scientific journals, which can inhibit the ability of readers to effectively understand the information presented. To address this issue within the context of visualizing environmental data, we list ten guidelines for effective data visualization in scientific publications. These guidelines support the primary objective of data visualization, i.e. to effectively convey information. We believe that this small set of guidelines based on a review of key visualization literature can help researchers improve the communication of their results using effective visualization. Enhancement of environmental data visualization will further improve research presentation and communication within and across disciplines.\n  Lohr (2010): Sharon L. Lohr's SAMPLING: DESIGN AND ANALYSIS, 2ND EDITION, provides a modern introduction to the field of survey sampling intended for a wide audience of statistics students. Practical and authoritative, the book is listed as a standard reference for training on real-world survey problems by a number of prominent surveying organizations. Lohr concentrates on the statistical aspects of taking and analyzing a sample, incorporating a multitude of applications from a variety of disciplines. The text gives guidance on how to tell when a sample is valid or not, and how to design and analyze many different forms of sample surveys. Recent research on theoretical and applied aspects of sampling is included, as well as optional technology instructions for using statistical software with survey data.\n  Zuur et al. (2009): Building on the successful Analysing Ecological Data (Zuur et al., 2007), the authors now provide an expanded introduction to using regression and its extensions in analysing ecological data. As with the earlier book, real data sets from postgraduate ecological studies or research projects are used throughout. The first part of the book is a largely non-mathematical introduction to linear mixed effects modelling, GLM and GAM, zero inflated models, GEE, GLMM and GAMM. The second part provides ten case studies that range from koalas to deep sea research. These chapters provide an invaluable insight into analysing complex ecological datasets, including comparisons of different approaches to the same problem. By matching ecological questions and data structure to a case study, these chapters provide an excellent starting point to analysing your own data. Data and R code from all chapters are available from www.highstat.com.\n  Zuur \u0026amp; Ieno (2016):\n Scientific investigation is of value only insofar as relevant results are obtained and communicated, a task that requires organizing, evaluating, analysing and unambiguously communicating the significance of data. In this context, working with ecological data, reflecting the complexities and interactions of the natural world, can be a challenge. Recent innovations for statistical analysis ofmultifaceted interrelated datamake obtaining more accu- rate andmeaningful results possible, but key decisions of the analyses to use, and which components to present in a scientific paper or report, may be overwhelming. We offer a 10-step protocol to streamline analysis of data thatwill enhance understanding of the data, the statistical models and the results, and optimize communication with the reader with respect to both the procedure and the outcomes. The protocol takes the investigator from study design and organization of data (formulating relevant questions, visualizing data collection, data exploration, identifying dependency), through conducting analysis (presenting, fitting and validating the model) and presenting output (numerically and visually), to extending themodel via simulation. Each step includes procedures to clarify aspects of the data that affect statistical analysis, as well as guidelines for written presentation. Steps are illustrated with examples using data from the literature. Following this protocol will reduce the organization, analysis and presentation ofwhatmay be an overwhelming information avalanche into sequential and, more to the point, manageable, steps. It provides guidelines for selecting optimal statistical tools to assess data relevance and significance, for choosing aspects of the analysis to include in a published report and for clearly communicating information.    Gelman \u0026amp; Hill (2007): Data Analysis Using Regression and Multilevel/Hierarchical Models is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors\u0026rsquo; own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout.\n  Lindenmayer \u0026amp; Likens (2010): Long-term monitoring programs are fundamental to understanding the natural environment and effectively tackling major environmental problems. Yet they are often done very poorly and ineffectively. Effective Ecological Monitoring describes what makes successful and unsuccessful long-term monitoring programs. Short and to the point, it illustrates key aspects with case studies and examples. It is based on the collective experience of running long-term research and monitoring programs of the two authors \u0026ndash; experience which spans more than 70 years. The book first outlines why long-term monitoring is important, then discusses why long-term monitoring programs often fail. The authors then highlight what makes good and effective monitoring. These good and bad aspects of long-term monitoring programs are further illustrated in the fourth chapter of the book. The final chapter sums up the future of long-term monitoring programs and how to make them better, more effective and better targeted.\n  Bolker (2008): Ecological Models and Data in R is the first truly practical introduction to modern statistical methods for ecology. In step-by-step detail, the book teaches ecology graduate students and researchers everything they need to know in order to use maximum likelihood, information-theoretic, and Bayesian techniques to analyze their own data using the programming language R. Drawing on extensive experience teaching these techniques to graduate students in ecology, Benjamin Bolker shows how to choose among and construct statistical models for data, estimate their parameters and confidence limits, and interpret the results. The book also covers statistical frameworks, the philosophy of statistical modeling, and critical mathematical functions and probability distributions. It requires no programming background\u0026ndash;only basic calculus and statistics.\n Practical, beginner-friendly introduction to modern statistical techniques for ecology using the programming language R Step-by-step instructions for fitting models to messy, real-world data Balanced view of different statistical approaches Wide coverage of techniques \u0026ndash; from simple (distribution fitting) to complex (state-space modeling) Techniques for data manipulation and graphical display Companion Web site with data and R code for all examples    Bibliography Agresti A. (2002). Categorical Data Analysis (Second Edition). John Wiley \u0026amp; Sons, Inc.\nBaddeley A., Rubak E. \u0026amp; Turner R. (2015). Spatial Point Patterns: Methodology and Applications with R. Chapman; Hall/CRC, Boca Raton.\nBolker B.M. (2008). Ecological Models and Data in R. Princeton University Press, Princeton, NJ.\nEmden H. van (2008). Statistics for Terrified Biologists. Blackwell Publishing.\nGelman A. \u0026amp; Hill J. (2007). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press, Cambridge. URL: http://www.loc.gov/catdir/enhancements/fy0668/2006040566-t.html.\nGrolemund G. \u0026amp; Wickham H. (2016). R for Data Science. URL: http://r4ds.had.co.nz/.\nHobbs N.T. \u0026amp; Hooten M.B. (2015). Bayesian Models: A Statistical Primer for Ecologists. Princeton University Press.\nJames G., Witten D., Hastie T. \u0026amp; Tibshirani R. (2013). An Introduction to Statistical Learning with Applications in R. Springer.\nKass R.E., Caffo B.S., Davidian M., Meng X.-L., Yu B. \u0026amp; Reid N. (2016). Ten Simple Rules for Effective Statistical Practice. PLOS Computational Biology 12 (6): e1004961. URL: http://dx.plos.org/10.1371/journal.pcbi.1004961. DOI: 10.1371/journal.pcbi.1004961.\nKelleher C. \u0026amp; Wagener T. (2011). Ten guidelines for effective data visualization in scientific publications. Environmental Modelling \u0026amp; Software 26 (6): 822–827. URL: https://www.sciencedirect.com/science/article/pii/S1364815210003270. DOI: 10.1016/J.ENVSOFT.2010.12.006.\nLindenmayer D. \u0026amp; Likens G.E. (2010). Effective ecological monitoring. Earthscan, London, UK.\nLohr S.L. (2010). Sampling: Design and Analysis, Second Edi. ed. Brooks/Cole.\nMcElreath R. (2015). Statistical rethinking : a Bayesian course with examples in R and Stan. Chapman; Hall/CRC, Boca Raton.\nQuinn G. \u0026amp; Keough M. (2002). Experimental design and data analysis for biologists. Cambridge University Press. URL: http://www.cambridge.org.\nvan Belle G. (2008). Statistical Rules of Thumb: Second Edition. John Wiley \u0026amp; Sons, Inc. DOI: 10.1002/9780470377963.\nZuur A.F. \u0026amp; Ieno E.N. (2016). A protocol for conducting and presenting results of regression-type analyses. Methods in Ecology and Evolution 7 (6): 636–645. URL: http://doi.wiley.com/10.1111/2041-210X.12577. DOI: 10.1111/2041-210X.12577.\nZuur A.F., Ieno E.N. \u0026amp; Elphick C.S. (2010). A protocol for data exploration to avoid common statistical problems. Methods in Ecology and Evolution 1 (9999): 3–14.\nZuur A.F., Ieno E.N. \u0026amp; Smith G.M. (2007). Analysing ecological data. Springer Verlag.\nZuur A.F., Ieno E.N., Anatoly, A \u0026amp; Saveliev (2017). Beginner’s guide to spatial, temporal, and spatial-temporal ecological data analysis with R-INLA. Highland Statistics Ltd. URL: http://www.highstat.com/Books/BGS/SpatialTemp/Zuuretal2017_TOCOnline.pdf.\nZuur A.F., Ieno E.N., Walker N.J., Saveliev A.A. \u0026amp; Smith G.M. (2009). Mixed effects models and extensions in ecology with R. Springer.\n","href":"/tutorials/articles/statistics/","title":"Books and articles on statistics"},{"content":"This page lists selected literature and online resources. Some are related to existing tutorial pages, while others are not. They are supposed to be of high interest to this site's users.\nSeveral of the resources were added based on an inspiring talk by Julia Lowndes at the SAFRED conference, Brussels, 27 Feb 2018.\nOverviews  Wilson et al. (2017): set of good computing practices that every researcher can adopt British Ecological Society (2014): planning the data life cycle; creating, processing, documenting, preserving, sharing \u0026amp; reusing data Cooper \u0026amp; Hsing (2017): file organisation, workflow documentation, code reproducibility and readability, writing reproducible reports, version control and code archiving Ibanez et al. (2014): vision on reproducible science, routine practices, collaboration, literate computing  See also some resources related to learning and education and the discipline of open and reproducible science.\nSpecific tools  Ross et al. (2017): about tidyverse workflow and tools  Focus on version control workflows  Bryan (2017): rationale, workflows and tools regarding version control for project organization Bryan et al. (2019): getting started with git and github workflows in RStudio  Bibliography British Ecological Society (Ed.) (2014). A guide to data management in ecology and evolution. BES Guides to Better Science. British Ecological Society, London.\nBryan J. (2017). Excuse me, do you have a moment to talk about version control? PeerJ Preprints 5: e3159v2. URL: https://peerj.com/preprints/3159. DOI: 10.7287/peerj.preprints.3159v2.\nBryan J., the STAT 545 TAs \u0026amp; Hester J. (2019). Happy Git and GitHub for the useR. URL: https://happygitwithr.com/.\nCooper N. \u0026amp; Hsing P.-Y. (Eds.) (2017). A guide to reproducible code in ecology and evolution. BES Guides to Better Science. British Ecological Society, London.\nIbanez L., Schroeder W.J. \u0026amp; Hanwell M.D. (2014). Practicing open science. In: Stodden V., Leisch F., Peng R.D. (editors). Implementing reproducible research. CRC Press, Boca Raton, FL.\nRoss Z., Wickham H. \u0026amp; Robinson D. (2017). Declutter your R workflow with tidy tools. PeerJ Preprints 5: e3180v1. URL: https://peerj.com/preprints/3180. DOI: 10.7287/peerj.preprints.3180v1.\nWilson G., Bryan J., Cranston K., Kitzes J., Nederbragt L. \u0026amp; Teal T.K. (2017). Good enough practices in scientific computing. PLOS Computational Biology 13 (6): e1005510. URL: http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510. DOI: 10.1371/journal.pcbi.1005510.\n","href":"/tutorials/articles/computing/","title":"Scientific computing and data handling workflows"},{"content":"This page lists selected literature and online resources. Some are related to existing tutorial pages, while others are not. They are supposed to be of high interest to this site's users.\nSeveral of the resources were added based on an inspiring talk by Julia Lowndes at the SAFRED conference, Brussels, 27 Feb 2018.\nThese resources focus on the learning and teaching aspect, while they often also give an overview of scientific computing workflows.\nOpen data science in general  Lowndes et al. (2017): learning open data science tools Hampton et al. (2017): training approaches and needed skills in data science Stevens et al. (2018): local community of practice for scientific programming: why, how (including scheme), challenges  In relation to statistics teaching  Kaplan (2017): ten organizing blocks for introductory statistics teaching in the present data science context Cetinkaya-Rundel \u0026amp; Rundel (2017): computational infrastructure and toolkit choices to allow for the necessary pedagogical innovations in statistics education  Bibliography Cetinkaya-Rundel M. \u0026amp; Rundel C.W. (2017). Infrastructure and tools for teaching computing throughout the statistical curriculum. PeerJ Preprints 5: e3181v1. URL: https://peerj.com/preprints/3181. DOI: 10.7287/peerj.preprints.3181v1.\nHampton S.E., Jones M.B., Wasser L.A., Schildhauer M.P., Supp S.R., Brun J., Hernandez R.R., Boettiger C., Collins S.L., Gross L.J., et al. (2017). Skills and Knowledge for Data-Intensive Environmental Research. BioScience 67 (6): 546–557. URL: https://academic.oup.com/bioscience/article/67/6/546/3784601. DOI: 10.1093/biosci/bix025.\nKaplan D.T. (2017). Teaching stats for data science. PeerJ Preprints 5: e3205v1. URL: https://peerj.com/preprints/3205. DOI: 10.7287/peerj.preprints.3205v1.\nLowndes J.S.S., Best B.D., Scarborough C., Afflerbach J.C., Frazier M.R., O’Hara C.C., Jiang N. \u0026amp; Halpern B.S. (2017). Our path to better science in less time using open data science tools. Nature Ecology \u0026amp; Evolution 1 (6): s41559–017–0160–017. URL: https://www.nature.com/articles/s41559-017-0160. DOI: 10.1038/s41559-017-0160.\nStevens S.L.R., Kuzak M., Martinez C., Moser A., Bleeker P.M. \u0026amp; Galland M. (2018). Building a local community of practice in scientific programming for Life Scientists. bioRxiv 265421. URL: https://www.biorxiv.org/content/early/2018/02/15/265421. DOI: 10.1101/265421.\n","href":"/tutorials/articles/skills/","title":"Acquiring the skills"},{"content":"This page lists selected literature and online resources. Some are related to existing tutorial pages, while others are not. They are supposed to be of high interest to this site's users.\nSeveral of the resources were added based on an inspiring talk by Julia Lowndes at the SAFRED conference, Brussels, 27 Feb 2018.\nSharing data  Ellis \u0026amp; Leek (2017): guidelines for providing data to a scientist / statistician: provide raw data, format consistently, include metadata \u0026amp; preprocessing steps Wilkinson et al. (2016): the FAIR data principles: date are to be findable, accessible, interoperable and reusable Culina et al. (2018): overview of online data infrastructures and considerations to be made Perkel (2016): data repository sites like github and others  Communicating  Smith et al. (2016): recommendations of the FORCE11 Software Citation Working Group  Bibliography Culina A., Baglioni M., Crowther T.W., Visser M.E., Woutersen-Windhouwer S. \u0026amp; Manghi P. (2018). Navigating the unfolding open data landscape in ecology and evolution. Nature Ecology \u0026amp; Evolution 2 (3): 420–426. URL: https://www.nature.com/articles/s41559-017-0458-2. DOI: 10.1038/s41559-017-0458-2.\nEllis S.E. \u0026amp; Leek J.T. (2017). How to share data for collaboration. PeerJ Preprints 5: e3139v5. URL: https://peerj.com/preprints/3139. DOI: 10.7287/peerj.preprints.3139v5.\nPerkel J. (2016). Democratic databases: Science on GitHub. Nature News 538 (7623): 127. URL: http://www.nature.com/news/democratic-databases-science-on-github-1.20719. DOI: 10.1038/538127a.\nSmith A.M., Katz D.S. \u0026amp; Niemeyer K.E. (2016). Software citation principles. PeerJ Computer Science 2: e86. URL: https://peerj.com/articles/cs-86. DOI: 10.7717/peerj-cs.86.\nWilkinson M.D., Dumontier M., Aalbersberg I.J., Appleton G., Axton M., Baak A., Blomberg N., Boiten J.-W., Silva Santos L.B. da, Bourne P.E., et al. (2016). The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data 3: 160018. URL: http://dx.doi.org/10.1038/sdata.2016.18.\n","href":"/tutorials/articles/communicating/","title":"Sharing and communicating"},{"content":"This page lists selected literature and online resources. Some are related to existing tutorial pages, while others are not. They are supposed to be of high interest to this site's users.\nSeveral of the resources were added based on an inspiring talk by Julia Lowndes at the SAFRED conference, Brussels, 27 Feb 2018.\nThese resources focus on the discipline as a whole, its perception, principles ,etc., while they often also give an overview of scientific computing workflows.\n Ibanez et al. (2014): open and reproducible science: vision, routine practices, collaboration, literate computing Hampton et al. (2015): workflows, tools, obstacles and needed mindshifts for open science Donati \u0026amp; Woolston (2017): how data science is becoming a large discipline  Focus on reproducible research  Stodden et al. (2014): book on computational reproducibility and (experiment) replicability; the three parts are Tools, Practices and Guidelines, Platforms Stodden \u0026amp; Miguez (2014): a formalized set of best practice recommendations for reproducible research Begley et al. (2015): current irreproducibility and good institutional practice  Bibliography Begley C.G., Buchan A.M. \u0026amp; Dirnagl U. (2015). Institutions must do their part for reproducibility. Nature 525 (7567): 25–27. URL: http://www.nature.com/news/robust-research-institutions-must-do-their-part-for-reproducibility-1.18259. DOI: 10.1038/525025a.\nDonati G. \u0026amp; Woolston C. (2017). Information management: Data domination. Nature 548 (7669): 613–614. URL: https://www.nature.com/nature/journal/v548/n7669/full/nj7669-613a.html?foxtrotcallback=true. DOI: 10.1038/nj7669-613a.\nHampton S.E., Anderson S.S., Bagby S.C., Gries C., Han X., Hart E.M., Jones M.B., Lenhardt W.C., MacDonald A., Michener W.K., et al. (2015). The Tao of open science for ecology. Ecosphere 6 (7): 1–13. URL: http://onlinelibrary.wiley.com/doi/10.1890/ES14-00402.1/abstract. DOI: 10.1890/ES14-00402.1.\nIbanez L., Schroeder W.J. \u0026amp; Hanwell M.D. (2014). Practicing open science. In: Stodden V., Leisch F., Peng R.D. (editors). Implementing reproducible research. CRC Press, Boca Raton, FL.\nStodden V. \u0026amp; Miguez S. (2014). Best Practices for Computational Science: Software Infrastructure and Environments for Reproducible and Extensible Research. Journal of Open Research Software 2 (1). URL: http://openresearchsoftware.metajnl.com/articles/10.5334/jors.ay/. DOI: 10.5334/jors.ay.\nStodden V., Leisch F. \u0026amp; Peng R.D. (2014). Implementing reproducible research. CRC Press, Boca Raton, FL.\n","href":"/tutorials/articles/open_science/","title":"The discipline of open science"},{"content":"","href":"/tutorials/articles/","title":"Articles"},{"content":"","href":"/tutorials/tutorials/","title":"Tutorials"},{"content":"Introduction First of all, thanks to consider making a new tutorial! By providing a tutorial, you are actively supporting your colleagues and the wider community and making work more efficient.\nIn this page, the roadmap towards a new tutorial will be explained.\nWriting a tutorial Each tutorial is setup in a subfolder of the content/tutorials folder. Within this folder, different files and formats may exist as you create them, but a file with the name index.md will be used for the tutorials website. So, you can create the tutorial directly in markdown or create it based on a Rmarkdown, a Jupyter notebook or any other format, as long as there is a single markdown file with the name index.md in your tutorial folder.\nThere are different ways to create this file. We will provide in this document specific instructions for markdown an Rmarkdown based tutorials. Still, if you got useful information or text in another format or you got stuck with the description, do not hesitate to describe your submission in a new issue . If you do not have a Github account, ask the IT helpdesk. We will try to support you as soon as possible.\n1. writing a markdown tutorial If you are directly writing your documentation in markdown syntax, you can either work online using Github or work on your own computer while using git.\nOnline submission Although providing less functionalities (adding custom images is not directly supported), the Github interface provides already a powerfull interface to add new content.\nTo write a new tutorial online, go to the INBO-tutorials Github repository and navigate to the /content/tutorials page, or use this link.\nNext, click the Create new file button\nYou will be directed to a new page and asked to Name your file\u0026hellip;\nProviding this name is very important, so make sure:\n provide a folder name + / + index.md the folder name needs to be all lowercase the folder name should not have spaces, but you can use _ to separate words provide a meaningful name without dates or names  For example: r_tidy_data/index.md, database_query_inboveg/index.md or statistics_regression/index.md\nNotice: The moment you type the /, Github will guide you by translating this to a folder name.\nNext, in the edit field you can start typing your tutorial. The header has an agreed format and you should copy paste this to start with:\n--- title: \u0026quot;YOUR TITLE\u0026quot; description: \u0026quot;SHORT DESCRIPTION ON TUTORIAL\u0026quot; author: \u0026quot;YOUR NAME\u0026quot; date: YYYY-MM-DD categories: [\u0026quot;YOUR_CATEGORY\u0026quot;] tags: [\u0026quot;FIRST_TAG\u0026quot;, \u0026quot;SECOND_TAG\u0026quot;, \u0026quot;...\u0026quot;] --- # your text starts here... Replace all the CAPITAL words with appropriate information:\n a short clear title a single or two line description your name the creation date, in the specified forma (year-month-day), e.g. 2019-01-04 or 2018-04-02 one or two categories from the specified list available here multiple tags you can pick yourself, all lowercase words. Have a look at the current wordcloud to check which would be useful for your submission.  Underneath the last triple dahs (---), you can write the tutorial as you like using markdown syntax. Use the Preview button to check how it would look like rendered on a website.\nNotice: You can add images from online source by using the URL of the image, e.g. ![short image description](http://.../my_image.jpg). For example, https://www.inbo.be/sites/all/themes/bootstrap_inbo/img/inbo/logo_nl.png will impor the INBO logo into your document:\nIf you are ready, commit your file to the website maintainers by filling in the boxes:\n Create new file: exchange this by a short message about the additions, e.g. Add tutorial to explain tidy data in R or Add tutorial about database queries in inboveg,\u0026hellip; Add an optional extended description: If you think more background info is suitable, add that in this box. yourgithubnam-patch you can replace this by the same name as your folder name above (e..g. r_tidy_data) to clarify your submission.  (the checkbox will always be on create a new branch, this is also the required option)\nNext, click commit new file and your submission will be reviewed by the website maintainers. If accepted, the tutorial will be automatically integrated in the tutorials website.\nUsing git (or Github Desktop, Rstudio,\u0026hellip;) When you ever used git or Github before, either using the command line, rstudio, Github Desktop,\u0026hellip; you can work on your own computer and submit the tutorial using git. In this section, we assume you are familiar to the git command or interface and have some basic knowledge. If not, no worries, we have a dedicated course to get you up to speed, see the INBO git course.\nNotice: The links in the different steps will refer to the Rstudio steps in the INBO git tutorial, but can be done using other interfaces or the command line as well. Pick the one you prefer\nIf it is your first submission using your computer, clone the INBO tutorials repository (so use the clone button on the https://github.com/inbo/tutorials page!).\nNext, we use the git workflow to submit the new tutorial:\n Update your code and create a new branch, cfr. STEP 1 of the workflow. Provide a useful name for your branch related to the topic of your tutorial, e.g. r_tidy_data or database_query_inboveg and not my_tutorial. Navigate to the subfolder content/tutorials and create a new subfolder in this directory. This will be the directory of your tutorial. Again, use a representative name for the directory name similar to the branch name. Within the folder, create a new file markdown index.md. The header of this file has an agreed format and you should copy paste this to start with:  --- title: \u0026quot;YOUR TITLE\u0026quot; description: \u0026quot;SHORT DESCRIPTION ON TUTORIAL\u0026quot; author: \u0026quot;YOUR NAME\u0026quot; date: YYYY-MM-DD categories: [\u0026quot;YOUR_CATEGORY\u0026quot;] tags: [\u0026quot;FIRST_TAG\u0026quot;, \u0026quot;SECOND_TAG\u0026quot;, \u0026quot;...\u0026quot;] --- # your text starts here... Replace all the CAPITAL words with appropriate information:\n a short clear title a single or two line description your name the creation date, in the specified forma (year-month-day), e.g. 2019-01-04 or 2018-04-02 one or two categories from the specified list available here multiple tags you can pick yourself, all lowercase words. Have a look at the current wordcloud to check which would be useful for your submission.  Underneath the last triple dahs (---), you can write the tutorial as you like using markdown syntax.\n add/commit the file to git, cfr. STEP 2 of the workflow. You can commit the tutorial all in once or split the commits in different steps, that is up to you. To make sure your work is updated online as well, push the tutorial as in STEP 3 of the workflow. When ready, push your tutorial a last time and create a Pull request to the website maintainers as explained in STEP 4 of the workflow.  After you pull request, your submission will be checked and reviewed. When accepted and merged, you tutorial will be online.\n2. Writing an Rmarkdown tutorial As you are writing the tutorial in Rmarkdown, we assume you are using Rstudio to write the tutorial. In this section, we assume you are familiar to the Rstudio git interface and have some basic knowledge. If not, no worries, we have a dedicated course to get you up to speed with git in Rstudio, see the INBO git course.\nIf it is your first submission using your computer, clone the INBO tutorials repository (so use the clone button on the https://github.com/inbo/tutorials page!).\nNext, we use the git workflow to submit the new tutorial:\n Update your code and create a new branch, cfr. STEP 1 of the workflow. Provide a useful name for your branch related to the topic of your tutorial, e.g. r_tidy_data or database_query_inboveg and not my_tutorial. Navigate to the subfolder content/tutorials and create a new subfolder in this directory. This will be the directory of your tutorial. Again, use a representative name for the directory name similar to the branch name. Within the folder, create a new file markdown index.Rmd. The header of this file has an agreed format and you should copy paste this to start with:  --- title: \u0026quot;YOUR TITLE\u0026quot; description: \u0026quot;SHORT DESCRIPTION ON TUTORIAL\u0026quot; author: \u0026quot;YOUR NAME\u0026quot; date: YYYY-MM-DD categories: [\u0026quot;YOUR_CATEGORY\u0026quot;] tags: [\u0026quot;FIRST_TAG\u0026quot;, \u0026quot;SECOND_TAG\u0026quot;, \u0026quot;...\u0026quot;] output: md_document: preserve_yaml: true --- # your text starts here... Replace all the CAPITAL words with appropriate information:\n a short clear title a single or two line description your name the creation date, in the specified forma (year-month-day), e.g. 2019-01-04 or 2018-04-02 one or two categories from the specified list available here multiple tags you can pick yourself, all lowercase words. Have a look at the current wordcloud to check which would be useful for your submission. leave the output section as it is  Underneath the last triple dahs (---), you can write the tutorial as you like using markdown syntax and add code chunks to run R code.\nAs the Rmarkown file will not be part of the website, make sure to click the knit button to create the equivalent markdown file (index.md) in the same directory:\nNotice: always knit the Rmarkdown file before you start committing the changes!\n add/commit both files to git, cfr. STEP 2 of the workflow. You can commit the tutorial all in once or split the commits in different steps, that is up to you. To make sure your work is updated online as well, push the tutorial as in STEP 3 of the workflow. When ready, push your tutorial a last time and create a Pull request to the website maintainers as explained in STEP 4 of the workflow.  After you pull request, your submission will be checked and reviewed. When accepted and merged, you tutorial will be online.\n","href":"/tutorials/create_tutorial/","title":"Create tutorial"},{"content":"Check the administrator installation or user installation pages in function of the administrator rights on your computer.\n","href":"/tutorials/installation/","title":"Installation notes"},{"content":"","href":"/tutorials/categories/","title":"Categories"},{"content":"","href":"/tutorials/tags/analysis/","title":"analysis"},{"content":"","href":"/tutorials/tags/mixed-model/","title":"mixed model"},{"content":"What you will learn In this tutorial we explain the analogy between the paired t-test and the corresponding mixed model formulation.\nUsed packages library(knitr) library(lme4) library(tidyr) library(broom) library(DHARMa) Data  plot: identifies paired measurements response: measurement values treatment: identifies two treatments (a and b)  set.seed(124) paired_data \u0026lt;- data.frame( plot = rep(1:10, 2), response = c(rnorm(10), rnorm(10, 3, 1.5)), treatment = rep(c(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;), each = 10) ) paired_data$treatment \u0026lt;- as.factor(paired_data$treatment) paired_data$plot \u0026lt;- as.factor(paired_data$plot) # in wide format paired_data_wide \u0026lt;- pivot_wider( paired_data, id_cols = plot, names_from = treatment, values_from = response ) kable(paired_data)    plot response treatment     1 -1.3850706 a   2 0.0383232 a   3 -0.7630302 a   4 0.2123061 a   5 1.4255380 a   6 0.7444798 a   7 0.7002294 a   8 -0.2293546 a   9 0.1970939 a   10 1.2071538 a   1 3.4775051 b   2 0.8643017 b   3 2.3923637 b   4 4.4930798 b   5 4.4382267 b   6 4.3771318 b   7 2.7735456 b   8 1.1653968 b   9 1.6967636 b   10 1.4362720 b    kable(paired_data_wide)    plot a b     1 -1.3850706 3.4775051   2 0.0383232 0.8643017   3 -0.7630302 2.3923637   4 0.2123061 4.4930798   5 1.4255380 4.4382267   6 0.7444798 4.3771318   7 0.7002294 2.7735456   8 -0.2293546 1.1653968   9 0.1970939 1.6967636   10 1.2071538 1.4362720    The paired t-test ttest \u0026lt;- with( paired_data_wide, t.test(y = a, x = b, paired = TRUE) ) kable(tidy(ttest))    estimate statistic p.value parameter conf.low conf.high method alternative     2.496692 5.157401 0.0005972 9 1.401584 3.591799 Paired t-test two.sided    Alternative, but equivalent formulation via a lineair mixed model Plot identifies the paired measurements. A random effect for plot allows us to take this dependence into account.\nmm \u0026lt;- lmer(response ~ treatment + (1 | plot), data = paired_data ) The parameter estimates for treatment b gives the difference compared to treatment a (= intercept), accounting for the paired nature of the data. This difference is the same as the estimate for the paired t-test.\nkable(tidy(mm))    term estimate std.error statistic group     (Intercept) 0.2147669 0.3711260 0.578690 fixed   treatmentb 2.4966918 0.4840988 5.157401 fixed   sd_(Intercept).plot 0.4534165 NA NA plot   sd_Observation.Residual 1.0824778 NA NA Residual    The anova output gives us a test for treatment in terms of an F-test. The t-test is based on the t-statistic. Both test statistics are related: (F = t^2).\nkable(anova(mm))     npar Sum Sq Mean Sq F value     treatment 1 31.16735 31.16735 26.59879    anova(mm)[[\u0026#34;F value\u0026#34;]] ## [1] 26.59879  unname(ttest[[\u0026#34;statistic\u0026#34;]])^2 ## [1] 26.59879  We can calculate the confidence interval given as part of the t-test output, based on the t-distributie.\ndifference \u0026lt;- data.frame( diff = summary(mm)$coefficients[2, 1], se = summary(mm)$coefficients[2, 2] ) difference$lwr \u0026lt;- difference$diff - qt(p = 1 - 0.05 / 2, df = 9) * difference$se difference$upr \u0026lt;- difference$diff + qt(p = 1 - 0.05 / 2, df = 9) * difference$se kable(difference)    diff se lwr upr     2.496692 0.4840988 1.401584 3.591799    The recommended procedure to calculate a confidence interval for parameters of mixed models is, however, to use the `confint function. Either an approximation (Wald statistic) or a profile likelihood confidence interval can be calculated. These intervals are slightly different from the t-distribution based confidence interval.\nVia profile likelihood:\nkable(confint(mm, parm = \u0026#34;treatmentb\u0026#34;, method = \u0026#34;profile\u0026#34;)) ## Computing profile confidence intervals ...      2.5 % 97.5 %     treatmentb 1.505743 3.487635    Wald-type confidence interval:\nkable(confint(mm, parm = \u0026#34;treatmentb\u0026#34;, method = \u0026#34;Wald\u0026#34;))     2.5 % 97.5 %     treatmentb 1.547876 3.445508    Were model assuptions met? Yes.\nDHARMa::plotQQunif(mm) Take home message The standard paired t-test is typically used to test for a significant differences between two paired treatments. We can formulate the test in terms of a mixed model. The benefit is that we get more informative model output, which allows us among other things to check if model assumptions were met. For the paired t-test, one assumption is that the paired differences between treatments follow a normal distribution. When these assumptions are not met, the flexibility of the mixed model framework allows to improve the model to better fit the requirements for the data at hand. For instance, one can choose from a number of parametric statistical distributions that are likely to fit the data (for counts, the Poisson or negative binomial distribution can be chosen, and for binary or proportional data, a binomial distribution is an obvious choice).\n","href":"/tutorials/tutorials/r_paired_t_test/","title":"Mixed model formulation of a paired t-test"},{"content":"","href":"/tutorials/tags/r/","title":"r"},{"content":"","href":"/tutorials/categories/r/","title":"r"},{"content":"Used packages library(ggplot2) ## Warning: package 'ggplot2' was built under R version 4.0.2  library(lme4) ## Loading required package: Matrix  Dummy data For the sake of this demontration we use a very simple dataset with a very high signal versus noise ratio. Let’s look at a simple timeseries with multiple observations per timepoint.\nset.seed(213354) n.year \u0026lt;- 30 n.replicate \u0026lt;- 10 sd.noise \u0026lt;- 0.1 dataset \u0026lt;- expand.grid( Replicate = seq_len(n.replicate), Year = seq_len(n.year) ) dataset$fYear \u0026lt;- factor(dataset$Year) dataset$Noise \u0026lt;- rnorm(nrow(dataset), mean = 0, sd = sd.noise) dataset$Linear \u0026lt;- 1 + 2 * dataset$Year + dataset$Noise dataset$Quadratic \u0026lt;- 1 + 20 * dataset$Year - 0.5 * dataset$Year ^ 2 + dataset$Noise ggplot(dataset, aes(x = Year, y = Linear)) + geom_point() ggplot(dataset, aes(x = Year, y = Quadratic)) + geom_point() Quadratic trend but fit as a linear trend Assume that we assume that the trend is linear but in reality it is quadratic. In this case the random intercept of year will pickup the differences with the linear trend.\nmodel.quadratric \u0026lt;- lmer(Quadratic ~ Year + (1|Year), data = dataset) summary(model.quadratric) ## Linear mixed model fit by REML ['lmerMod'] ## Formula: Quadratic ~ Year + (1 | Year) ## Data: dataset ## ## REML criterion at convergence: -90.8 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.13929 -0.65021 -0.00099 0.55503 3.14673 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Year (Intercept) 1198.1591 34.6144 ## Residual 0.0111 0.1054 ## Number of obs: 300, groups: Year, 30 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 83.6622 12.9622 6.454 ## Year 4.4999 0.7301 6.163 ## ## Correlation of Fixed Effects: ## (Intr) ## Year -0.873  dataset$QuadraticPredict \u0026lt;- predict(model.quadratric) dataset$QuadraticFixed \u0026lt;- predict(model.quadratric, re.form = ~0) ggplot(dataset, aes(x = Year, y = Quadratic)) + geom_point() + geom_line(aes(y = QuadraticFixed), colour = \u0026#34;red\u0026#34;) + geom_line(aes(y = QuadraticPredict), colour = \u0026#34;blue\u0026#34;) The random effects will have a strong pattern. Indicating that a second order polynomial makes more sense than a linear trend.\nrf \u0026lt;- data.frame( Year = seq_len(n.year), RandomIntercept = ranef(model.quadratric)$Year[, 1] ) ggplot(rf, aes(x = Year, y = RandomIntercept)) + geom_point() What if the trend is perfectly linear? Both the fixed effect and the random effect can perfectly model the pattern in the data. So the model seems to be unidentifiable. However the likelihood of the model contains a penalty term for the random intercept. The stronger the absolute value of the random effect, the stronger the penalty. The fixed effects have no penalty term. Hence, model with strong fixed effect will have a higher likelihood than exactly the same fit generated by strong random effects.\nIn this case the linear trend is very strong compared to the noise. So the linear trend in the fixed effect fits the data very well. Note that the random effect variance is zero.\nmodel.linear \u0026lt;- lmer(Linear ~ Year + (1|Year), data = dataset) ## boundary (singular) fit: see ?isSingular  summary(model.linear) ## Linear mixed model fit by REML ['lmerMod'] ## Formula: Linear ~ Year + (1 | Year) ## Data: dataset ## ## REML criterion at convergence: -483.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.3565 -0.6669 -0.0309 0.5717 3.0381 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Year (Intercept) 0.00000 0.0000 ## Residual 0.01095 0.1046 ## Number of obs: 300, groups: Year, 30 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 0.995497 0.012392 80.34 ## Year 1.999948 0.000698 2865.22 ## ## Correlation of Fixed Effects: ## (Intr) ## Year -0.873 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular  dataset$LinearPredict \u0026lt;- predict(model.linear) dataset$LinearFixed \u0026lt;- predict(model.linear, re.form = ~0) ggplot(dataset, aes(x = Year, y = Linear)) + geom_point() + geom_line(aes(y = LinearFixed), colour = \u0026#34;red\u0026#34;) + geom_line(aes(y = LinearPredict), colour = \u0026#34;blue\u0026#34;) What about fitting year as a factor in the fixed effects Combining the same variables as a factor in the fixed effects and as a random intercept doesn’t make sense. They allow exactly the same model fit and thus the random intercept will always shrink to zero.\n","href":"/tutorials/tutorials/r_fixed_random/","title":"Same variable in fixed and random effects"},{"content":"","href":"/tutorials/categories/statistics/","title":"statistics"},{"content":"","href":"/tutorials/tags/","title":"Tags"},{"content":"","href":"/tutorials/tags/sql/","title":"SQL"},{"content":"","href":"/tutorials/tags/styleguide/","title":"styleguide"},{"content":"","href":"/tutorials/categories/styleguide/","title":"styleguide"},{"content":"SQL is a standard language for storing, manipulating and retrieving data in databases. This is not a SQL-course but a styleguide, describing how to enhance the readability of your SQL-scripts. Focus of the styleguide is on scripts for retrieving data.\nIn short  --This is how a basic SQL-script should look like /** Description: Lijst met broedvogels per UTM1-hok sinds 2010 Created: 2015-08-12 Created by: Frederic Piesschaert **/ SELECT w.WRNG_JAR AS jaar , w.WRNG_UTM1_CDE AS UTM1 , s.SPEC_NAM_WET AS wetenschappelijke_naam , s.SPEC_NAM_NED AS nederlandse_naam , t.TOPO_DES AS locatie FROM tblWaarneming w INNER JOIN tblWaarnemingMeting wm ON w.WRNG_ID = wm.WRME_WRNG_ID INNER JOIN tblSoort s ON wm.WRME_SPEC_CDE = s.SPEC_CDE LEFT JOIN tblToponiem t on t.TOPO_ID = w.WRNG_TOPO_ID --toponiemen werden niet altijd ingevuld WHERE 1=1 AND w.WRNG_UTM1_CDE IS NOT NULL AND w.WRNG_JAR \u0026gt; 2010 GROUP BY w.WRNG_JAR , w.WRNG_UTM1_CDE , s.SPEC_NAM_WET , s.SPEC_NAM_NED , t.TOPO_DES ORDER BY s.SPEC_NAM_NED  SQL-keywords (SELECT, FROM, JOIN, WHERE, GROUP BY, \u0026hellip;) are written in capitals Table names and field names are capitalized as they are defined in the database Use short and meaningfull aliases and write them in lowercase Use a new line for each field in the SELECT-statement and each argument in the WHERE and GROUP BY clause Put the comma in front of the line in SELECT and GROUP BY statements Indent each field in the SELECT-statement and each argument in the WHERE and GROUP BY clause When multiple arguments are used in the WHERE clause, AND/OR keywords are always placed at the front Use full INNER JOIN statements JOINS should be indented Subqueries should be indented and properly named Document your scripts  Layout   SQL-keywords (SELECT, FROM, JOIN, WHERE, GROUP BY, \u0026hellip;) are written in capitals\n--Good SELECT * FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 --Bad Select * From person p Inner join address a on a.personid = p.id where p.age \u0026gt; 50 --Ugly :)   Table names and field names are capitalized as they are defined in the database, i.e. lowercase when lowercase in the database, capitals when capitals in the database\n  Aliases are written in lowercase\n--Good SELECT p.* , a.city FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 --Bad SELECT P.* , A.city FROM person P INNER JOIN address A ON A.personid = P.id WHERE P.age \u0026gt; 50   Table aliases are short and meaningfull in the context of the query\n--Good SELECT p.* , a.city FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 --Bad SELECT x.* , placewheresomeonelives.city FROM person x INNER JOIN address placewheresomeonelives ON placewheresomeonelives.personid = x.id WHERE x.age \u0026gt; 50   Always use aliases\n  Use a new line for each field in the SELECT-statement and each argument in the WHERE and GROUP BY clause.\n--Good SELECT p.firstname , p.lastname , a.city FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 AND a.city like ‘Ar%’ --Bad SELECT p.firstname, p.lastname, a.city FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 AND a.city like ‘Ar%’   Put the comma in front of the line in SELECT and GROUP BY statements\n--Good SELECT p.firstname , p.lastname , a.city , COUNT(*) AS Aantal FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 AND a.city like ‘Ar%’ GROUP BY p.firstname , p.lastname , a.city --Less good SELECT p.firstname, p.lastname, a.city, COUNT(*) AS Aantal FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 AND a.city like ‘Ar%’ GROUP BY p.firstname, p.lastname, a.city   Indent each field in the SELECT-statement and each argument in the WHERE and GROUP BY clause\n--Good SELECT p.firstname , p.lastname , a.city FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 AND a.city like ‘Ar%’ --Bad SELECT p.firstname , p.lastname , a.city FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 AND a.city like ‘Ar%’   When multiple arguments are used in the WHERE clause, AND/OR keywords are always placed at the front\n--Good SELECT * FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 AND a.city like ‘Ar%’ AND p.firstname = 'Billy' --Bad SELECT * FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 AND a.city like ‘Ar%’ AND p.firstname = 'Billy'   Multiple constraints for a JOIN are on the same line\n--Good layout (for a poorly designed database) SELECT * FROM person p INNER JOIN address a ON a.firstname = p.firstname AND a.lastname = p.lastname WHERE p.age \u0026gt; 50 AND a.city like ‘Ar%’ AND p.firstname = 'Billy' --Bad SELECT * FROM person p INNER JOIN address a ON a.firstname = p.firstname AND a.lastname = p.lastname WHERE p.age \u0026gt; 50 AND a.city like ‘Ar%’ AND p.firstname = 'Billy'   Use a full INNER JOIN statement\n--Good SELECT p.* , a.city FROM person p INNER JOIN address a ON a.personid = p.id LEFT JOIN hobby h ON h.personid = p.id --Bad SELECT p.* , a.city FROM person p JOIN address a ON a.personid = p.id LEFT JOIN hobby h ON h.personid = p.id   Joins should be indented\n--Good SELECT p.* , a.city FROM person p INNER JOIN address a ON a.personid = p.id LEFT OUTER JOIN hobby h ON h.personid = p.id WHERE p.age \u0026gt; 50 --Bad SELECT p.* , a.city FROM person p INNER JOIN address a ON a.personid = p.id LEFT OUTER JOIN hobby h ON h.personid = p.id WHERE p.age \u0026gt; 50   Subqueries should be indented and properly named. Opening bracket is on a new line and aligned with the JOIN. Closing bracket is aligned with the opening bracket.\n--Good SELECT ppds.ppnt_cde , ds.drso_ser_nbr , dsth.dsth_dsha_cde , dsth.dsth_ocr_dte FROM tblDruksondetoestandhistoriek dsth INNER JOIN tblDruksonde ds ON ds.drso_id = dsth.dsth_drso_id INNER JOIN ( SELECT p.ppnt_cde , pd.ppds_drso_id FROM relPeilpuntdruksonde pd INNER JOIN tblPeilpunt p ON p.ppnt_id = pd.ppds_ppnt_id WHERE p.ppnt_cde like 'KAMP%' )ppds ON ppds.ppds_drso_id = dsth.dsth_drso_id WHERE dsth.dsth_dsha_cde = 'PROG’ --Bad SELECT ppds.ppnt_cde , ds.drso_ser_nbr , dsth.dsth_dsha_cde , dsth.dsth_ocr_dte FROM tblDruksondetoestandhistoriek dsth INNER JOIN tblDruksonde ds ON ds.drso_id = dsth.dsth_drso_id INNER JOIN ( SELECT p.ppnt_cde , pd.ppds_drso_id FROM relPeilpuntdruksonde pd INNER JOIN tblPeilpunt p ON p.ppnt_id = pd.ppds_ppnt_id WHERE p.ppnt_cde like 'KAMP%')ppds ON ppds.ppds_drso_id = dsth.dsth_drso_id WHERE dsth.dsth_dsha_cde = 'PROG’   Documentation   Rename your output fields when necessary. It makes the output comprehensible for users that are not familiar with the datamodel.\n--Good SELECT mpnt_cde AS meetpunt , mpnt_mpst_cde AS meetpuntstatus , mpnt_mptp_cde AS meetpuntype FROM tblmeetpunt --Bad SELECT mpnt_cde , mpnt_mpst_cde , mpnt_mptp_cde FROM tblmeetpunt   Use /** and **/ for comment blocks, e.g. a description at the beginning of the query\n--Example /** Deze query haalt naam en gemeente op van de werknemers boven de 50 jaar CreateDate: 21/05/2015 Created by: Bill Gates **/ SELECT p.name , a.city , p.age FROM person p LEFT OUTER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 AND p.firstname = ‘Piet’   Use \u0026ndash; for small comments in the query\n--Example SELECT mpnt_cde AS meetpunt , mpnt_mpst_cde AS meetpuntstatus , mpnt_mptp_cde AS meetpuntype FROM tblmeetpunt WHERE mpnt_mpst_cde = ‘VLD’ --only validated points   Tips and tricks   Use TOP 10 (or LIMIT 10 in Postgres) when designing queries with a large resultset (taking a long time to run). It saves a lot of time in the design stage.\n  Use 1=1 as the first line of the WHERE clause. This allows you to easily turn on and off all restrictions while designing your query. Beware of OR: where 1=1 OR age\u0026gt;50 doesn’t mean that everybody is +50.\n--Example SELECT p.firstname , p.lastname , a.city FROM person p INNER JOIN address a ON a.personid = p.id WHERE 1=1 --AND p.age \u0026gt; 50 AND a.city like ‘Ar%’ --Try to turn on and off the age constraint in this case. Pretty annoying, isn’t it? SELECT p.firstname , p.lastname , a.city FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 AND a.city like ‘Ar%’   Ordering the output records can be done by explicitly using the name of the fields or by using the field number as you have defined them in the SELECT statement\n--Example SELECT p.firstname , p.lastname , a.city FROM person p INNER JOIN address a ON a.personid = p.id ORDER BY a.city, p.lastname --This will return the same result SELECT p.firstname , p.lastname , a.city FROM person p INNER JOIN address a ON a.personid = p.id ORDER BY 3, 2   For advanced users: use Common Table Expressions instead of complex subqueries. It makes your query modular and easier to understand for other users\n  ","href":"/tutorials/tutorials/sql_styleguide/","title":"Styleguide SQL-scripts"},{"content":"","href":"/tutorials/tags/gis/","title":"gis"},{"content":"","href":"/tutorials/tags/maps/","title":"maps"},{"content":"What is WFS? In computing, the Open Geospatial Consortium (OGC) Web Feature Service (WFS) Interface Standard provides an interface allowing requests for geographical features across the web using platform-independent calls. One can think of geographical features as the “source code” behind a map, whereas the Web Mapping Service (WMS) interface or online tiled mapping portals like Google Maps return only an image, which end-users cannot edit or spatially analyze.\nThe standard output format for reading or downloading data from a WFS is the XML-based Geography Markup Language (GML), but other formats like shapefiles or geojson are also available. In early 2006 the OGC members approved the OpenGIS GML Simple Features Profile. This profile is designed both to increase interoperability between WFS servers and to improve the ease of implementation of the WFS standard. (Source: Wikipedia)\nBefore we embark on using a WFS service in R, we would like to draw your attention to the following issue when using a WFS service in a workflow that needs to be reproducible in the longer term. A distributed, stable storage of the GIS data used in a workflow is necessary to make that workflow reproducible on a longer term. When using dynamic databases and web services, where the data they present are expected to evolve (e.g. by always pointing at the latest release in these WFS-cases), this is not guaranteed if you just use the WFS service to directly read GIS data from it. Instead of directly reading the GIS data, it is also possible to download the data from the WFS service to disk (and possibly archive on, e.g., Zenodo) in order to obtain a stable version that can be re-used later by reading it from disk. Also, in case it takes a long time to get the GIS data from the WFS service, downloading instead of directly reading is often a better choice. On the other hand, some types of workflows might instead require the most up to date GIS data that are available to be retrieved while no workflow reproducibility is needed. Do note that publishing the workflow is always recommended for published work. It serves the scientific community and it is also a way of proving scientific quality - hence reproducibility is often the best choice for published work.\nSome of the material presented in this tutorial benefitted from a tutorial presented at the Use of R in Official Statistics conference in The Hague, September 2018 Spatial Analysis in R with Open Geodata and from Lovelace, Nowosad, and Muenchow (2020).\nUseful overviews of web feature services WFS services for Belgium and regions in Belgium:\n overview compiled by Michel Stuyts, which is also available on gitlab overview maintained by DOV Vlaanderen  European portals:\n inspire geoportal: European portal for spatial data - some of which have a WFS service environmental data for Europe: many of the products listed have either a WMS or a WFS service  Worldwide coverage:\n spatineo directory  Used packages library(sf) # simple features packages for handling vector GIS data ## Linking to GEOS 3.8.0, GDAL 3.0.4, PROJ 6.3.1  library(httr) # generic webservice package library(tidyverse) # a suite of packages for data wrangling, transformation, plotting, ... ## -- Attaching packages ---------------------------------------------------------- tidyverse 1.3.0 -- ## v ggplot2 3.3.0 v purrr 0.3.4 ## v tibble 3.0.1 v dplyr 0.8.5 ## v tidyr 1.0.2 v stringr 1.4.0 ## v readr 1.3.1 v forcats 0.5.0 ## -- Conflicts ------------------------------------------------------------- tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag()  library(ows4R) # interface for OGC webservices ## Loading required package: geometa ## Loading ISO 19139 XML schemas... ## Loading ISO 19115 codelists...  Get to know what the service can do with GetCapabilities First of all we need the URL of the service.\nwfs_bwk \u0026lt;- \u0026#34;https://geoservices.informatievlaanderen.be/overdrachtdiensten/BWK/wfs\u0026#34; url \u0026lt;- parse_url(wfs_bwk) url$query \u0026lt;- list(service = \u0026#34;WFS\u0026#34;, version = \u0026#34;2.0.0\u0026#34;, request = \u0026#34;GetCapabilities\u0026#34;) request \u0026lt;- build_url(url) request ## [1] \u0026quot;https://geoservices.informatievlaanderen.be/overdrachtdiensten/BWK/wfs?service=WFS\u0026amp;version=2.0.0\u0026amp;request=GetCapabilities\u0026quot;  With GetCapabilities, we obtain a complete overview of all metadata for the web service.\nTo see all capabilities, you can visit the request in the webbrowser. For instance opening the page in the webbrowser and searching for “Filter_Capabilities” allows you to see all possible ways to filter the data from a WFS layer (e.g. restrict the downloaded data to a specified bounding box with SpatialOperator name=\u0026quot;BBOX\u0026quot;).\nInstead of searching the page on the web, there are several ways to access specific pieces of information programmatically. We will show here how to do this using functions in the ows4R package. The first thing we need to do is generate a connection to the WFS with the aid of WFSClient$new().\nbwk_client \u0026lt;- WFSClient$new(wfs_bwk, serviceVersion = \u0026#34;2.0.0\u0026#34;) ## list() ## Warning in CPL_crs_from_input(x): GDAL Message 1: +init=epsg:XXXX syntax is ## deprecated. It might return a CRS with a non-EPSG compliant axis order.  The resulting object bwk_client is an R6 object. If you are not familiar with R6 object, you might want to read the R6 chapter in Advanced R.\nPrinting `bwk_client looks like this:\nbwk_client ## \u0026lt;WFSClient\u0026gt; ## Inherits from: \u0026lt;OWSClient\u0026gt; ## Public: ## attrs: list ## capabilities: WFSCapabilities, OWSCapabilities, OGCAbstractObject, R6 ## clone: function (deep = FALSE) ## defaults: list ## describeFeatureType: function (typeName) ## encode: function (addNS = TRUE, geometa_validate = TRUE, geometa_inspire = FALSE) ## ERROR: function (text) ## getCapabilities: function () ## getClass: function () ## getClassName: function () ## getFeatures: function (typeName, ...) ## getFeatureTypes: function (pretty = FALSE) ## getPwd: function () ## getToken: function () ## getUrl: function () ## getUser: function () ## getVersion: function () ## INFO: function (text) ## initialize: function (url, serviceVersion = NULL, user = NULL, pwd = NULL, ## logger: function (type, text) ## loggerType: NULL ## reloadCapabilities: function () ## url: https://geoservices.informatievlaanderen.be/overdrachtdi ... ## verbose.debug: FALSE ## verbose.info: FALSE ## version: 2.0.0 ## WARN: function (text) ## wrap: FALSE ## Private: ## pwd: NULL ## serviceName: WFS ## system_fields: verbose.info verbose.debug loggerType wrap attrs defaults ## token: NULL ## user: NULL ## xmlElement: NULL ## xmlNamespace: NULL ## xmlNodeToCharacter: function (x, ..., indent = \u0026quot;\u0026quot;, tagSeparator = \u0026quot;\\n\u0026quot;)  The features listed can be accessed using $. We can see that some of them are again functions. As a first example, the following code will list all available layers for that WFS.\nbwk_client$getFeatureTypes(pretty = TRUE) ## name title ## 1 BWK:Bwkhab BWK 2 - BWK-zone en Natura 2000 Habitat ## 2 BWK:Bwkfauna BWK 2 - Faunistisch belangrijke gebieden ## 3 BWK:Hab3260 BWK 2 - Habitattype 3260  Another way of accessing this information is:\nbwk_client$getFeatureTypes() %\u0026gt;% map_chr(function(x){x$getName()}) ## [1] \u0026quot;BWK:Bwkhab\u0026quot; \u0026quot;BWK:Bwkfauna\u0026quot; \u0026quot;BWK:Hab3260\u0026quot;  bwk_client$getFeatureTypes() %\u0026gt;% map_chr(function(x){x$getTitle()}) ## [1] \u0026quot;BWK 2 - BWK-zone en Natura 2000 Habitat\u0026quot; ## [2] \u0026quot;BWK 2 - Faunistisch belangrijke gebieden\u0026quot; ## [3] \u0026quot;BWK 2 - Habitattype 3260\u0026quot;  We are using purrr::map() functionalities here, because the output of bwk_client$getFeatureTypes() is a list and each element of the list contains information about a layer.\nLet’s turn back our attention to getCapabilities() and see what information we can extract from it.\nbwk_client$getCapabilities() ## \u0026lt;WFSCapabilities\u0026gt; ## Inherits from: \u0026lt;OWSCapabilities\u0026gt; ## Public: ## attrs: list ## clone: function (deep = FALSE) ## defaults: list ## encode: function (addNS = TRUE, geometa_validate = TRUE, geometa_inspire = FALSE) ## ERROR: function (text) ## findFeatureTypeByName: function (expr, exact = FALSE) ## getClass: function () ## getClassName: function () ## getFeatureTypes: function (pretty = FALSE) ## getOperationsMetadata: function () ## getOWSVersion: function () ## getRequest: function () ## getService: function () ## getServiceIdentification: function () ## getServiceProvider: function () ## getServiceVersion: function () ## getUrl: function () ## INFO: function (text) ## initialize: function (url, version, logger = NULL) ## logger: function (type, text) ## loggerType: NULL ## verbose.debug: FALSE ## verbose.info: FALSE ## WARN: function (text) ## wrap: FALSE ## Private: ## featureTypes: list ## fetchFeatureTypes: function (xmlObj, version) ## operationsMetadata: OWSOperationsMetadata, R6 ## owsVersion: 1.1 ## request: OWSGetCapabilities, OWSRequest, OGCAbstractObject, R6 ## service: WFS ## serviceIdentification: OWSServiceIdentification, R6 ## serviceProvider: OWSServiceProvider, R6 ## serviceVersion: 2.0.0 ## system_fields: verbose.info verbose.debug loggerType wrap attrs defaults ## url: https://geoservices.informatievlaanderen.be/overdrachtdi ... ## xmlElement: NULL ## xmlNamespace: NULL ## xmlNodeToCharacter: function (x, ..., indent = \u0026quot;\u0026quot;, tagSeparator = \u0026quot;\\n\u0026quot;)  This is again an R6 class object and the $ can be used to chain together several functions, much in the same way as the pipe operator %\u0026gt;%. The following chunk illustrates its use. Try executing this incrementally (select and execute code up to first $, second $, …) to see what happens.\nbwk_client$ getCapabilities()$ findFeatureTypeByName(\u0026#34;BWK:Bwkhab\u0026#34;)$ getDescription() %\u0026gt;% map_chr(function(x){x$getName()}) ## list() ## [1] \u0026quot;UIDN\u0026quot; \u0026quot;OIDN\u0026quot; \u0026quot;TAG\u0026quot; \u0026quot;EVAL\u0026quot; \u0026quot;EENH1\u0026quot; ## [6] \u0026quot;EENH2\u0026quot; \u0026quot;EENH3\u0026quot; \u0026quot;EENH4\u0026quot; \u0026quot;EENH5\u0026quot; \u0026quot;EENH6\u0026quot; ## [11] \u0026quot;EENH7\u0026quot; \u0026quot;EENH8\u0026quot; \u0026quot;V1\u0026quot; \u0026quot;V2\u0026quot; \u0026quot;V3\u0026quot; ## [16] \u0026quot;HERK\u0026quot; \u0026quot;INFO\u0026quot; \u0026quot;BWKLABEL\u0026quot; \u0026quot;HAB1\u0026quot; \u0026quot;PHAB1\u0026quot; ## [21] \u0026quot;HAB2\u0026quot; \u0026quot;PHAB2\u0026quot; \u0026quot;HAB3\u0026quot; \u0026quot;PHAB3\u0026quot; \u0026quot;HAB4\u0026quot; ## [26] \u0026quot;PHAB4\u0026quot; \u0026quot;HAB5\u0026quot; \u0026quot;PHAB5\u0026quot; \u0026quot;HERKHAB\u0026quot; \u0026quot;HERKPHAB\u0026quot; ## [31] \u0026quot;HABLEGENDE\u0026quot; \u0026quot;SHAPE\u0026quot;  This lists all available fields for the layer “BWK:Bwkhab”.\nHere is how to get a character vector naming all available operations of the WFS:\nbwk_client$ getCapabilities()$ getOperationsMetadata()$ getOperations() %\u0026gt;% map_chr(function(x){x$getName()}) ## [1] \u0026quot;GetCapabilities\u0026quot; \u0026quot;DescribeFeatureType\u0026quot; \u0026quot;GetFeature\u0026quot; ## [4] \u0026quot;GetPropertyValue\u0026quot; \u0026quot;ListStoredQueries\u0026quot; \u0026quot;DescribeStoredQueries\u0026quot; ## [7] \u0026quot;CreateStoredQuery\u0026quot; \u0026quot;DropStoredQuery\u0026quot;  The next chunk shows how we can extract the available output formats. We will see later that GetFeature is the operation needed to read or download data from the WFS. The metadata for this operation has what we want and we can extract it with a combination of purrr::map() and `purrr::pluck().\nbwk_client$ getCapabilities()$ getOperationsMetadata()$ getOperations() %\u0026gt;% map(function(x){x$getParameters()}) %\u0026gt;% pluck(3, \u0026#34;outputFormat\u0026#34;) ## [1] \u0026quot;text/xml; subtype=gml/3.2\u0026quot; ## [2] \u0026quot;gml32\u0026quot; ## [3] \u0026quot;application/gml+xml; version=3.2\u0026quot; ## [4] \u0026quot;GML2\u0026quot; ## [5] \u0026quot;KML\u0026quot; ## [6] \u0026quot;SHAPE-ZIP\u0026quot; ## [7] \u0026quot;application/json\u0026quot; ## [8] \u0026quot;application/vnd.google-earth.kml xml\u0026quot; ## [9] \u0026quot;application/vnd.google-earth.kml+xml\u0026quot; ## [10] \u0026quot;csv\u0026quot; ## [11] \u0026quot;gml3\u0026quot; ## [12] \u0026quot;json\u0026quot; ## [13] \u0026quot;text/xml; subtype=gml/2.1.2\u0026quot; ## [14] \u0026quot;text/xml; subtype=gml/3.1.1\u0026quot;  Some more examples follow. Let’s extract the bounding boxes for all layers.\nbwk_client$ getCapabilities()$ getFeatureTypes() %\u0026gt;% map(function(x){x$getBoundingBox()}) ## [[1]] ## min max ## x 2.525262 5.936009 ## y 50.673762 51.505480 ## ## [[2]] ## min max ## x 2.525262 5.936009 ## y 50.673762 51.505480 ## ## [[3]] ## min max ## x 2.525262 5.936009 ## y 50.673762 51.505480  As expected for this WFS, the bounding boxes are the same for all layers.\nThe final example shows how to get the abstract so we can read about the contents of the layers.\nbwk_client$ getCapabilities()$ getFeatureTypes() %\u0026gt;% map_chr(function(x){x$getAbstract()}) ## [1] \u0026quot;Zone waaraan een biologische waarde gegeven wordt, alsook een aanduiding van het Natura 2000-habitattype, de vegetatiekundige eenheden of ecotopen, bodembedekking en gegevens over eventueel aanwezige kleine landschapselementen (BWK-karteringseenheden).\u0026quot; ## [2] \u0026quot;Op de Biologische Waarderingskaart krijgen een aantal gebieden een specifieke arcering omwille van de aanwezigheid van bepaalde fauna-elementen. De afbakening is gebaseerd op soorten die tot de Rode lijst-categorieën 'Met uitsterven bedreigd', 'Bedreigd' en 'Kwetsbaar' behoren. Een ruimere omschrijving wordt verstrekt in het afzonderlijk document per kaartblad \\\u0026quot;toelichtingXX\\\u0026quot;, met XX het kaartbladnummer.\u0026quot; ## [3] \u0026quot;Voorkomen van het Natura 2000 habitattype 3260, de submontane- en laaglandrivieren met waterranonkel- en fonteinkruidvegetaties.\u0026quot;  Read or donwload vector data from WFS: GetFeature Example 1: an entire layer The map of regions of Belgium.\nwfs_regions \u0026lt;- \u0026#34;https://eservices.minfin.fgov.be/arcgis/services/R2C/Regions/MapServer/WFSServer\u0026#34; url \u0026lt;- parse_url(wfs_regions) url$query \u0026lt;- list(service = \u0026#34;WFS\u0026#34;, version = \u0026#34;2.0.0\u0026#34;, request = \u0026#34;GetCapabilities\u0026#34;) request \u0026lt;- build_url(url) url \u0026lt;- parse_url(wfs_regions) url$query \u0026lt;- list(service = \u0026#34;wfs\u0026#34;, version = \u0026#34;2.0.0\u0026#34;, request = \u0026#34;GetFeature\u0026#34;, typename = \u0026#34;regions\u0026#34;, srsName = \u0026#34;EPSG:4326\u0026#34;, outputFormat = \u0026#34;GEOJSON\u0026#34;) request \u0026lt;- build_url(url) bel_regions \u0026lt;- read_sf(request) #Lambert2008 ggplot(bel_regions) + geom_sf() Example 2: restrict to a bounding box This examples illustrates how you can read or download information from a WFS for further use in R.\nLet’s start by reading in the “BWK:Bwkhab” layer for the Hallerbos area.\nThe main part is defining the input and output. We store the data in GeoJSON format which is an open standard format designed for representing simple geographical features, along with their non-spatial attributes. It is based on JSON, the JavaScript Object Notation.\nWe also add the bounding box from which we want to retrieve the data. This is very important to add. If you omit a bounding box, the service will return the entire map which can be very large.\nurl \u0026lt;- parse_url(wfs_bwk) url$query \u0026lt;- list(service = \u0026#34;WFS\u0026#34;, version = \u0026#34;2.0.0\u0026#34;, request = \u0026#34;GetFeature\u0026#34;, typename = \u0026#34;BWK:Bwkhab\u0026#34;, bbox = \u0026#34;142600,153800,146000,156900\u0026#34;, outputFormat = \u0026#34;application/json\u0026#34;) request \u0026lt;- build_url(url) Let’s check what we are about to read with sf::st_layers(). This time, the function does return useful information, but the layer name BWK:Bwkhab seems to be overwritten.\nst_layers(request) ## Driver: GeoJSON ## Available layers: ## layer_name geometry_type features fields ## 1 OGRGeoJSON Polygon 583 33  bwk_hallerbos \u0026lt;- read_sf(request) Note that the layer = ... argument of sf:read_sf() cannot be used to specify the layer when you pass a WFS GetFeature request to the function.\nLet’s make a simple plot of the object. Note that the object contains features outside of the bounding box. Those are features which have only some part within the bounding box.\nggplot(bwk_hallerbos) + geom_sf() You can use sf::st_write() to save this layer in any format that is listed by sf::st_drivers().\nContinuing from the same request we got earlier, we can download the data with httr::GET and httr::write_disk().\nfile \u0026lt;- tempfile(fileext = \u0026#34;.geojson\u0026#34;) GET(url = request, write_disk(file)) ## Response [https://geoservices.informatievlaanderen.be/overdrachtdiensten/BWK/wfs?service=WFS\u0026amp;version=2.0.0\u0026amp;request=GetFeature\u0026amp;typename=BWK%3ABwkhab\u0026amp;bbox=142600%2C153800%2C146000%2C156900\u0026amp;outputFormat=application%2Fjson] ## Date: 2020-06-17 14:25 ## Status: 200 ## Content-Type: application/json;charset=UTF-8 ## Size: 879 kB ## \u0026lt;ON DISK\u0026gt; C:\\Users\\HANS_V~1\\AppData\\Local\\Temp\\Rtmp6lClEJ\\file263c7b7b50ab.geojson  At this point, all features are downloaded and can be used in R as we would we any other local file. So we need to load the file with read_sf() from sf.\nbwk_hallerbos2 \u0026lt;- read_sf(file) Example 3: extract feature data at particular points In some situations, we do not need the spatial features (polygons, lines, points), but are interested in the data at a particular point (i.e. attribute table data) of the spatial feature. When working in a local GIS environment, one would use a spatial operator to extract the data (e.g. within, intersects, contains,…). As we discussed earlier, WFS supports certain spatial operators as part of the service to directly query this data and overcomes the need to download the spatial feature data first.\nConsider the following use case: You want to extract the attribute data from a soil map for a number of sampling points (point coordinates). This use case can be tackled by relying on the WFS service and the affiliated spatial operators.\nOur example data point (in Lambert 72):\nx_lam \u0026lt;- 173995.67 y_lam \u0026lt;- 212093.44 From this point we know the data, so we can verify the result (in Dutch):\n Bodemtype: s-Pgp3(v) Bodemserie: Pgp Textuurklasse: licht zandleem Drainageklasse: uiterst nat, gereduceerd  Hence, we now want to extract these soil properties from the WFS, for the coordinates defined above.\nproperties_of_interest \u0026lt;- c(\u0026#34;Drainageklasse\u0026#34;, \u0026#34;Textuurklasse\u0026#34;, \u0026#34;Bodemserie\u0026#34;, \u0026#34;Bodemtype\u0026#34;) The URL of the wfs service of the soil map of the Flemish region:\nwfs_bodemtypes \u0026lt;- \u0026#34;https://www.dov.vlaanderen.be/geoserver/bodemkaart/bodemtypes/wfs?\u0026#34; The essential part is to set up the proper query! The required data for the service is defined in the metadata description. This can look a bit overwhelming at the start, but is a matter of looking for some specific elements of the (XML) document:\n service (WFS), request (GetFeature) and version (1.1.0) are mandatory fields (see below) typeName: Look at the different \u0026lt;FeatureType... enlisted and pick the \u0026lt;Name\u0026gt; of the one you’re interested in. In this particular case bodemkaart:bodemtypes is the only one available. outputFormat: The supported output formats are enlisted in \u0026lt;ows:Parameter name=\u0026quot;outputFormat\u0026quot;\u0026gt;. As the service provides CSV as output, this is a straightforward option. json is another popular one. propertyname: A list of the attribute table fields (cfr. supra). A full list of the Flanders soil map is provided here. We also define the CRS, using the EPSG code. CQL_FILTER: Define the spatial operator, in this case INTERSECTS of the WFS geom and our POINT coordinate. The operators are enlisted in the \u0026lt;ogc:SpatialOperators\u0026gt; field.  Formatting all this information in a query and executing the request (GET) towards the service:\nquery \u0026lt;- list(service = \u0026#34;WFS\u0026#34;, request = \u0026#34;GetFeature\u0026#34;, version = \u0026#34;1.1.0\u0026#34;, typeName = \u0026#34;bodemkaart:bodemtypes\u0026#34;, outputFormat = \u0026#34;csv\u0026#34;, propertyname = as.character(paste(properties_of_interest, collapse = \u0026#34;,\u0026#34;)), CRS = \u0026#34;EPSG:31370\u0026#34;, CQL_FILTER = sprintf(\u0026#34;INTERSECTS(geom,POINT(%s %s))\u0026#34;, x_lam, y_lam)) result \u0026lt;- GET(wfs_bodemtypes, query = query) result ## Response [https://www.dov.vlaanderen.be/geoserver/bodemkaart/bodemtypes/wfs?service=WFS\u0026amp;request=GetFeature\u0026amp;version=1.1.0\u0026amp;typeName=bodemkaart%3Abodemtypes\u0026amp;outputFormat=csv\u0026amp;propertyname=Drainageklasse%2CTextuurklasse%2CBodemserie%2CBodemtype\u0026amp;CRS=EPSG%3A31370\u0026amp;CQL_FILTER=INTERSECTS%28geom%2CPOINT%28173995.67%20212093.44%29%29] ## Date: 2020-06-17 14:25 ## Status: 200 ## Content-Type: text/csv;charset=UTF-8 ## Size: 129 B ## FID,Bodemtype,Bodemserie,Textuurklasse,Drainageklasse ## bodemtypes.72727,s-Pgp3(v),Pgp,licht zandleem,\u0026quot;uiterst nat, gereduceerd\u0026quot;  The result is not yet formatted to be used as a dataframe. We need to use a small trick using the textConnection function to get from the result (bits) towards a readable output in a dataframe:\ndf \u0026lt;- read.csv(textConnection(content(result, \u0026#39;text\u0026#39;))) knitr::kable(df)    FID Bodemtype Bodemserie Textuurklasse Drainageklasse     bodemtypes.72727 s-Pgp3(v) Pgp licht zandleem uiterst nat, gereduceerd    Which indeed corresponds to the data of the coordinate.\nThis procedure can also be turned into a function with lat, long and properties of interest as parameters.\nReferences Lovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2020. Geocomputation with R. https://geocompr.robinlovelace.net.\n","href":"/tutorials/tutorials/spatial_wfs_services/","title":"Using WFS service in R"},{"content":"","href":"/tutorials/tags/webservice/","title":"webservice"},{"content":"","href":"/tutorials/tags/git/","title":"git"},{"content":"","href":"/tutorials/tags/github/","title":"github"},{"content":"Naming   Use lowercase for repository, directory, and file names. For R-related files, use uppercase R.\n https://github.com/inbo/data-publication ../tutorials/gis/leaflet-R.Rmd    Use dash (-) to separate words in directory and file names. Don't use underscores.\n .../datasets/bird-tracking-gull-occurrences/mapping/dwc-occurrence.sql    Avoid the use of dash (-) in the name of repositories that you intend for R package development (it is OK to use a dash in repository names for other purposes). Especially if you intend to publish the package at CRAN at some point. CRAN demands package names to comply with the following: \u0026ldquo;contain only (ASCII) letters, numbers and dot, have at least two characters and start with a letter and not end in a dot\u0026rdquo;. Few packages include a dot in their name, so avoid this too.\n  READMEs ","href":"/tutorials/tutorials/styleguide_repos/","title":"Styleguide new git repositories"},{"content":"","href":"/tutorials/tags/database/","title":"database"},{"content":"","href":"/tutorials/categories/databases/","title":"databases"},{"content":"","href":"/tutorials/tags/queries/","title":"queries"},{"content":"Introduction The Flemish vegetation database, INBOVEG, is an application developed to provide a repository of relevés and makes the relevés available for future use.\nINBOVEG supports different types of recordings: BioHab recordings (protocol of Natura 2000 monitoring) and the classic relevés. The classic relevés can stand alone, be an element of a collection or element of a chain where the linkage is used to give information about the relative position of recording within a series. Ample selection and export functions toward analysis tools are provided. It also provides standardized lists of species, habitats, life forms, scales etc. Original observations are preserved and a full history of subsequent identifications is saved.\nAim In this tutorial we make functions available to query data directly from the INBOVEG SQL-server database. This to avoid writing your own queries or to copy/paste them from the access-frontend for INBOVEG.\nWe have provided functions to query\n  survey (INBOVEG-projects)\n  recordings (vegetation relevés)\n  metadata of recordings (header info)\n  classification (Natura2000 or local classification like BWK)\n  qualifiers (management and site characteristics)\n  Packages and connection In order to run the functionalities, some R packags need to be installed.\nThe main functions that we will use in this tutorial all start with inboveg_*. These functions are made available by loading the inbodb package.\nYou can install inbodb from github with:\ninstall.packages(\u0026quot;remotes\u0026quot;) remotes::install_github(\u0026quot;inbo/inbodb\u0026quot;)  This tutorial will only work for people with access to the INBO network. As an INBO employee, you should make sure you have reading-rights for CYDONIA, otherwise place an ICT-call.\nlibrary(glue) library(DBI) library(assertthat) library(dplyr) library(inbodb)  The following R-code can be used to establish a connection to INBOVEG by using ‘connect_inbo_dbase’ of the inbodb-package with the database ‘Cydonia’ on the inbo-sql07-prd server:\ncon \u0026lt;- connect_inbo_dbase(\u0026quot;D0010_00_Cydonia\u0026quot;)  Functionality Survey information The function get_inboveg_survey queries the INBOVEG database for survey information (metadata about surveys) for one or more survey(s) by the name of the survey.\n###Examples\nThree examples are given, this can be used as base to continue selecting the wanted data.\nGet information of a specific survey and collect data.\nsurvey_info \u0026lt;- get_inboveg_survey(con, survey_name = \u0026quot;OudeLanden_1979\u0026quot;, collect = TRUE) survey_info ## # A tibble: 1 x 5 ## Id Name Description Owner creator ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 172 OudeLanden~ Verlinden A, Leys G en Slembrouck J (1979) G~ \u0026lt;NA\u0026gt; els_deb~  Get information of all surveys. This time we will not use collect = TRUE, which will return a lazy query:\nallsurveys \u0026lt;- get_inboveg_survey(con) allsurveys ## # Source: SQL [?? x 5] ## # Database: Microsoft SQL Server ## # 13.00.5598[INBO\\els_lommelen@INBO-SQL07-PRD\\LIVE/D0010_00_Cydonia] ## Id Name Description Owner creator ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 ZLB Opnamen Zandleembrabant en omgev~ Gisèle W~ luc_vanher~ ## 2 2 Sigma_Biohab_2~ Biohab opnames in Sigmagebieden~ Wim Mert~ wim_mertens ## 3 3 Sigma_LSVI_2012 Perceelsopnamen van volledige so~ wim mert~ wim_mertens ## 4 4 MILKLIM_Alopec~ Standplaatsonderzoek graslanden ~ MILKLIM maud_raman ## 5 5 MILKLIM_WZ_Aal~ Opnamen van PQ's in regio Aalst ~ MILKLIM floris_van~ ## 6 6 MILKLIM_Hei(sc~ PQ's in kader van onderzoek naar~ MILKLIM floris_van~ ## 7 7 MILKLIM_Heide Standplaatsonderzoek heide MILKLIM jan_wouters ## 8 8 MILKLIM_W\u0026amp;Z_Le~ Evaluatie maaibeheer Leiebermen ~ MILKLIM maud_raman ## 9 9 MILKLIM_W\u0026amp;Z_Ge~ Vegetatieopnames in het kader va~ MILKLIM luc_vanher~ ## 10 10 MILKLIM_Heisch~ Vegetatieopnames in het kader va~ MILKLIM cecile_herr ## # ... with more rows  If only a part of the survey name is known, you can make use of wildcards such as %.\npartsurveys \u0026lt;- get_inboveg_survey(con, survey_name = \u0026quot;%MILKLIM%\u0026quot;, collect = TRUE) head(partsurveys, 10) ## # A tibble: 10 x 5 ## Id Name Description Owner creator ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 4 MILKLIM_Alopecur~ Standplaatsonderzoek graslanden b~ MILKL~ maud_raman ## 2 5 MILKLIM_WZ_Aalst~ Opnamen van PQ's in regio Aalst e~ MILKL~ floris_van~ ## 3 6 MILKLIM_Hei(schr~ PQ's in kader van onderzoek naar ~ MILKL~ floris_van~ ## 4 7 MILKLIM_Heide Standplaatsonderzoek heide MILKL~ jan_wouters ## 5 8 MILKLIM_W\u0026amp;Z_Leie~ Evaluatie maaibeheer Leiebermen g~ MILKL~ maud_raman ## 6 9 MILKLIM_W\u0026amp;Z_Gera~ Vegetatieopnames in het kader van~ MILKL~ luc_vanher~ ## 7 10 MILKLIM_Heischra~ Vegetatieopnames in het kader van~ MILKL~ cecile_herr ## 8 11 MILKLIM_W\u0026amp;Z_Varia Losse opnamen in het kader van kl~ MILKL~ floris_van~ ## 9 12 MILKLIM_W\u0026amp;Z_Berm~ Ecologische opvolging van bermveg~ MILKL~ els_debie ## 10 14 MILKLIM_W\u0026amp;Z_Oeve~ Oeveropnamen langs de Leie ter ev~ Maud ~ luc_vanher~  Recording information The function get_inboveg_recordings queries the INBOVEG database for relevé information (which species were recorded in which plots and in which vegetation layers with which cover) for one or more surveys.\nExamples Four examples are given, this can be used as base to continue selecting the wanted data.\nGet the relevés from one survey and collect the data\nrecording_heischraal2012 \u0026lt;- get_inboveg_recordings(con, survey_name = \u0026quot;MILKLIM_Heischraal2012\u0026quot;, collect = TRUE) head(recording_heischraal2012, 10) ## # A tibble: 10 x 10 ## Name RecordingGivid LayerCode CoverCode OriginalName ScientificName ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 MILK~ IV20120816113~ M 5 Rhytidiadel~ Rhytidiadelph~ ## 2 MILK~ IV20120816113~ M 5 Pseudoscler~ Pseudosclerop~ ## 3 MILK~ IV20120816113~ K 90 Juncus acut~ Juncus acutif~ ## 4 MILK~ IV20120816113~ K 90 Nardus stri~ Nardus strict~ ## 5 MILK~ IV20120816113~ K 90 Potentilla ~ Potentilla er~ ## 6 MILK~ IV20120816113~ K 90 Anthoxanthu~ Anthoxanthum ~ ## 7 MILK~ IV20120816113~ K 90 Molinia cae~ Molinia caeru~ ## 8 MILK~ IV20120816113~ K 90 Lysimachia ~ Lysimachia vu~ ## 9 MILK~ IV20120816113~ K 90 Luzula mult~ Luzula multif~ ## 10 MILK~ IV20120816113~ K 90 Carex pilul~ Carex pilulif~ ## # ... with 4 more variables: PhenologyCode \u0026lt;chr\u0026gt;, CoverageCode \u0026lt;chr\u0026gt;, ## # PctValue \u0026lt;dbl\u0026gt;, RecordingScale \u0026lt;chr\u0026gt;  Get all recordings from MILKLIM surveys (partial matching), don’t collect\nrecording_milkim \u0026lt;- get_inboveg_recordings(con, survey_name = \u0026quot;%MILKLIM%\u0026quot;) recording_milkim ## # Source: SQL [?? x 10] ## # Database: Microsoft SQL Server ## # 13.00.5598[INBO\\els_lommelen@INBO-SQL07-PRD\\LIVE/D0010_00_Cydonia] ## Name RecordingGivid LayerCode CoverCode OriginalName ScientificName ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 MILK~ IV20120803144~ KH 99 Anthoxanthu~ Anthoxanthum ~ ## 2 MILK~ IV20120803144~ KH 99 Glechoma he~ Glechoma hede~ ## 3 MILK~ IV20120803144~ KH 99 Heracleum s~ Heracleum sph~ ## 4 MILK~ IV20120803144~ KH 99 Agrostis ca~ Agrostis capi~ ## 5 MILK~ IV20120803144~ KH 99 Trifolium r~ Trifolium rep~ ## 6 MILK~ IV20120803144~ KH 99 Veronica ar~ Veronica arve~ ## 7 MILK~ IV20120803144~ KH 99 Bromus moll~ Bromus hordea~ ## 8 MILK~ IV20120803144~ KH 99 Festuca rub~ Festuca rubra~ ## 9 MILK~ IV20120803144~ KH 99 Filipendula~ Filipendula u~ ## 10 MILK~ IV20120803144~ KH 99 Veronica se~ Veronica serp~ ## # ... with more rows, and 4 more variables: PhenologyCode \u0026lt;chr\u0026gt;, ## # CoverageCode \u0026lt;chr\u0026gt;, PctValue \u0026lt;dbl\u0026gt;, RecordingScale \u0026lt;chr\u0026gt;  Get recordings from several specific surveys\nrecording_severalsurveys \u0026lt;- get_inboveg_recordings(con, survey_name = c(\u0026quot;MILKLIM_Heischraal2012\u0026quot;, \u0026quot;NICHE Vlaanderen\u0026quot;), multiple = TRUE, collect = TRUE) head(recording_severalsurveys, 10) ## # A tibble: 10 x 10 ## Name RecordingGivid LayerCode CoverCode OriginalName ScientificName ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 MILK~ IV20120816113~ M 5 Rhytidiadel~ Rhytidiadelph~ ## 2 MILK~ IV20120816113~ M 5 Pseudoscler~ Pseudosclerop~ ## 3 MILK~ IV20120816113~ K 90 Juncus acut~ Juncus acutif~ ## 4 MILK~ IV20120816113~ K 90 Nardus stri~ Nardus strict~ ## 5 MILK~ IV20120816113~ K 90 Potentilla ~ Potentilla er~ ## 6 MILK~ IV20120816113~ K 90 Anthoxanthu~ Anthoxanthum ~ ## 7 MILK~ IV20120816113~ K 90 Molinia cae~ Molinia caeru~ ## 8 MILK~ IV20120816113~ K 90 Lysimachia ~ Lysimachia vu~ ## 9 MILK~ IV20120816113~ K 90 Luzula mult~ Luzula multif~ ## 10 MILK~ IV20120816113~ K 90 Carex pilul~ Carex pilulif~ ## # ... with 4 more variables: PhenologyCode \u0026lt;chr\u0026gt;, CoverageCode \u0026lt;chr\u0026gt;, ## # PctValue \u0026lt;dbl\u0026gt;, RecordingScale \u0026lt;chr\u0026gt;  Get all relevés of all surveys, don’t collect the data\nallrecordings \u0026lt;- get_inboveg_recordings(con) allrecordings ## # Source: SQL [?? x 10] ## # Database: Microsoft SQL Server ## # 13.00.5598[INBO\\els_lommelen@INBO-SQL07-PRD\\LIVE/D0010_00_Cydonia] ## Name RecordingGivid LayerCode CoverCode OriginalName ScientificName ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 ZLB IV20120524163~ B 75 Betula pend~ Betula pendul~ ## 2 ZLB IV20120524163~ B 75 Populus x c~ Populus x can~ ## 3 ZLB IV20120524163~ B 75 Prunus aviu~ Prunus avium ~ ## 4 ZLB IV20120524163~ B 75 Fagus sylva~ Fagus sylvati~ ## 5 ZLB IV20120524163~ B 75 Acer campes~ Acer campestr~ ## 6 ZLB IV20120524163~ K 85 Fagus sylva~ Fagus sylvati~ ## 7 ZLB IV20120524163~ K 85 Athyrium fi~ Athyrium fili~ ## 8 ZLB IV20120524163~ S 50 Corylus ave~ Corylus avell~ ## 9 ZLB IV20120524163~ S 50 Crataegus m~ Crataegus mon~ ## 10 ZLB IV20120524163~ S 50 Fraxinus ex~ Fraxinus exce~ ## # ... with more rows, and 4 more variables: PhenologyCode \u0026lt;chr\u0026gt;, ## # CoverageCode \u0026lt;chr\u0026gt;, PctValue \u0026lt;dbl\u0026gt;, RecordingScale \u0026lt;chr\u0026gt;  Header information The function get_inboveg_header queries the INBOVEG database for header information (metadata for a vegetation-relevé) for one or more survey by the name of the survey(s) and the recorder type.\n###Examples\nThree examples are given, this can be used as base to continue selecting the wanted data.\nGet header information from a specific survey and a specific recording type and collect the data:\nheader_info \u0026lt;- get_inboveg_header(con, survey_name = \u0026quot;OudeLanden_1979\u0026quot;, rec_type = \u0026quot;Classic\u0026quot;, collect = TRUE) head(header_info, 10) ## # A tibble: 10 x 15 ## RecordingGivid Name UserReference Observer LocationCode Latitude Longitude ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 IV20160210121~ Oude~ 5 Alex Ve~ Ekeren 500 500 ## 2 IV20160210140~ Oude~ 1 Alex Ve~ Ekeren 500 500 ## 3 IV20160210142~ Oude~ 2 Alex Ve~ Ekeren 500 500 ## 4 IV20160210153~ Oude~ 4 Alex Ve~ Ekeren 500 500 ## 5 IV20160210155~ Oude~ 81 Alex Ve~ Ekeren 500 500 ## 6 IV20160210160~ Oude~ 20 Alex Ve~ Ekeren 500 500 ## 7 IV20160210161~ Oude~ 17 Alex Ve~ Ekeren 500 500 ## 8 IV20160210162~ Oude~ 19 Alex Ve~ Ekeren 500 500 ## 9 IV20160210163~ Oude~ 18 Alex Ve~ Ekeren 500 500 ## 10 IV20160210164~ Oude~ 23 Alex Ve~ Ekeren 500 500 ## # ... with 8 more variables: Area \u0026lt;dbl\u0026gt;, Length \u0026lt;int\u0026gt;, Width \u0026lt;int\u0026gt;, ## # VagueDateType \u0026lt;chr\u0026gt;, VagueDateBegin \u0026lt;chr\u0026gt;, VagueDateEnd \u0026lt;chr\u0026gt;, ## # SurveyId \u0026lt;int\u0026gt;, RecTypeID \u0026lt;int\u0026gt;  Get header information from several specific surveys by using multiple\nheader_severalsurveys \u0026lt;- get_inboveg_header(con, survey_name = c(\u0026quot;MILKLIM_Heischraal2012\u0026quot;, \u0026quot;NICHE Vlaanderen\u0026quot;), multiple = TRUE) header_severalsurveys ## # Source: SQL [?? x 15] ## # Database: Microsoft SQL Server ## # 13.00.5598[INBO\\els_lommelen@INBO-SQL07-PRD\\LIVE/D0010_00_Cydonia] ## RecordingGivid Name UserReference Observer LocationCode Latitude Longitude ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 IV20120816094~ MILK~ HS_036 Cécile ~ Walenbos 51.0 4.80 ## 2 IV20120816102~ MILK~ HS_037 Cécile ~ Walenbos 51.0 4.80 ## 3 IV20120816103~ MILK~ HS_044 Cécile ~ Walenbos 51.0 4.80 ## 4 IV20120816105~ MILK~ HS_140 Cécile ~ Walenbos 51.0 4.80 ## 5 IV20120816112~ MILK~ HS_106 Cécile ~ Liereman 51.1 5.45 ## 6 IV20120816113~ MILK~ HS_107 Cécile ~ Liereman 51.1 5.45 ## 7 IV20120816113~ MILK~ HS_1001 Cécile ~ Liereman 51.1 5.45 ## 8 IV20120816124~ MILK~ HS_1003 Cécile ~ Langdonken 51.0 4.95 ## 9 IV20120816141~ MILK~ HS_1006 Cécile ~ Gulke Putten 50.0 5.02 ## 10 IV20120816143~ MILK~ HS_1007 Cécile ~ Gulke Putten 50.0 5.01 ## # ... with more rows, and 8 more variables: Area \u0026lt;dbl\u0026gt;, Length \u0026lt;int\u0026gt;, ## # Width \u0026lt;int\u0026gt;, VagueDateType \u0026lt;chr\u0026gt;, VagueDateBegin \u0026lt;chr\u0026gt;, VagueDateEnd \u0026lt;chr\u0026gt;, ## # SurveyId \u0026lt;int\u0026gt;, RecTypeID \u0026lt;int\u0026gt;  Get header information of all surveys, don’t collect the data\nall_header_info \u0026lt;- get_inboveg_header(con) all_header_info ## # Source: SQL [?? x 15] ## # Database: Microsoft SQL Server ## # 13.00.5598[INBO\\els_lommelen@INBO-SQL07-PRD\\LIVE/D0010_00_Cydonia] ## RecordingGivid Name UserReference Observer LocationCode Latitude Longitude ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 IV20120524163~ ZLB ZLB-GW-2011-~ Weyembe~ Overijse 500 500 ## 2 IV20120608135~ ZLB ZLB-GW-2011-~ Weyembe~ Overijse 500 500 ## 3 IV20120608155~ Sigm~ wm2012-0001 wim mer~ Weymeerbroek 500 500 ## 4 IV20120608161~ Sigm~ wm2012-0002 wim mer~ Weymeerbroek 500 500 ## 5 IV20120608163~ ZLB ZLB-GW-2006-~ Weyembe~ Rosières 500 500 ## 6 IV20120614155~ ZLB ZLB-GW-2005-~ Weyembe~ Rixensart 500 500 ## 7 IV20120802135~ MILK~ 130 Jan Wou~ Heist op de~ 51.1 4.75 ## 8 IV20120802143~ MILK~ 131 Jan Wou~ Heist op de~ 51.1 4.75 ## 9 IV20120802150~ MILK~ 131B Jan Wou~ Heist op de~ 51.1 4.75 ## 10 IV20120802153~ MILK~ 378 Jan Wou~ Schulens Br~ 51.0 5.17 ## # ... with more rows, and 8 more variables: Area \u0026lt;dbl\u0026gt;, Length \u0026lt;int\u0026gt;, ## # Width \u0026lt;int\u0026gt;, VagueDateType \u0026lt;chr\u0026gt;, VagueDateBegin \u0026lt;chr\u0026gt;, VagueDateEnd \u0026lt;chr\u0026gt;, ## # SurveyId \u0026lt;int\u0026gt;, RecTypeID \u0026lt;int\u0026gt;  Classification information The function get_inboveg_classification queries the INBOVEG database for information on the field classification (N2000 or BWK-code) of the relevé for one or more survey(s) by the name of the survey.\n###Examples\nTwo examples are given, this can be used as base to continue selecting the wanted data.\nGet a specific classification from a survey and collect the data:\nclassif_info \u0026lt;- get_inboveg_classification(con, survey_name = \u0026quot;MILKLIM_Heischraal2012\u0026quot;, classif = \u0026quot;4010\u0026quot;, collect = TRUE) head(classif_info, 10) ## # A tibble: 1 x 9 ## RecordingGivid Name Classif ActionGroup ListName LocalClassifica~ Habitattype ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 IV20130318144~ MILK~ 4010 N2k Habitat~ \u0026lt;NA\u0026gt; Noord-Atla~ ## # ... with 2 more variables: Cover \u0026lt;chr\u0026gt;, PctValue \u0026lt;dbl\u0026gt;  Get all surveys, all classifications, don’t collect the data\nallecodes \u0026lt;- get_inboveg_classification(con) allecodes ## # Source: SQL [?? x 9] ## # Database: Microsoft SQL Server ## # 13.00.5598[INBO\\els_lommelen@INBO-SQL07-PRD\\LIVE/D0010_00_Cydonia] ## RecordingGivid Name Classif ActionGroup ListName LocalClassifica~ ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 IV20120524163~ ZLB vc BWK Ecotoop~ \u0026quot;elzen-essenbos~ ## 2 IV20120524163~ ZLB qa BWK Ecotoop~ \u0026quot;eiken-haagbeuk~ ## 3 IV20120608155~ Sigm~ kd BWK Ecotoop~ \u0026quot;dijk \u0026quot; ## 4 IV20120608161~ Sigm~ kbj BWK Ecotoop~ \u0026quot;bomenrij met d~ ## 5 IV20120608163~ ZLB vm BWK Ecotoop~ \u0026quot;mesotroof elze~ ## 6 IV20120608135~ ZLB mk BWK Ecotoop~ \u0026quot;alkalisch laag~ ## 7 IV20120614155~ ZLB msb- BWK Ecotoop~ \u0026quot;zuur laagveen ~ ## 8 IV20120802135~ MILK~ k(hf) BWK Ecotoop~ \u0026quot;bermen, percee~ ## 9 IV20120802135~ MILK~ k(hc) BWK Ecotoop~ \u0026quot;bermen, percee~ ## 10 IV20120802135~ MILK~ hp+ BWK Ecotoop~ \u0026quot;soortenrijk pe~ ## # ... with more rows, and 3 more variables: Habitattype \u0026lt;chr\u0026gt;, Cover \u0026lt;chr\u0026gt;, ## # PctValue \u0026lt;dbl\u0026gt;  Qualifiers information This function get_inboveg_qualifiersqueries the INBOVEG database for qualifier information on recordings for one or more surveys. These qualifiers give information on management (management qualifier ‘MQ’) or location description (site qualifier’SQ’).\n###Examples\nFour examples are given, this can be used as base to continue selecting the wanted data.\nGet the qualifiers from one survey\nqualifiers_heischraal2012 \u0026lt;- get_inboveg_qualifiers(con, survey_name = \u0026quot;MILKLIM_Heischraal2012\u0026quot;) head(qualifiers_heischraal2012, 10) ## Name RecordingGivid UserReference ## 1 MILKLIM_Heischraal2012 IV2012081615083167 HS_008 ## 2 MILKLIM_Heischraal2012 IV2012081613133274 HS_009 ## 3 MILKLIM_Heischraal2012 IV2013041614525228 HS_035 ## 4 MILKLIM_Heischraal2012 IV2012081609450300 HS_036 ## 5 MILKLIM_Heischraal2012 IV2012081610204607 HS_037 ## 6 MILKLIM_Heischraal2012 IV2012081610393743 HS_044 ## 7 MILKLIM_Heischraal2012 IV2012081712451811 HS_052 ## 8 MILKLIM_Heischraal2012 IV2012081611565583 HS_060 ## 9 MILKLIM_Heischraal2012 IV2012081612200087 HS_061 ## 10 MILKLIM_Heischraal2012 IV2012081611445288 HS_063 ## Observer QualifierType Q1Code ## 1 Cécile Herr en Robin Guelinckx MQ A ## 2 Cécile Herr en Robin Guelinckx MQ A ## 3 Cécile Herr, Patrik Oosterlynck, Robin Guelinckx MQ A ## 4 Cécile Herr en Robin Guelinckx MQ A ## 5 Cécile Herr, Robin Guelinckx, Patrik Oosterlynck MQ A ## 6 Cécile Herr en Robin Guelinckx MQ A ## 7 Cécile Herr MQ A ## 8 Cécile Herr MQ A ## 9 Cécile Herr MQ A ## 10 Cécile Herr MQ A ## Q1Description Q2Code Q2Description Q3Code Q3Description Elucidation ## 1 \u0026lt;NA\u0026gt; PBuis Peilbuizen GUPP042A \u0026lt;NA\u0026gt; ## 2 \u0026lt;NA\u0026gt; PBuis Peilbuizen GUPP043B \u0026lt;NA\u0026gt; ## 3 Active PBuis Peilbuizen WALP161X \u0026lt;NA\u0026gt; ## 4 \u0026lt;NA\u0026gt; PBuis Peilbuizen WALP157X \u0026lt;NA\u0026gt; ## 5 \u0026lt;NA\u0026gt; PBuis Peilbuizen WALP117X \u0026lt;NA\u0026gt; ## 6 \u0026lt;NA\u0026gt; PBuis Peilbuizen WALP162X \u0026lt;NA\u0026gt; ## 7 \u0026lt;NA\u0026gt; PBuis Peilbuizen VOTP017X \u0026lt;NA\u0026gt; ## 8 \u0026lt;NA\u0026gt; PBuis Peilbuizen LDOP014B \u0026lt;NA\u0026gt; ## 9 \u0026lt;NA\u0026gt; PBuis Peilbuizen LDOP001X \u0026lt;NA\u0026gt; ## 10 \u0026lt;NA\u0026gt; PBuis Peilbuizen LDOP006D \u0026lt;NA\u0026gt; ## NotSure ParentID QualifierResource ## 1 0 NA \u0026lt;NA\u0026gt; ## 2 0 NA \u0026lt;NA\u0026gt; ## 3 0 NA RS2012060811060080 ## 4 0 NA \u0026lt;NA\u0026gt; ## 5 0 NA \u0026lt;NA\u0026gt; ## 6 0 NA \u0026lt;NA\u0026gt; ## 7 0 NA \u0026lt;NA\u0026gt; ## 8 0 NA \u0026lt;NA\u0026gt; ## 9 0 NA \u0026lt;NA\u0026gt; ## 10 0 NA \u0026lt;NA\u0026gt;  Get all site qualifiers (SQ) from MILKLIM surveys (partial matching):\nqualifiers_milkim \u0026lt;- get_inboveg_qualifiers(con, survey_name = \u0026quot;%MILKLIM%\u0026quot;, qualifier_type = \u0026quot;SQ\u0026quot;) head(qualifiers_milkim, 10) ## Name RecordingGivid UserReference ## 1 MILKLIM_LevelII_BraunBlanquet IV2013082613200562 001_LevelII ## 2 MILKLIM_LevelII_Londo IV2013082711054782 001_LevelII ## 3 MILKLIM_LevelII_BraunBlanquet IV2013082613220113 002_LevelII ## 4 MILKLIM_LevelII_Londo IV2013082711080272 002_LevelII ## 5 MILKLIM_LevelII_BraunBlanquet IV2013082613231173 003_LevelII ## 6 MILKLIM_LevelII_Londo IV2013082711091698 003_LevelII ## 7 MILKLIM_LevelII_BraunBlanquet IV2013082613251752 004_LevelII ## 8 MILKLIM_LevelII_Londo IV2013082711105205 004_LevelII ## 9 MILKLIM_LevelII_BraunBlanquet IV2013082613300967 005_LevelII ## 10 MILKLIM_LevelII_Londo IV2013082711122731 005_LevelII ## Observer QualifierType Q1Code Q1Description Q2Code Q2Description ## 1 Onbekend SQ 11.01 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 2 Onbekend SQ 11.1 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 3 Onbekend SQ 11.02 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 4 Onbekend SQ 11.2 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 5 Onbekend SQ 11.03 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 6 Johan Neirynck SQ 11.3 1000-2 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 7 Onbekend SQ 11.04 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 8 Johan Neirynck SQ 11.4 1000-1 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 9 Onbekend SQ 11.05 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 10 Johan Neirynck SQ 11.5 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## Q3Code Q3Description Elucidation NotSure ParentID QualifierResource ## 1 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 NA RS2013082612251090 ## 2 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 NA RS2013082612251090 ## 3 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 NA RS2013082612251090 ## 4 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 NA RS2013082612251090 ## 5 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 NA RS2013082612251090 ## 6 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 NA RS2013082612251090 ## 7 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 NA RS2013082612251090 ## 8 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 NA RS2013082612251090 ## 9 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 NA RS2013082612251090 ## 10 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 NA RS2013082612251090  Get qualifiers from several specific surveys\nqualifiers_severalsurveys \u0026lt;- get_inboveg_qualifiers(con, survey_name = c(\u0026quot;MILKLIM_Heischraal2012\u0026quot;, \u0026quot;NICHE Vlaanderen\u0026quot;), multiple = TRUE) head(qualifiers_severalsurveys, 10) ## Name RecordingGivid UserReference Observer QualifierType ## 1 NICHE Vlaanderen IV2013091209383088 1 . 3 O Els De Bie SQ ## 2 NICHE Vlaanderen IV2013091209230447 1 . 5 O Els De Bie SQ ## 3 NICHE Vlaanderen IV2013091208585526 1 . 7 D Els De Bie SQ ## 4 NICHE Vlaanderen IV2013091016243864 1.8 O Els De Bie SQ ## 5 NICHE Vlaanderen IV2013101009450362 15 Wim Mertens SQ ## 6 NICHE Vlaanderen IV2013101010054825 19 Wim Mertens SQ ## 7 NICHE Vlaanderen IV2013092409234825 2.1 O Els De Bie SQ ## 8 NICHE Vlaanderen IV2013092415280064 2.7 Els De Bie SQ ## 9 NICHE Vlaanderen IV2013092415561030 2.8 Els De Bie SQ ## 10 NICHE Vlaanderen IV2013092415460492 2.8 bis Els De Bie SQ ## Q1Code Q1Description Q2Code Q2Description Q3Code Q3Description Elucidation ## 1 OLEP003X \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 2 OLEP005X \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 3 OLEP107X \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 4 OLEP108X \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 5 VIEP102X \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 6 VIEP151X \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 7 OLEP009X \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 8 LIEP014X \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 9 LIEP015X \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 10 LIEP015X \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## NotSure ParentID QualifierResource ## 1 0 NA RS2012080211350655 ## 2 0 NA RS2012080211350655 ## 3 0 NA RS2012080211350655 ## 4 0 NA RS2012080211350655 ## 5 0 NA RS2012080211350655 ## 6 0 NA RS2012080211350655 ## 7 0 NA RS2012080211350655 ## 8 0 NA RS2012080211350655 ## 9 0 NA RS2012080211350655 ## 10 0 NA RS2012080211350655  Get all qualifiers of all surveys\nallqualifiers \u0026lt;- get_inboveg_qualifiers(con) head(allqualifiers,10) ## Name RecordingGivid UserReference ## 1 MILKLIM_Alopecurion IV2012080609161322 0 ## 2 Ecosysteemvisie - Kalkense Meersen IV2014071014024489 001 ## 3 OudeKreken_Assenede IV2014090409172133 001 ## 4 Ecosysteemvisie - Kalkense Meersen IV2014071014024489 001 ## 5 OudeKreken_Assenede IV2014090409172133 001 ## 6 TerYde_1996 IV2014090109343376 001 ## 7 MILKLIM_LevelII_BraunBlanquet IV2013082613200562 001_LevelII ## 8 MILKLIM_LevelII_Londo IV2013082711054782 001_LevelII ## 9 LosseOpnames_IndraJacobs_Londo IV2017022410342012 001-KES ## 10 OudeKreken_Assenede IV2014090216034066 002 ## Observer QualifierType Q1Code Q1Description ## 1 Maud Raman en Arthur De Haeck MQ A Active ## 2 Leen Martens MQ 0 \u0026lt;NA\u0026gt; ## 3 Henk Coudenys MQ 0 Geen informatie ## 4 Leen Martens SQ Kalkense meersen \u0026lt;NA\u0026gt; ## 5 Henk Coudenys SQ Krekengebied \u0026lt;NA\u0026gt; ## 6 Hans Baeté SQ Ter Yde \u0026lt;NA\u0026gt; ## 7 Onbekend SQ 11.01 \u0026lt;NA\u0026gt; ## 8 Onbekend SQ 11.1 \u0026lt;NA\u0026gt; ## 9 Indra Jacobs SQ Koningssteen \u0026lt;NA\u0026gt; ## 10 Henk Coudenys MQ 0 Geen informatie ## Q2Code Q2Description Q3Code Q3Description Elucidation NotSure ParentID ## 1 PBuis Peilbuizen ASEP001X \u0026lt;NA\u0026gt; 0 NA ## 2 -9 geen peilbuis \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 NA ## 3 BEH BeheerIngrepen hooil hooiland 0 NA ## 4 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 NA ## 5 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 NA ## 6 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 NA ## 7 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 NA ## 8 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 NA ## 9 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 NA ## 10 BEH BeheerIngrepen hooil hooiland 0 NA ## QualifierResource ## 1 RS2012060811060080 ## 2 \u0026lt;NA\u0026gt; ## 3 RS2014070915553622 ## 4 RS2012080211350639 ## 5 RS2012080211350639 ## 6 RS2012080211350639 ## 7 RS2013082612251090 ## 8 RS2013082612251090 ## 9 \u0026lt;NA\u0026gt; ## 10 RS2014070915553622  More complex queries These functions give basis information out of INBOVEG. If more detailed information is needed ‘dplyr’ is the magic word. In future more complex functions can be build to help the inboveg-users.\nClosing the connection Close the connection when done\ndbDisconnect(con) rm(con)  ","href":"/tutorials/tutorials/r_inboveg/","title":"Tutorial on how to retrieve data from the INBOVEG database"},{"content":"","href":"/tutorials/categories/gis/","title":"gis"},{"content":"This tutorial uses a few basic functions from the dplyr and raster packages. While only a few functions are used, you can use the previous hyperlinks to access the tutorials (vignettes) of these packages for more functions and information.\noptions(stringsAsFactors = FALSE) library(raster) library(tidyverse) library(inborutils) You will find a bit more background about ‘why and what’, regarding the considered open standards, in a separate post on this website.\nIn short, the GeoTIFF and GeoPackage formats are ideal for exchange, publication, interoperability \u0026amp; durability and to open science in general.\nThe below table compares a few raster formats that are currently used a lot. This tutorial focuses on the open formats.\n   Property GeoTIFF GeoPackage ESRI geodatabase     Open standard? yes yes no   Write support by GDAL yes yes no   Supported OS cross-platform cross-platform Windows   Extends non-spatial format: TIFF SQLite MS Access (for personal gdb)   Text or binary? binary binary binary   Number of files 1 1 1 (personal gdb) / many (file gdb)   Inspect version’s differences in git version control? no no no   Can store multiple layers? yes yes yes   Do layers need same extent and resolution? yes no no   Coordinate reference system (CRS) in file same as input CRS same as input CRS same as input CRS    How to make and use GeoTIFF files (*.tif) Making a mono-layered GeoTIFF file from a RasterLayer R object Let’s create a small dummy RasterLayer object from scratch, for some area in Belgium (using the CRS [1] Belgian Lambert 72, i.e. EPSG-code 31370):\nartwork \u0026lt;- raster(extent(188500, 190350, 227550, 229550), # xmin, xmax, ymin, ymax res = 50, # resolution of 50 meters crs = CRS(\u0026#34;+init=epsg:31370\u0026#34;)) %\u0026gt;% setValues(runif(ncell(.))) # fill with random values What does this look like?\nartwork ## class : RasterLayer ## dimensions : 40, 37, 1480 (nrow, ncol, ncell) ## resolution : 50, 50 (x, y) ## extent : 188500, 190350, 227550, 229550 (xmin, xmax, ymin, ymax) ## crs : +init=epsg:31370 +proj=lcc +lat_1=51.16666723333333 +lat_2=49.8333339 +lat_0=90 +lon_0=4.367486666666666 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +towgs84=-106.8686,52.2978,-103.7239,0.3366,-0.457,1.8422,-1.2747 +units=m +no_defs ## source : memory ## names : layer ## values : 0.0001557397, 0.9997439 (min, max)  A simple trick to plot this raster:\nspplot(artwork) To write this RasterLayer object as a GeoTIFF, you can use the raster::writeRaster() function. In the background, it uses the GeoTIFF driver of the powerful GDAL library.\nartwork %\u0026gt;% writeRaster(\u0026#34;artwork.tif\u0026#34;) And now?\nSay HURRAY!!\nMaking a multi-layered GeoTIFF file from a RasterBrick R object Let’s create a RasterBrick object of three layers:\narts \u0026lt;- brick(artwork) # RasterBrick with one layer (the RasterLayer from above) arts[[2]] \u0026lt;- artwork + 10 # Add second layer, e.g. based on first one arts[[3]] \u0026lt;- calc(arts[[2]], function(x) {20 ^ x}) # Making third layer from second names(arts) \u0026lt;- paste0(\u0026#34;layer\u0026#34;, 1:3) Note: for complex formulas on large datasets, the calc() function is more efficient than simple algebraic expressions such as for layer2 (see ?raster::calc).\nHow does the result look like?\narts ## class : RasterBrick ## dimensions : 40, 37, 1480, 3 (nrow, ncol, ncell, nlayers) ## resolution : 50, 50 (x, y) ## extent : 188500, 190350, 227550, 229550 (xmin, xmax, ymin, ymax) ## crs : +init=epsg:31370 +proj=lcc +lat_1=51.16666723333333 +lat_2=49.8333339 +lat_0=90 +lon_0=4.367486666666666 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +towgs84=-106.8686,52.2978,-103.7239,0.3366,-0.457,1.8422,-1.2747 +units=m +no_defs ## source : memory ## names : layer1, layer2, layer3 ## min values : 1.557397e-04, 1.000016e+01, 1.024478e+13 ## max values : 9.997439e-01, 1.099974e+01, 2.046429e+14  arts %\u0026gt;% as.list %\u0026gt;% lapply(spplot) ## [[1]]  ## ## [[2]]  ## ## [[3]]  So now what?\nLet’s write it!\narts %\u0026gt;% writeRaster(\u0026#34;arts.tif\u0026#34;) But, I want to add 20 extra layers!\n(…😣😮…)\narts2 \u0026lt;- calc(artwork, function(x) {-1:-20 * x}, # first layer = -1 * artwork # second layer = -2 * artwork # .... forceapply = TRUE) names(arts2) \u0026lt;- paste0(\u0026#34;neg_layer\u0026#34;, 1:20) # adding it to arts: arts \u0026lt;- brick(list(arts, arts2)) # saving layer names for later use: mynames \u0026lt;- names(arts) nlayers(arts) ## [1] 23  names(arts) ## [1] \u0026quot;layer1\u0026quot; \u0026quot;layer2\u0026quot; \u0026quot;layer3\u0026quot; \u0026quot;neg_layer1\u0026quot; \u0026quot;neg_layer2\u0026quot; ## [6] \u0026quot;neg_layer3\u0026quot; \u0026quot;neg_layer4\u0026quot; \u0026quot;neg_layer5\u0026quot; \u0026quot;neg_layer6\u0026quot; \u0026quot;neg_layer7\u0026quot; ## [11] \u0026quot;neg_layer8\u0026quot; \u0026quot;neg_layer9\u0026quot; \u0026quot;neg_layer10\u0026quot; \u0026quot;neg_layer11\u0026quot; \u0026quot;neg_layer12\u0026quot; ## [16] \u0026quot;neg_layer13\u0026quot; \u0026quot;neg_layer14\u0026quot; \u0026quot;neg_layer15\u0026quot; \u0026quot;neg_layer16\u0026quot; \u0026quot;neg_layer17\u0026quot; ## [21] \u0026quot;neg_layer18\u0026quot; \u0026quot;neg_layer19\u0026quot; \u0026quot;neg_layer20\u0026quot;  Overwrite the earlier written file:\narts %\u0026gt;% writeRaster(\u0026#34;arts.tif\u0026#34;, overwrite = TRUE) That’s about it!\nReading a GeoTIFF file Nothing can be more simple…\nReading a mono-layered GeoTIFF file with raster() gives back the RasterLayer:\nartwork_test \u0026lt;- raster(\u0026#34;artwork.tif\u0026#34;) artwork_test ## class : RasterLayer ## dimensions : 40, 37, 1480 (nrow, ncol, ncell) ## resolution : 50, 50 (x, y) ## extent : 188500, 190350, 227550, 229550 (xmin, xmax, ymin, ymax) ## crs : +proj=lcc +lat_1=51.16666723333333 +lat_2=49.8333339 +lat_0=90 +lon_0=4.367486666666666 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +towgs84=-106.8686,52.2978,-103.7239,0.3366,-0.457,1.8422,-1.2747 +units=m +no_defs ## source : /media/floris/DATA/git_repositories/tutorials/content/tutorials/spatial_standards_raster/artwork.tif ## names : artwork ## values : 0.0001557397, 0.9997439 (min, max)  Reading a multi-layered GeoTIFF file with brick() returns the RasterBrick:\narts_test \u0026lt;- brick(\u0026#34;arts.tif\u0026#34;) However:\nnames(arts_test) ## [1] \u0026quot;arts.1\u0026quot; \u0026quot;arts.2\u0026quot; \u0026quot;arts.3\u0026quot; \u0026quot;arts.4\u0026quot; \u0026quot;arts.5\u0026quot; \u0026quot;arts.6\u0026quot; \u0026quot;arts.7\u0026quot; ## [8] \u0026quot;arts.8\u0026quot; \u0026quot;arts.9\u0026quot; \u0026quot;arts.10\u0026quot; \u0026quot;arts.11\u0026quot; \u0026quot;arts.12\u0026quot; \u0026quot;arts.13\u0026quot; \u0026quot;arts.14\u0026quot; ## [15] \u0026quot;arts.15\u0026quot; \u0026quot;arts.16\u0026quot; \u0026quot;arts.17\u0026quot; \u0026quot;arts.18\u0026quot; \u0026quot;arts.19\u0026quot; \u0026quot;arts.20\u0026quot; \u0026quot;arts.21\u0026quot; ## [22] \u0026quot;arts.22\u0026quot; \u0026quot;arts.23\u0026quot;  As you see, layer names are not saved in the GeoTIFF. You define them in R:\nnames(arts_test) \u0026lt;- mynames arts_test ## class : RasterBrick ## dimensions : 40, 37, 1480, 23 (nrow, ncol, ncell, nlayers) ## resolution : 50, 50 (x, y) ## extent : 188500, 190350, 227550, 229550 (xmin, xmax, ymin, ymax) ## crs : +proj=lcc +lat_1=51.16666723333333 +lat_2=49.8333339 +lat_0=90 +lon_0=4.367486666666666 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +towgs84=-106.8686,52.2978,-103.7239,0.3366,-0.457,1.8422,-1.2747 +units=m +no_defs ## source : /media/floris/DATA/git_repositories/tutorials/content/tutorials/spatial_standards_raster/arts.tif ## names : layer1, layer2, layer3, neg_layer1, neg_layer2, neg_layer3, neg_layer4, neg_layer5, neg_layer6, neg_layer7, neg_layer8, neg_layer9, neg_layer10, neg_layer11, neg_layer12, ... ## min values : 1.557397e-04, 1.000016e+01, 1.024478e+13, -9.997439e-01, -1.999488e+00, -2.999232e+00, -3.998976e+00, -4.998719e+00, -5.998463e+00, -6.998207e+00, -7.997951e+00, -8.997695e+00, -9.997439e+00, -1.099718e+01, -1.199693e+01, ... ## max values : 9.997439e-01, 1.099974e+01, 2.046429e+14, -1.557397e-04, -3.114794e-04, -4.672192e-04, -6.229589e-04, -7.786986e-04, -9.344383e-04, -1.090178e-03, -1.245918e-03, -1.401657e-03, -1.557397e-03, -1.713137e-03, -1.868877e-03, ...  That’s what we wanted!\nThe actual data are not loaded into memory, but read in chunks when performing operations. This makes it convenient when using larger rasters:\ninMemory(arts_test) ## [1] FALSE  Selecting a specific layer by its name:\narts_test$neg_layer20 ## class : RasterLayer ## band : 23 (of 23 bands) ## dimensions : 40, 37, 1480 (nrow, ncol, ncell) ## resolution : 50, 50 (x, y) ## extent : 188500, 190350, 227550, 229550 (xmin, xmax, ymin, ymax) ## crs : +proj=lcc +lat_1=51.16666723333333 +lat_2=49.8333339 +lat_0=90 +lon_0=4.367486666666666 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +towgs84=-106.8686,52.2978,-103.7239,0.3366,-0.457,1.8422,-1.2747 +units=m +no_defs ## source : /media/floris/DATA/git_repositories/tutorials/content/tutorials/spatial_standards_raster/arts.tif ## names : neg_layer20 ## values : -19.99488, -0.003114794 (min, max)  How to make and use GeoPackages with raster layers (*.gpkg) ‘GeoPackage’ may sound new and unfamiliar to you – more information can be found in a separate post on this website.\nWhile its vector capabilities are already beautifully supported by GDAL and the sf package (demonstrated in the other tutorial)), its raster capabilities are still less supported by GDAL and dependent applications such as the R-packages raster and stars. This is something we can expect to grow in the future.\nGDAL’s GPKG-raster driver itself is still less worked out than its drivers for GeoTIFF or GPKG-vector (note that one GPKG file can accommodate both layer types). For example, only the Byte, Int16, UInt16 and Float32 datatypes can be written by GDAL, while for GeoTIFFs these are Byte UInt16, Int16, UInt32, Int32, Float32, Float64, CInt16, CInt32, CFloat32 and CFloat64 [2].\nFrom my experience, raster GeoPackage files are smaller than GeoTIFF files in the case of larger rasters. This, and the capability to combine raster and vector layers, certainly make it worthwile to consider the GeoPackage format for rasters, if you’re not hindered by the supported data types.\nMaking a single-raster GeoPackage from a RasterLayer R object This it is no more difficult than:\nartwork %\u0026gt;% writeRaster(\u0026#34;artwork.gpkg\u0026#34;, format = \u0026#34;GPKG\u0026#34;) ## Warning in .gd_SetNoDataValue(object, ...): setting of missing value not ## supported by this driver  A bit more information on the ‘missing value’ warning can be found in GDAL’s documentation of GeoPackage raster. You should know that the raster package does not yet officially support the GeoPackage! (see ?writeFormats())\nHowever, the stars package (see further)) fully supports GDAL’s capabilities, and therefore is able to write multiple raster layers, as we will do in a minute. Anyway, raster::writeRaster already works fine for single RasterLayer objects.\nReading the GeoPackage:\nartwork_gpkg \u0026lt;- raster(\u0026#34;artwork.gpkg\u0026#34;) artwork_gpkg ## class : RasterLayer ## dimensions : 40, 37, 1480 (nrow, ncol, ncell) ## resolution : 50, 50 (x, y) ## extent : 188500, 190350, 227550, 229550 (xmin, xmax, ymin, ymax) ## crs : +proj=lcc +lat_1=51.16666723333333 +lat_2=49.8333339 +lat_0=90 +lon_0=4.367486666666666 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +towgs84=-106.8686,52.2978,-103.7239,0.3366,-0.457,1.8422,-1.2747 +units=m +no_defs ## source : /media/floris/DATA/git_repositories/tutorials/content/tutorials/spatial_standards_raster/artwork.gpkg ## names : artwork ## values : -3.4e+38, 0.9997439 (min, max)  Let’s make sure: are the data we’ve read from the GeoTIFF identical to those from the GeoPackage?\nall.equal(artwork_test[], artwork_gpkg[]) ## [1] TRUE  Yeah!\nGiven that the GPKG-support of raster is limited, we’re lucky that Edzer Pebesma – the creator of sf – has also made the amazing package stars!!\nunlink(\u0026#34;artwork.gpkg\u0026#34;) # delete gpkg; we\u0026#39;re going to create it here again Sys.setenv(GDAL_PAM_ENABLED = \u0026#34;NO\u0026#34;) # prevents an auxiliary file being written next to *.gpkg library(stars) ## Loading required package: abind ## Loading required package: sf ## Linking to GEOS 3.5.1, GDAL 2.2.2, PROJ 4.9.2  We could as well have written artwork to a GeoPackage with stars, so let’s just see what we get by converting the RasterLayer object to a stars object and then apply write_stars(), hm?\nartwork %\u0026gt;% st_as_stars %\u0026gt;% # this converts the RasterLayer to a stars object write_stars(\u0026#34;artwork.gpkg\u0026#34;, driver = \u0026#34;GPKG\u0026#34;) Reading it back with stars::read_stars(), followed by back-conversion to a RasterLayer:\nartwork_gpkg_stars \u0026lt;- read_stars(\u0026#34;artwork.gpkg\u0026#34;) %\u0026gt;% as(\u0026#34;Raster\u0026#34;) artwork_gpkg_stars ## class : RasterLayer ## dimensions : 40, 37, 1480 (nrow, ncol, ncell) ## resolution : 50, 50 (x, y) ## extent : 188500, 190350, 227550, 229550 (xmin, xmax, ymin, ymax) ## crs : +proj=lcc +lat_1=51.16666723333333 +lat_2=49.8333339 +lat_0=90 +lon_0=4.367486666666666 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +towgs84=-106.8686,52.2978,-103.7239,0.3366,-0.457,1.8422,-1.2747 +units=m +no_defs ## source : memory ## names : layer ## values : 0.0001557397, 0.9997439 (min, max)  And yes again, the data we’ve read from the GeoTIFF file are identical to those from the GeoPackage:\nall.equal(artwork_test[], artwork_gpkg_stars[]) ## [1] TRUE  That’s it!\nKnowing how to write and read with stars will help us for the multi-layer case!\nMaking a multi-raster GeoPackage Indeed, just as with vector layers, GeoPackage can accommodate multiple raster layers (or vector + raster layers).\nLet’s suppose we’d like to add layer2 (a RasterLayer) from the RasterBrick object arts.\narts$layer2 ## class : RasterLayer ## dimensions : 40, 37, 1480 (nrow, ncol, ncell) ## resolution : 50, 50 (x, y) ## extent : 188500, 190350, 227550, 229550 (xmin, xmax, ymin, ymax) ## crs : +init=epsg:31370 +proj=lcc +lat_1=51.16666723333333 +lat_2=49.8333339 +lat_0=90 +lon_0=4.367486666666666 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +towgs84=-106.8686,52.2978,-103.7239,0.3366,-0.457,1.8422,-1.2747 +units=m +no_defs ## source : memory ## names : layer2 ## values : 10.00016, 10.99974 (min, max)  Unfortunately, the raster package does not support GDAL’s options to add extra raster layers in a GPKG file:\ntry( arts$layer2 %\u0026gt;% writeRaster(\u0026#34;artwork.gpkg\u0026#34;, format = \u0026#34;GPKG\u0026#34;, options = c(\u0026#34;RASTER_TABLE=layer2\u0026#34;, \u0026#34;APPEND_SUBDATASET=YES\u0026#34;)) ) ## Error in .getGDALtransient(x, filename = filename, options = options, : ## filename exists; use overwrite=TRUE  So let’s proceed with stars!\narts$layer2 %\u0026gt;% st_as_stars %\u0026gt;% write_stars(\u0026#34;artwork.gpkg\u0026#34;, driver = \u0026#34;GPKG\u0026#34;, options = c(\u0026#34;RASTER_TABLE=layer2\u0026#34;, \u0026#34;APPEND_SUBDATASET=YES\u0026#34;)) Mind the options argument: those options are passed directly to GDAL’s GPKG-raster driver, and they’re documented at GDAL.\nOver there we read:\n RASTER_TABLE=string. Name of tile user table. By default, based on the filename (i.e. if filename is foo.gpkg, the table will be called “foo”).\nAPPEND_SUBDATASET=YES/NO: If set to YES, an existing GeoPackage will not be priorly destroyed, such as to be able to add new content to it. Defaults to NO.\n Ahaa!\nWe got no errors above, but no feedback either…\nThrilling!\nLet’s peek:\ngdalUtils::gdalinfo(\u0026#34;artwork.gpkg\u0026#34;) %\u0026gt;% cat(sep = \u0026#34;\\n\u0026#34;) ## Driver: GPKG/GeoPackage ## Files: artwork.gpkg ## Size is 512, 512 ## Coordinate System is `' ## Subdatasets: ## SUBDATASET_1_NAME=GPKG:artwork.gpkg:artwork ## SUBDATASET_1_DESC=artwork - artwork ## SUBDATASET_2_NAME=GPKG:artwork.gpkg:layer2 ## SUBDATASET_2_DESC=layer2 - layer2 ## Corner Coordinates: ## Upper Left ( 0.0, 0.0) ## Lower Left ( 0.0, 512.0) ## Upper Right ( 512.0, 0.0) ## Lower Right ( 512.0, 512.0) ## Center ( 256.0, 256.0)  Yay!\nIt’s interesting to see how the info at this level disregards CRS and extent.\nWhen we query the metadata of one sublayer, it is seen that CRS and extent are layer-specific:\ngdalUtils::gdalinfo(\u0026#34;artwork.gpkg\u0026#34;, # provide metadata of first subdataset: sd=1, # the following arguments just control formatting of the output: approx_stats = TRUE, mm = TRUE, proj4 = TRUE) %\u0026gt;% cat(sep = \u0026#34;\\n\u0026#34;) ## Driver: GPKG/GeoPackage ## Files: none associated ## Size is 37, 40 ## Coordinate System is: ## PROJCS[\u0026quot;unnamed\u0026quot;, ## GEOGCS[\u0026quot;International 1909 (Hayford)\u0026quot;, ## DATUM[\u0026quot;unknown\u0026quot;, ## SPHEROID[\u0026quot;intl\u0026quot;,6378388,297], ## TOWGS84[-106.8686,52.2978,-103.7239,0.3366,-0.457,1.8422,-1.2747]], ## PRIMEM[\u0026quot;Greenwich\u0026quot;,0], ## UNIT[\u0026quot;degree\u0026quot;,0.0174532925199433]], ## PROJECTION[\u0026quot;Lambert_Conformal_Conic_2SP\u0026quot;], ## PARAMETER[\u0026quot;standard_parallel_1\u0026quot;,51.16666723333333], ## PARAMETER[\u0026quot;standard_parallel_2\u0026quot;,49.8333339], ## PARAMETER[\u0026quot;latitude_of_origin\u0026quot;,90], ## PARAMETER[\u0026quot;central_meridian\u0026quot;,4.367486666666666], ## PARAMETER[\u0026quot;false_easting\u0026quot;,150000.013], ## PARAMETER[\u0026quot;false_northing\u0026quot;,5400088.438], ## UNIT[\u0026quot;Meter\u0026quot;,1]] ## PROJ.4 string is: ## '+proj=lcc +lat_1=51.16666723333333 +lat_2=49.8333339 +lat_0=90 +lon_0=4.367486666666666 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +towgs84=-106.8686,52.2978,-103.7239,0.3366,-0.457,1.8422,-1.2747 +units=m +no_defs ' ## Origin = (188500.000000000000000,229550.000000000000000) ## Pixel Size = (50.000000000000000,-50.000000000000000) ## Metadata: ## IDENTIFIER=artwork ## ZOOM_LEVEL=0 ## Image Structure Metadata: ## INTERLEAVE=PIXEL ## Corner Coordinates: ## Upper Left ( 188500.000, 229550.000) ( 4d55'13.29\u0026quot;E, 51d22'29.75\u0026quot;N) ## Lower Left ( 188500.000, 227550.000) ( 4d55'12.52\u0026quot;E, 51d21'25.04\u0026quot;N) ## Upper Right ( 190350.000, 229550.000) ( 4d56'48.92\u0026quot;E, 51d22'29.30\u0026quot;N) ## Lower Right ( 190350.000, 227550.000) ( 4d56'48.12\u0026quot;E, 51d21'24.59\u0026quot;N) ## Center ( 189425.000, 228550.000) ( 4d56' 0.71\u0026quot;E, 51d21'57.17\u0026quot;N) ## Band 1 Block=256x256 Type=Float32, ColorInterp=Undefined ## Computed Min/Max=0.000,1.000 ## Minimum=0.000, Maximum=1.000, Mean=0.502, StdDev=0.289 ## Metadata: ## STATISTICS_MAXIMUM=0.9997438788414 ## STATISTICS_MEAN=0.50228321918167 ## STATISTICS_MINIMUM=0.00015573971904814 ## STATISTICS_STDDEV=0.28903126880165  raster will not help us for reading the layers. But read_stars() is there to assist us!!\n# brick(\u0026#34;artwork.gpkg\u0026#34;) ## this won\u0026#39;t work... # but this will work: artwork_gpkg2 \u0026lt;- read_stars(\u0026#34;artwork.gpkg\u0026#34;, sub = \u0026#34;artwork\u0026#34;, quiet = TRUE) %\u0026gt;% as(\u0026#34;Raster\u0026#34;) artwork_gpkg2 ## class : RasterLayer ## dimensions : 40, 37, 1480 (nrow, ncol, ncell) ## resolution : 50, 50 (x, y) ## extent : 188500, 190350, 227550, 229550 (xmin, xmax, ymin, ymax) ## crs : +proj=lcc +lat_1=51.16666723333333 +lat_2=49.8333339 +lat_0=90 +lon_0=4.367486666666666 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +towgs84=-106.8686,52.2978,-103.7239,0.3366,-0.457,1.8422,-1.2747 +units=m +no_defs ## source : memory ## names : layer ## values : 0.0001557397, 0.9997439 (min, max)  Wow!\nChecking data again with GeoTIFF result:\nall.equal(artwork_test[], artwork_gpkg2[]) ## [1] TRUE  Same story for the other layer:\nread_stars(\u0026#34;artwork.gpkg\u0026#34;, sub = \u0026#34;layer2\u0026#34;, quiet = TRUE) %\u0026gt;% as(\u0026#34;Raster\u0026#34;) ## class : RasterLayer ## dimensions : 40, 37, 1480 (nrow, ncol, ncell) ## resolution : 50, 50 (x, y) ## extent : 188500, 190350, 227550, 229550 (xmin, xmax, ymin, ymax) ## crs : +proj=lcc +lat_1=51.16666723333333 +lat_2=49.8333339 +lat_0=90 +lon_0=4.367486666666666 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +towgs84=-106.8686,52.2978,-103.7239,0.3366,-0.457,1.8422,-1.2747 +units=m +no_defs ## source : memory ## names : layer ## values : 10.00016, 10.99974 (min, max)  Splendid.\nBy the way, this is how the full stars object looks like – it holds information similar to a RasterBrick:\nread_stars(\u0026#34;artwork.gpkg\u0026#34;, quiet = TRUE) ## stars object with 2 dimensions and 2 attributes ## attribute(s): ## artwork layer2 ## Min. :0.0001557 Min. :10.00 ## 1st Qu.:0.2475555 1st Qu.:10.25 ## Median :0.5052805 Median :10.51 ## Mean :0.5022832 Mean :10.50 ## 3rd Qu.:0.7533531 3rd Qu.:10.75 ## Max. :0.9997439 Max. :11.00 ## dimension(s): ## from to offset delta refsys point values ## x 1 37 188500 50 +proj=lcc +lat_1=51.16666... NA NULL [x] ## y 1 40 229550 -50 +proj=lcc +lat_1=51.16666... NA NULL [y]  Homework: further explore the amazing stars package Enter deep hyperspace and explore the stars package, which stores multidimensional hypercubes… Really, visit its website and never look (or turn?) back!\nlibrary(stars) For now, my time’s up and I’ll just demonstrate how easy it is to transform a Raster* object into a stars object:\ninterstellar \u0026lt;- arts[[1:5]] %\u0026gt;% st_as_stars() interstellar ## stars object with 3 dimensions and 1 attribute ## attribute(s): ## layer1 ## Min. :-2.000e+00 ## 1st Qu.:-1.000e+00 ## Median : 1.000e+00 ## Mean : 1.306e+13 ## 3rd Qu.: 1.100e+01 ## Max. : 2.046e+14 ## dimension(s): ## from to offset delta refsys point ## x 1 37 188500 50 +init=epsg:31370 +proj=lc... NA ## y 1 40 229550 -50 +init=epsg:31370 +proj=lc... NA ## band 1 5 NA NA NA NA ## values ## x NULL [x] ## y NULL [y] ## band layer1,...,neg_layer2  It does make sense, right?\nWhat about:\ninterstellar %\u0026gt;% split(\u0026#34;band\u0026#34;) ## stars object with 2 dimensions and 5 attributes ## attribute(s): ## layer1 layer2 layer3 neg_layer1 ## Min. :0.0001557 Min. :10.00 Min. :1.024e+13 Min. :-0.9997439 ## 1st Qu.:0.2475555 1st Qu.:10.25 1st Qu.:2.150e+13 1st Qu.:-0.7533531 ## Median :0.5052805 Median :10.51 Median :4.652e+13 Median :-0.5052805 ## Mean :0.5022832 Mean :10.50 Mean :6.529e+13 Mean :-0.5022832 ## 3rd Qu.:0.7533531 3rd Qu.:10.75 3rd Qu.:9.782e+13 3rd Qu.:-0.2475555 ## Max. :0.9997439 Max. :11.00 Max. :2.046e+14 Max. :-0.0001557 ## neg_layer2 ## Min. :-1.9994878 ## 1st Qu.:-1.5067062 ## Median :-1.0105610 ## Mean :-1.0045664 ## 3rd Qu.:-0.4951110 ## Max. :-0.0003115 ## dimension(s): ## from to offset delta refsys point values ## x 1 37 188500 50 +init=epsg:31370 +proj=lc... NA NULL [x] ## y 1 40 229550 -50 +init=epsg:31370 +proj=lc... NA NULL [y]  The stars package has a number of efficient geospatial algorithms that make it worth using, even for simple raster layers!\nAnd sure, as seen above, you can read from files with read_stars(), write to files with write_stars(), convert to Raster* objects with as(\u0026quot;Raster\u0026quot;) and backconvert with st_as_stars()!\n  CRS = coordinate reference system\n  See the GDAL datatype definitions – note that raster uses its own abbreviations: ?raster::dataType\n  ","href":"/tutorials/tutorials/spatial_standards_raster/","title":"How to use open raster file formats in R: GeoTIFF \u0026 GeoPackage"},{"content":"This tutorial uses a few basic functions from the dplyr and sf packages. While only a few functions are used, you can use the previous hyperlinks to access the tutorials (vignettes) of these packages for more functions and information.\noptions(stringsAsFactors = FALSE) library(tidyverse) library(sf) library(inborutils) You will find a bit more background about ‘why and what’, regarding the considered open standards, in a separate post on this website.\nIn short, the GeoPackage and GeoJSON formats are ideal for exchange, publication, interoperability \u0026amp; durability and to open science in general.\nThe below table compares a few vector formats that are currently used a lot. This tutorial focuses on the open formats.\n   Property GeoPackage GeoJSON RFC7946 Shapefile ESRI geodatabase     Open standard? yes yes no no   Write support by GDAL (OGR) yes yes yes no   Supported OS cross-platform cross-platform cross-platform Windows   Extends non-spatial format: SQLite JSON dBase IV MS Access (for personal gdb)   Text or binary? binary text binary binary   Number of files 1 1 3 or more 1 (personal gdb) / many (file gdb)   Inspect version’s differences in git version control? no yes (but care must be taken) no no   Can store multiple layers? yes no no yes   Multiple geometry types allowed per layer? yes yes no yes   Coordinate reference system (CRS) in file same as input CRS WGS84 same as input CRS same as input CRS    How to make and use GeoPackages (*.gpkg) Making a GeoPackage from a geospatial sf object in R As an example, we download a geospatial layer of Special Areas of Conservation in Flanders (version sac_2013-01-18) from Zenodo:\n# meeting a great function from the \u0026#39;inborutils\u0026#39; package: download_zenodo(doi = \u0026#34;10.5281/zenodo.3386815\u0026#34;) Did you know this: you can visit a website of this dataset by just prefixing the DOI [1] with doi.org/!\nThe data source is a shapefile, in this case consisting of 6 different files. Read the geospatial data into R as an sf object, and let’s just keep the essentials (though it doesn’t matter for the GeoPackage):\nsac \u0026lt;- read_sf(\u0026#34;sac.shp\u0026#34;) %\u0026gt;% select(sac_code = GEBCODE, sac_name = NAAM, subsac_code = DEELGEBIED, polygon_id = POLY_ID) Have a look at its contents by printing the object:\nsac ## Simple feature collection with 616 features and 4 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 22084.25 ymin: 153207.4 xmax: 258865 ymax: 243333 ## epsg (SRID): NA ## proj4string: +proj=lcc +lat_1=49.8333339 +lat_2=51.16666723333333 +lat_0=90 +lon_0=4.367486666666666 +x_0=150000.01256 +y_0=5400088.4378 +ellps=intl +units=m +no_defs ## # A tibble: 616 x 5 ## sac_code sac_name subsac_code polygon_id geometry ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;POLYGON [m]\u0026gt; ## 1 BE2100020 Heesbossen, V… BE2100020-4 1 ((180272.3 243198.7, 180275.… ## 2 BE2100020 Heesbossen, V… BE2100020-2 2 ((178655.5 241042.4, 178602.… ## 3 BE2100024 Vennen, heide… BE2100024-… 3 ((197492.4 234451.4, 197286.… ## 4 BE2100015 Kalmthoutse H… BE2100015-1 4 ((153735.8 228386, 153838.5 … ## 5 BE2100024 Vennen, heide… BE2100024-… 5 ((198272.4 234699, 198568.8 … ## 6 BE2100020 Heesbossen, V… BE2100020-6 6 ((181098 233705.3, 181395.1 … ## 7 BE2100024 Vennen, heide… BE2100024-… 7 ((199185.8 233540.2, 199122.… ## 8 BE2100024 Vennen, heide… BE2100024-… 8 ((199553.4 233061.2, 199141.… ## 9 BE2100024 Vennen, heide… BE2100024-5 9 ((192190.9 232648.7, 192196 … ## 10 BE2100024 Vennen, heide… BE2100024-2 10 ((187597 231264.9, 187549.3 … ## # … with 606 more rows  To write the GeoPackage, we just use the GPKG driver of the powerful GDAL library (supporting most open and some closed formats), which can be elegantly accessed through sf::st_write():\nsac %\u0026gt;% st_write(\u0026#34;sac.gpkg\u0026#34;) ## Updating layer `sac' to data source `sac.gpkg' using driver `GPKG' ## Writing 616 features with 4 fields and geometry type Polygon.  Is that all?\nYES :-)\nReally?\nYES :-)\nWell, hmmm, if you really want to know a little bit more…\nA GeoPackage can contain many layers. So, it is good practice to explicitly define the layer name within the GeoPackage (above, it was automatically called ‘sac’). For example:\nsac %\u0026gt;% st_write(\u0026#34;sac.gpkg\u0026#34;, layer = \u0026#34;special_areas_conservation\u0026#34;, delete_dsn = TRUE) ## Deleting source `sac.gpkg' using driver `GPKG' ## Updating layer `special_areas_conservation' to data source `sac.gpkg' using driver `GPKG' ## Writing 616 features with 4 fields and geometry type Polygon.  Note, delete_dsn was set as TRUE to replace the whole GeoPackage. (There is also a delete_layer argument to overwrite an existing layer with the same name.)\nLet’s extract a selection of features from the special_areas_conservation layer, and add it as a second layer into the GeoPackage file:\nsac %\u0026gt;% filter(str_detect(sac_name, \u0026#34;Turnhout\u0026#34;)) %\u0026gt;% # only polygons having \u0026#39;Turnhout\u0026#39; in their name field st_write(\u0026#34;sac.gpkg\u0026#34;, layer = \u0026#34;turnhout\u0026#34;) ## Updating layer `turnhout' to data source `sac.gpkg' using driver `GPKG' ## Writing 16 features with 4 fields and geometry type Polygon.  So yes, adding layers to a GeoPackage is done simply by st_write() again to the same GeoPackage file (by default, delete_dsn is FALSE), and defining the new layer’s name.\nSo, which layers are available in the GeoPackage?\ngdalUtils::ogrinfo(\u0026#34;sac.gpkg\u0026#34;) %\u0026gt;% cat(sep = \u0026#34;\\n\u0026#34;) ## INFO: Open of `sac.gpkg' ## using driver `GPKG' successful. ## 1: special_areas_conservation (Polygon) ## 2: turnhout (Polygon)  You see?\nReading a GeoPackage file Can it become more simple than this?\n# (note: the \u0026#39;layer\u0026#39; argument is unneeded if there\u0026#39;s just one layer) sac_test \u0026lt;- st_read(\u0026#34;sac.gpkg\u0026#34;, layer = \u0026#34;special_areas_conservation\u0026#34;) ## Reading layer `special_areas_conservation' from data source `/media/floris/DATA/git_repositories/tutorials/content/tutorials/spatial_standards_vector/sac.gpkg' using driver `GPKG' ## Simple feature collection with 616 features and 4 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 22084.25 ymin: 153207.4 xmax: 258865 ymax: 243333 ## epsg (SRID): NA ## proj4string: +proj=lcc +lat_1=49.8333339 +lat_2=51.16666723333333 +lat_0=90 +lon_0=4.367486666666666 +x_0=150000.01256 +y_0=5400088.4378 +ellps=intl +units=m +no_defs  Ready!\nst_read() is a function of the great sf package – hence the result is an sf object again.\nAlso other geospatial software will (or should) be able to open the GeoPackage format. It is an open standard, after all!\nHow to make and use GeoJSON files (*.geojson) Making a GeoJSON file from a geospatial sf object in R As another example, let’s download a shapefile of stream habitat 3260 in Flanders (version 2018):\ndownload_zenodo(doi = \u0026#34;10.5281/zenodo.3386246\u0026#34;) Again: you can visit a website of this dataset by just prefixing the DOI with doi.org/!\nThe data source is a shapefile again, in this case consisting of 4 different files. Similar as above, we read the geospatial data into R as an sf object and select a few attributes to work with:\nhabitatstreams \u0026lt;- read_sf(\u0026#34;habitatstreams.shp\u0026#34;) %\u0026gt;% select(river_name = NAAM, source = BRON) habitatstreams ## Simple feature collection with 560 features and 2 fields ## geometry type: LINESTRING ## dimension: XY ## bbox: xmin: 33097.92 ymin: 157529.6 xmax: 254039 ymax: 243444.6 ## epsg (SRID): NA ## proj4string: +proj=lcc +lat_1=49.8333339 +lat_2=51.16666723333333 +lat_0=90 +lon_0=4.367486666666666 +x_0=150000.01256 +y_0=5400088.4378 +ellps=intl +units=m +no_defs ## # A tibble: 560 x 3 ## river_name source geometry ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;LINESTRING [m]\u0026gt; ## 1 WOLFPUTBEEK VMM (127857.1 167681.2, 127854.9 167684.5, 127844 167688.9… ## 2 OUDE KALE VMM (95737.01 196912.9, 95732.82 196912.4, 95710.38 196907… ## 3 VENLOOP EcoInv (169352.7 209314.9, 169358.8 209290.5, 169326.2 209283… ## 4 VENLOOP EcoInv (169633.6 209293.5, 169625 209289.2, 169594.4 209321, … ## 5 KLEINE NETE EcoInv (181087.1 208607.2, 181088.6 208608.1, 181089 208608.4… ## 6 KLEINE NETE EcoInv (180037.4 208360.4, 180038.3 208377.5, 180038.3 208378… ## 7 KLEINE NETE EcoInv (180520 208595.7, 180540.5 208607.4, 180541.2 208607.7… ## 8 KLEINE NETE EcoInv (187379.9 209998.8, 187381.3 209998.5, 187381.6 209998… ## 9 RAAMDONKSEBE… extrap… (183545.5 192409, 183541.9 192406.7, 183541.9 192403, … ## 10 KLEINE NETE EcoInv (183516.4 208261.7, 183567.3 208279.2, 183567.3 208279… ## # … with 550 more rows  Nowadays, it is recommended to use the more recent and strict RFC7946 implementation of GeoJSON. The previous ‘GeoJSON 2008’ implementation is now obsoleted (see the post on this tutorials website for a bit more background).\nThe RFC7946 standard is well supported by GDAL’s GeoJSON driver, however GDAL must be given the explicit option RFC7946=YES in order to use it already [2].\nWrite the GeoJSON file as follows:\nhabitatstreams %\u0026gt;% st_write(\u0026#34;habitatstreams.geojson\u0026#34;, layer_options = \u0026#34;RFC7946=YES\u0026#34;) ## Writing layer `habitatstreams' to data source `habitatstreams.geojson' using driver `GeoJSON' ## options: RFC7946=YES ## Writing 560 features with 2 fields and geometry type Line String.  Done creating!\nDo I look good? Hey wait, wasn’t a GeoJSON file just a text file?\nIndeed.\nSo I can just open it as a text file to get an idea of its contents?\nWell seen :-)\nHence, also use it in versioned workflows?\nDidn’t hear that. (Cool, though…)\nLet’s just look at the top 7 lines of the file:\n{ \u0026quot;type\u0026quot;: \u0026quot;FeatureCollection\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;habitatstreams\u0026quot;, \u0026quot;features\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;Feature\u0026quot;, \u0026quot;properties\u0026quot;: { \u0026quot;river_name\u0026quot;: \u0026quot;WOLFPUTBEEK\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;VMM\u0026quot; }, \u0026quot;geometry\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;LineString\u0026quot;, \u0026quot;coordinates\u0026quot;: [ [ 4.0532635, 50.8196905 ], [ 4.0532327, 50.8197202 ], [ 4.0530778, 50.8197594 ], [ 4.0528708, 50.8199422 ], [ 4.052834, 50.8201498 ], [ 4.0528767, 50.8204559 ] ] } }, { \u0026quot;type\u0026quot;: \u0026quot;Feature\u0026quot;, \u0026quot;properties\u0026quot;: { \u0026quot;river_name\u0026quot;: \u0026quot;OUDE KALE\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;VMM\u0026quot; }, \u0026quot;geometry\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;LineString\u0026quot;, \u0026quot;coordinates\u0026quot;: [ [ 3.5931564, 51.0803318 ], [ 3.5930966, 51.0803266 ], [ 3.5927771, 51.0802782 ], [ 3.5926209, 51.080259 ], [ 3.5925707, 51.0802465 ], [ 3.5925106, 51.0802316 ], [ 3.592303, 51.0801396 ], [ 3.5921047, 51.0800302 ], [ 3.5920091, 51.0799694 ], [ 3.5919755, 51.0799432 ], [ 3.5919328, 51.07991 ], [ 3.5919165, 51.0798833 ] ] } }, { \u0026quot;type\u0026quot;: \u0026quot;Feature\u0026quot;, \u0026quot;properties\u0026quot;: { \u0026quot;river_name\u0026quot;: \u0026quot;VENLOOP\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;EcoInv\u0026quot; }, \u0026quot;geometry\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;LineString\u0026quot;, \u0026quot;coordinates\u0026quot;: [ [ 4.6443172, 51.1940245 ], [ 4.644403, 51.1938051 ], [ 4.6439364, 51.1937415 ], [ 4.6438717, 51.1936806 ], [ 4.6439146, 51.1934056 ] ] } },  You can see it basically lists the feature attributes and the coordinates of the lines’ vertices, with each feature starting on a new line.\nCompare the coordinates with those of the sf object habitatstreams above: the data have automatically been transformed to WGS84!\nNote: in order to be still manageable (text file size, usage in versioning systems) it seems wise to use GeoJSON for more simple cases – points and rather simple lines and polygons – and use the binary GeoPackage format for larger (more complex) cases.\nReading a GeoJSON file Just do this:\nhabitatstreams_test \u0026lt;- st_read(\u0026#34;habitatstreams.geojson\u0026#34;) ## Reading layer `habitatstreams' from data source `/media/floris/DATA/git_repositories/tutorials/content/tutorials/spatial_standards_vector/habitatstreams.geojson' using driver `GeoJSON' ## Simple feature collection with 560 features and 2 fields ## geometry type: LINESTRING ## dimension: XY ## bbox: xmin: 2.69742 ymin: 50.72875 xmax: 5.85425 ymax: 51.50032 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs  Same story as for the GeoPackage: other geospatial software will (or should) be able to open the GeoJSON format as well, as it’s an open and well established standard.\nFrom the message of st_read() you can see the CRS is WGS84 (EPSG-code 4326) - this is always expected when reading a GeoJSON file.\nIf you want to transform the data to another CRS, e.g. Belgian Lambert 72 (EPSG-code 31370), use sf::st_transform():\nhabitatstreams_test %\u0026gt;% st_transform(31370) ## Simple feature collection with 560 features and 2 fields ## geometry type: LINESTRING ## dimension: XY ## bbox: xmin: 33013.71 ymin: 157590.5 xmax: 253945.9 ymax: 243502.9 ## epsg (SRID): 31370 ## proj4string: +proj=lcc +lat_1=51.16666723333333 +lat_2=49.8333339 +lat_0=90 +lon_0=4.367486666666666 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +towgs84=-106.8686,52.2978,-103.7239,0.3366,-0.457,1.8422,-1.2747 +units=m +no_defs ## First 10 features: ## river_name source geometry ## 1 WOLFPUTBEEK VMM LINESTRING (127768.8 167742... ## 2 OUDE KALE VMM LINESTRING (95650.24 196973... ## 3 VENLOOP EcoInv LINESTRING (169263.1 209374... ## 4 VENLOOP EcoInv LINESTRING (169544 209352.8... ## 5 KLEINE NETE EcoInv LINESTRING (180997 208666.5... ## 6 KLEINE NETE EcoInv LINESTRING (179947.3 208419... ## 7 KLEINE NETE EcoInv LINESTRING (180429.9 208655... ## 8 KLEINE NETE EcoInv LINESTRING (187289.6 210058... ## 9 RAAMDONKSEBEEK extrapol LINESTRING (183455.2 192468... ## 10 KLEINE NETE EcoInv LINESTRING (183426.2 208321...    DOI = Digital Object Identifier. See https://www.doi.org.\n  Though GeoJSON 2008 is obsoleted, the now recommended RFC7946 standard is still officially in a proposal stage. That is probably the reason why GDAL does not yet default to RFC7946. A somehow confusing stage, it seems.\n  ","href":"/tutorials/tutorials/spatial_standards_vector/","title":"How to use open vector file formats in R: GeoPackage \u0026 GeoJSON"},{"content":"Some inspiration for this post came from the beautiful books of Lovelace et al. (2020), Pebesma \u0026amp; Bivand (2019) and Hijmans (2019), and from various websites.\nWhy use open standards?  Open file standards ease collaboration, portability and compatibility between users, machines and applications. Their (file) structure is fully documented.  Consequently, scientists and programmers can build new software / packages and make innovations that use these standards, while maintaining interoperability with existing applications. And, there is a much higher chance that your data will still be readable in a hundred years from now. The standard’s open documentation makes it relatively easy to build tools that can read an ancient open-standard file!    Luckily, quite a list of open standards is available! Below, some powerful and widely-used single-file formats are introduced. Single-file data sources are readily amenable to exchange and publication.\nI see you can’t wait to start practicing, so you can also head straight over to the tutorial on vector formats and the tutorial on raster formats! In these tutorials, a comparison table of vector/raster file formats is also presented.\nA few words on the GDAL library GDAL (Geospatial Data Abstraction Library) is by far the most used collection of open-source drivers for:\n a lot of raster formats; a lot of vector formats.  In other words, it is the preferred workhorse for reading and writing many geospatial file formats, used in the background by a lot of geospatial applications . Using GDAL is the easiest way to conform to open standards.\nSo, in R we use packages that use GDAL in the background, such as rgdal, sp, sf, raster and stars.\nThe GeoPackage file format  Its website is https://www.geopackage.org. It is a standardized implementation of an SQLite database for geospatial data. Hence, a GeoPackage is a binary file (filename.gpkg). It shares this property with shapefiles, which however pose multiple limitations,[1] so the GeoPackage is a more than suitable replacement. The GeoPackage can store one or multiple vector layers (points, lines, polygons and related feature types). Besides vector data, it can also store raster data or extra standalone tables. These properties make it somehow comparable to the ‘personal geodatabase’ of ArcGIS – ESRI’s closed, Windows-only format.[2] The GeoPackage standard is maintained by the Open Geospatial Consortium (OGC), which stands out as a reference when it comes to open geospatial standards.  The GeoJSON file format  One GeoJSON file (filename.geojson) contains one vector layer. Note that one vector layer can combine different feature geometry types, e.g. points and linestrings. JSON itself is a common and straightforward open data format. It is a text file readable both by humans and machines (see the tutorial for an example). GeoJSON adds the necessary specification to JSON for standardized storage of geographic feature data, but it is still a plain JSON text file. The GeoJSON standard is maintained by the Internet Engineering Task Force (IETF), a large open standards organization that develops Internet standards under the auspices of the Internet Society. Although the previous version of the GeoJSON standard – GeoJSON 2008 – is still a lot in use, it is obsoleted and a new version RFC7946 is establishing.  This version is strict about the coordinate reference system (CRS) – it is always WGS84 – and it also differs on a few other aspects (such as the recommendation for applications not to inflate decimal coordinate precision). RFC7946 solves the problem that quite a few libraries – including GDAL – simply assumed WGS84 in GeoJSON 2008 (without checking or transforming), even though WGS84 was not a requirement of GeoJSON 2008 (it did support an explicit crs declaration). This resulted in inconveniences (cf. this post in the sf-repository). A specific section in the documentation of GDAL’s GeoJSON driver gives a summary of the differences between both GeoJSON versions.   While GDAL by default still follows the GeoJSON 2008 format,[3] RFC7946 is supported by the option RFC7946=YES. Here, reprojection to WGS84 will happen automatically. It applies 7 decimal places for coordinates, i.e. approximately 1 cm. Given the advantages, we advise to explicitly use RFC7946. Several functions in R allow the user to provide options that are passed to GDAL, so we can ask to deliver RFC7946 (see the tutorial). In order to keep it manageable (text file size, usage in versioning systems[4] ) it can be wise to use GeoJSON for more simple cases (points and rather simple lines and polygons), and use the binary GeoPackage format for larger (more complex) cases.  The GeoTIFF file format  GeoTIFF is the preferred single-file open standard for raster data. It adheres to the open TIFF specification; hence it is a TIFF image file (filename.tif). It uses a small set of reserved TIFF tags to store information about CRS, extent and resolution of the raster. A GeoTIFF file can contain one or multiple rasters with the same CRS, extent and resolution. The GeoTIFF standard is maintained by the Open Geospatial Consortium (OGC), which stands out as a reference when it comes to open geospatial standards.  Literature Hijmans R. (2019). Spatial Data Science with R. URL: https://rspatial.org/.\nLovelace R., Nowosad J. \u0026amp; Muenchow J. (2020). Geocomputation with R. URL: https://geocompr.robinlovelace.net.\nPebesma E. \u0026amp; Bivand R. (2019). Spatial Data Science. URL: https://www.r-spatial.org/book.\n  Some problems with shapefiles are: they’re not an open format, they consist of multiple files and they have restrictions regarding file size, column name length, number of columns and the feature types that can be accommodated.\n  Note that personal geodatabases have their size limited to 250-500 MB; a GeoPackage can have a size of about 140 TB if the filesystem can handle it.\n  Though GeoJSON 2008 is obsoleted, the now recommended RFC7946 standard is still officially in a proposal stage. That is probably the reason why GDAL does not yet default to RFC7946. A somehow confusing stage, it seems.\n  When versioning GeoJSON files, mind the order of your data when rewriting them: reordering could produce large diffs. Interested in combining GeoJSON and GitHub? Surprise yourself!\n  ","href":"/tutorials/articles/geospatial_standards/","title":"Meet some popular open geospatial standards!"},{"content":"","href":"/tutorials/tags/open-science/","title":"open science"},{"content":"","href":"/tutorials/categories/development/","title":"development"},{"content":"","href":"/tutorials/tags/packages/","title":"packages"},{"content":"","href":"/tutorials/tags/python/","title":"python"},{"content":"Here are the slides of a presentation about the effectclass package.\n","href":"/tutorials/tutorials/r_effectclass/","title":"Classification and visualisation of estimates and their uncertainty"},{"content":"","href":"/tutorials/tags/effectclass/","title":"effectclass"},{"content":"","href":"/tutorials/tags/uncertainty/","title":"uncertainty"},{"content":"","href":"/tutorials/tags/api/","title":"api"},{"content":"","href":"/tutorials/tags/biodiversity/","title":"biodiversity"},{"content":"","href":"/tutorials/tags/gbif/","title":"gbif"},{"content":"Introduction This tutorial will explain how you can match a list of scientific names against the GBIF backbone taxonomy.\nIt is important that you have the most recent version of inborutils installed and available:\nremotes::install_github(\u0026#34;inbo/inborutils\u0026#34;) # install inborutils library(tidyverse) # To do datascience library(rgbif) # To lookup names in the GBIF backbone taxonomy library(inborutils) # To wrap GBIF API data library(knitr) Read data file containing the scientific names Read file containing the scientific names you want to check against the GBIF taxonomic backbone:\nspecies_df \u0026lt;- read_csv(\u0026#34;https://raw.githubusercontent.com/inbo/inbo-pyutils/master/gbif/gbif_name_match/sample.csv\u0026#34;, trim_ws = TRUE, col_types = cols()) Take a look at the data:\nkable(species_df)    name kingdom euConcernStatus     Alopochen aegyptiaca Animalia under consideration   Cotoneaster ganghobaensis Plantae NA   Cotoneaster hylmoei Plantae NA   Cotoneaster x suecicus Plantae NA   Euthamia graminifolia Plantae under preparation    Request taxonomic information Given a data.frame, you can match the column containing the scientific name against GBIF Backbone Taxonomy, using the gbif_species_name_match function from the inborutils package. You need to pass a data.frame, df and a column name, name:\nspecies_df_matched \u0026lt;- gbif_species_name_match(df = species_df, name = \u0026#34;name\u0026#34;) ## [1] \u0026quot;All column names present\u0026quot;  As the name argument has \u0026quot;name\u0026quot; as default value, the code above is equivalent to:\nspecies_df_matched \u0026lt;- gbif_species_name_match(species_df) or using pipe %\u0026gt;%:\nspecies_df_matched \u0026lt;- species_df_matched %\u0026gt;% gbif_species_name_match() By default gbif_species_name_match returns the following GBIF fields: usageKey, scientificName, rank, order, matchType, phylum, kingdom, genus, class, confidence, synonym, status, family.\nTake a look at the updated data:\nkable(species_df_matched)    name kingdom euConcernStatus usageKey scientificName rank order matchType phylum kingdom1 genus class confidence synonym status family     Alopochen aegyptiaca Animalia under consideration 2498252 Alopochen aegyptiaca (Linnaeus, 1766) SPECIES Anseriformes EXACT Chordata Animalia Alopochen Aves 98 FALSE ACCEPTED Anatidae   Cotoneaster ganghobaensis Plantae NA 3025989 Cotoneaster ganghobaensis J.Fryer \u0026amp; B.HylmÃ¶ SPECIES Rosales EXACT Tracheophyta Plantae Cotoneaster Magnoliopsida 98 FALSE ACCEPTED Rosaceae   Cotoneaster hylmoei Plantae NA 3025758 Cotoneaster hylmoei K.E.Flinck \u0026amp; J.Fryer SPECIES Rosales EXACT Tracheophyta Plantae Cotoneaster Magnoliopsida 98 FALSE ACCEPTED Rosaceae   Cotoneaster x suecicus Plantae NA 3026040 Cotoneaster suecicus G.Klotz SPECIES Rosales EXACT Tracheophyta Plantae Cotoneaster Magnoliopsida 98 FALSE ACCEPTED Rosaceae   Euthamia graminifolia Plantae under preparation 3092782 Euthamia graminifolia (L.) Nutt. SPECIES Asterales EXACT Tracheophyta Plantae Euthamia Magnoliopsida 98 FALSE ACCEPTED Asteraceae    Notice that GBIF fields whose name is already used as column name are automatically renamed by adding suffix 1. In our case, input data.frame species_df contains already a column called kingdom. The GBIF kingdom values are returned in column kingdom1:\nspecies_df_matched %\u0026gt;% select(kingdom, kingdom1) ## # A tibble: 5 x 2 ## kingdom kingdom1 ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Animalia Animalia ## 2 Plantae Plantae ## 3 Plantae Plantae ## 4 Plantae Plantae ## 5 Plantae Plantae  You can also specify which GBIF fields you would like to have:\nspecies_df %\u0026gt;% gbif_species_name_match( gbif_terms = c( \u0026#39;scientificName\u0026#39;, \u0026#39;family\u0026#39;, \u0026#39;order\u0026#39;, \u0026#39;rank\u0026#39;, \u0026#39;matchType\u0026#39;, \u0026#39;confidence\u0026#39;, \u0026#39;status\u0026#39;)) %\u0026gt;% kable() ## [1] \u0026quot;All column names present\u0026quot;     name kingdom euConcernStatus scientificName family order rank matchType confidence status     Alopochen aegyptiaca Animalia under consideration Alopochen aegyptiaca (Linnaeus, 1766) Anatidae Anseriformes SPECIES EXACT 98 ACCEPTED   Cotoneaster ganghobaensis Plantae NA Cotoneaster ganghobaensis J.Fryer \u0026amp; B.HylmÃ¶ Rosaceae Rosales SPECIES EXACT 98 ACCEPTED   Cotoneaster hylmoei Plantae NA Cotoneaster hylmoei K.E.Flinck \u0026amp; J.Fryer Rosaceae Rosales SPECIES EXACT 98 ACCEPTED   Cotoneaster x suecicus Plantae NA Cotoneaster suecicus G.Klotz Rosaceae Rosales SPECIES EXACT 98 ACCEPTED   Euthamia graminifolia Plantae under preparation Euthamia graminifolia (L.) Nutt. Asteraceae Asterales SPECIES EXACT 98 ACCEPTED    The function inborutils::gbif_species_name_match is a wrapper around rgbif::name_backbone, so you can pass any argument of name_backbone. For example, you can set strict = TRUE to fuzzy match only the given names, but never a taxon in the upper classification:\nspecies_df %\u0026gt;% gbif_species_name_match(strict = TRUE) %\u0026gt;% kable() ## [1] \u0026quot;All column names present\u0026quot;     name kingdom euConcernStatus usageKey scientificName rank order matchType phylum kingdom1 genus class confidence synonym status family     Alopochen aegyptiaca Animalia under consideration 2498252 Alopochen aegyptiaca (Linnaeus, 1766) SPECIES Anseriformes EXACT Chordata Animalia Alopochen Aves 99 FALSE ACCEPTED Anatidae   Cotoneaster ganghobaensis Plantae NA 3025989 Cotoneaster ganghobaensis J.Fryer \u0026amp; B.HylmÃ¶ SPECIES Rosales EXACT Tracheophyta Plantae Cotoneaster Magnoliopsida 99 FALSE ACCEPTED Rosaceae   Cotoneaster hylmoei Plantae NA 3025758 Cotoneaster hylmoei K.E.Flinck \u0026amp; J.Fryer SPECIES Rosales EXACT Tracheophyta Plantae Cotoneaster Magnoliopsida 99 FALSE ACCEPTED Rosaceae   Cotoneaster x suecicus Plantae NA 3026040 Cotoneaster suecicus G.Klotz SPECIES Rosales EXACT Tracheophyta Plantae Cotoneaster Magnoliopsida 99 FALSE ACCEPTED Rosaceae   Euthamia graminifolia Plantae under preparation 3092782 Euthamia graminifolia (L.) Nutt. SPECIES Asterales EXACT Tracheophyta Plantae Euthamia Magnoliopsida 98 FALSE ACCEPTED Asteraceae    These are all accepted parameters of name_backbone: ‘rank’, ‘kingdom’, ‘phylum’, ‘class’, ‘order’, ‘family’, ‘genus’, ‘strict’, ‘verbose’, ‘start’, ‘limit’, ‘curlopts’. See ?name_backbone for more details.\nFor Python users, there is a similar (but no longer maintained) function in inbo-pyutils.\n","href":"/tutorials/tutorials/r_gbif_name_matching/","title":"Match scientific names with the GBIF Backbone Taxonomy"},{"content":"library(R.utils) library(rgdal) library(tidyverse) library(leaflet) library(sp) library(sf) library(rgbif) library(DBI) What we want to do In this short tutorial, we explore various options to deal with the situation where we have (1) a spatially referenced GIS file with polygons and (2) a spatially referenced set of points that overlaps with the GIS polygons.\nTypically, both data sources contain information (apart from the spatial locations) that needs to be related to each other in some way. In this case study, we want to know for each point in which polygon it is located.\nGet some data to work with For the point data, we will work with data on the invasive species - Chinese mitten crab (Eriocheir sinensis) in Flanders, Belgium, from the year 2008 (GBIF.org (20th June 2019) GBIF Occurrence Download https://doi.org/10.15468/dl.decefb).\nWe will use convenience functions from the rgbif package to download the data as a zip file and to import the data as a tibble in the R environment.\ninvasive_species \u0026lt;- occ_download_get(\u0026#34;0032582-190415153152247\u0026#34;, path = tempdir(), overwrite = TRUE) %\u0026gt;% occ_download_import() %\u0026gt;% filter(!is.na(decimalLongitude), !is.na(decimalLatitude)) ## Download file size: 0.01 MB ## On disk at C:\\Users\\HANS_V~1\\AppData\\Local\\Temp\\RtmpaaoJx1\\0032582-190415153152247.zip  We will use the European reference grid system from the European Environmental Agency as an example of a GIS vector layer (each grid cell is a polygon). The Belgian part of the grid system can be downloaded as a sqlite/spatialite database from the EEA website using the following code:\n# explicitly set mode = \u0026#34;wb\u0026#34;, otherwise zip file will be corrupt download.file(\u0026#34;https://www.eea.europa.eu/data-and-maps/data/eea-reference-grids-2/gis-files/belgium-spatialite/at_download/file\u0026#34;, destfile = file.path(tempdir(), \u0026#34;Belgium_spatialite.zip\u0026#34;), mode = \u0026#34;wb\u0026#34;) # this will extract a file Belgium.sqlite to the temporary folder unzip(zipfile = file.path(tempdir(), \u0026#34;Belgium_spatialite.zip\u0026#34;), exdir = tempdir()) Point in polygon with the sf package The spatial query can be done with the aid of the sf package. The package has built-in functions to read spatial data (which uses GDAL as backbone).\nWe will project the data to Belgian Lambert 72 (https://epsg.io/31370), because the join assumes planar coordinates.\nbe10grid \u0026lt;- read_sf(file.path(tempdir(), \u0026#34;Belgium.sqlite\u0026#34;), layer = \u0026#34;be_10km\u0026#34;) %\u0026gt;% # convert to Belgian Lambert 72 st_transform(crs = 31370) We now get a sf object which is also a data.frame and a tbl:\nclass(be10grid) ## [1] \u0026quot;sf\u0026quot; \u0026quot;tbl_df\u0026quot; \u0026quot;tbl\u0026quot; \u0026quot;data.frame\u0026quot;  Let's have a look at this object:\nbe10grid ## Simple feature collection with 580 features and 3 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -22402.56 ymin: -1449.985 xmax: 311353.3 ymax: 305932.2 ## epsg (SRID): 31370 ## proj4string: +proj=lcc +lat_1=51.16666723333333 +lat_2=49.8333339 +lat_0=90 +lon_0=4.367486666666666 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +towgs84=-106.8686,52.2978,-103.7239,0.3366,-0.457,1.8422,-1.2747 +units=m +no_defs ## # A tibble: 580 x 4 ## cellcode eoforigin noforigin GEOMETRY ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;POLYGON [m]\u0026gt; ## 1 10kmE376N~ 3760000 3180000 ((-20851.02 240718.4, -21626.25 250679.5~ ## 2 10kmE376N~ 3760000 3190000 ((-21626.25 250679.5, -22402.56 260640.9~ ## 3 10kmE377N~ 3770000 3140000 ((-7781.475 201650, -8552.253 211611, 14~ ## 4 10kmE377N~ 3770000 3150000 ((-8552.253 211611, -9324.075 221572.1, ~ ## 5 10kmE377N~ 3770000 3160000 ((-9324.075 221572.1, -10096.95 231533.3~ ## 6 10kmE377N~ 3770000 3170000 ((-10096.95 231533.3, -10870.87 241494.6~ ## 7 10kmE377N~ 3770000 3180000 ((-10870.87 241494.6, -11645.85 251456.1~ ## 8 10kmE377N~ 3770000 3190000 ((-11645.85 251456.1, -12421.9 261417.9,~ ## 9 10kmE377N~ 3770000 3200000 ((-12421.9 261417.9, -13199.02 271379.8,~ ## 10 10kmE377N~ 3770000 3210000 ((-13199.02 271379.8, -13977.21 281342.1~ ## # ... with 570 more rows  We can see that the spatial information resides in a GEOMETRY list column.\nSimilarly, the package has built-in functions to convert a data.frame containing coordinates to a spatial sf object:\ninvasive_spatial \u0026lt;- st_as_sf(invasive_species, coords = c(\u0026#34;decimalLongitude\u0026#34;, \u0026#34;decimalLatitude\u0026#34;), crs = 4326) %\u0026gt;% # convert to Lambert72 st_transform(crs = 31370) Resulting in:\ninvasive_spatial ## Simple feature collection with 513 features and 43 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: 35125.7 ymin: 188235.3 xmax: 153743.2 ymax: 220135 ## epsg (SRID): 31370 ## proj4string: +proj=lcc +lat_1=51.16666723333333 +lat_2=49.8333339 +lat_0=90 +lon_0=4.367486666666666 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +towgs84=-106.8686,52.2978,-103.7239,0.3366,-0.457,1.8422,-1.2747 +units=m +no_defs ## # A tibble: 513 x 44 ## gbifID datasetKey occurrenceID kingdom phylum class order family genus ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1.15e9 258c9ce5-~ INBO:VIS:00~ Animal~ Arthr~ Mala~ Deca~ Pseud~ Erio~ ## 2 1.15e9 258c9ce5-~ INBO:VIS:00~ Animal~ Arthr~ Mala~ Deca~ Pseud~ Erio~ ## 3 1.15e9 258c9ce5-~ INBO:VIS:00~ Animal~ Arthr~ Mala~ Deca~ Pseud~ Erio~ ## 4 1.15e9 258c9ce5-~ INBO:VIS:00~ Animal~ Arthr~ Mala~ Deca~ Pseud~ Erio~ ## 5 1.15e9 258c9ce5-~ INBO:VIS:00~ Animal~ Arthr~ Mala~ Deca~ Pseud~ Erio~ ## 6 1.15e9 258c9ce5-~ INBO:VIS:00~ Animal~ Arthr~ Mala~ Deca~ Pseud~ Erio~ ## 7 1.15e9 258c9ce5-~ INBO:VIS:00~ Animal~ Arthr~ Mala~ Deca~ Pseud~ Erio~ ## 8 1.15e9 258c9ce5-~ INBO:VIS:00~ Animal~ Arthr~ Mala~ Deca~ Pseud~ Erio~ ## 9 1.15e9 258c9ce5-~ INBO:VIS:00~ Animal~ Arthr~ Mala~ Deca~ Pseud~ Erio~ ## 10 1.15e9 258c9ce5-~ INBO:VIS:00~ Animal~ Arthr~ Mala~ Deca~ Pseud~ Erio~ ## # ... with 503 more rows, and 35 more variables: species \u0026lt;chr\u0026gt;, ## # infraspecificEpithet \u0026lt;lgl\u0026gt;, taxonRank \u0026lt;chr\u0026gt;, scientificName \u0026lt;chr\u0026gt;, ## # countryCode \u0026lt;chr\u0026gt;, locality \u0026lt;lgl\u0026gt;, publishingOrgKey \u0026lt;chr\u0026gt;, ## # coordinateUncertaintyInMeters \u0026lt;dbl\u0026gt;, coordinatePrecision \u0026lt;lgl\u0026gt;, ## # elevation \u0026lt;lgl\u0026gt;, elevationAccuracy \u0026lt;lgl\u0026gt;, depth \u0026lt;lgl\u0026gt;, ## # depthAccuracy \u0026lt;lgl\u0026gt;, eventDate \u0026lt;chr\u0026gt;, day \u0026lt;int\u0026gt;, month \u0026lt;int\u0026gt;, ## # year \u0026lt;int\u0026gt;, taxonKey \u0026lt;int\u0026gt;, speciesKey \u0026lt;int\u0026gt;, basisOfRecord \u0026lt;chr\u0026gt;, ## # institutionCode \u0026lt;chr\u0026gt;, collectionCode \u0026lt;lgl\u0026gt;, catalogNumber \u0026lt;lgl\u0026gt;, ## # recordNumber \u0026lt;lgl\u0026gt;, identifiedBy \u0026lt;chr\u0026gt;, dateIdentified \u0026lt;lgl\u0026gt;, ## # license \u0026lt;chr\u0026gt;, rightsHolder \u0026lt;chr\u0026gt;, recordedBy \u0026lt;chr\u0026gt;, typeStatus \u0026lt;lgl\u0026gt;, ## # establishmentMeans \u0026lt;lgl\u0026gt;, lastInterpreted \u0026lt;chr\u0026gt;, mediaType \u0026lt;lgl\u0026gt;, ## # issue \u0026lt;chr\u0026gt;, geometry \u0026lt;POINT [m]\u0026gt;  Now we are ready to make the spatial overlay. This is done with the aid of sf::st_join. The default join type is st_intersects. This will result in the same spatial overlay as sp::over (see next section). We join the information from the grid to the points through a left join. See the DE-9IM topological model for explanations about all possible spatial joins.\nNote that with st_intersects points on a polygon boundary and points corresponding to a polygon vertex are considered to be inside the polygon. In the case where points are joined with polygons, st_intersects and st_covered_by will give the same result. The join type st_within can be used if we want to join only when points are completely within (excluding polygon boundary).\ninvasive_be10grid_sf \u0026lt;- invasive_spatial %\u0026gt;% st_join(be10grid, join = st_intersects, left = TRUE) %\u0026gt;% select(-eoforigin, -noforigin) # exclude the EofOrigin NofOrigin columns Looking at selected columns of the resulting object:\ninvasive_be10grid_sf %\u0026gt;% select(species, eventDate, cellcode, geometry) ## Simple feature collection with 513 features and 3 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: 35125.7 ymin: 188235.3 xmax: 153743.2 ymax: 220135 ## epsg (SRID): 31370 ## proj4string: +proj=lcc +lat_1=51.16666723333333 +lat_2=49.8333339 +lat_0=90 +lon_0=4.367486666666666 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +towgs84=-106.8686,52.2978,-103.7239,0.3366,-0.457,1.8422,-1.2747 +units=m +no_defs ## First 10 features: ## species eventDate cellcode ## 1 Eriocheir sinensis 2008-09-17T00:00:00Z 10kmE392N312 ## 2 Eriocheir sinensis 2008-06-03T00:00:00Z 10kmE392N312 ## 3 Eriocheir sinensis 2008-03-20T00:00:00Z 10kmE389N311 ## 4 Eriocheir sinensis 2008-07-03T00:00:00Z 10kmE393N311 ## 5 Eriocheir sinensis 2008-09-17T00:00:00Z 10kmE392N312 ## 6 Eriocheir sinensis 2008-04-10T00:00:00Z 10kmE392N312 ## 7 Eriocheir sinensis 2008-03-13T00:00:00Z 10kmE381N314 ## 8 Eriocheir sinensis 2008-03-19T00:00:00Z 10kmE391N312 ## 9 Eriocheir sinensis 2008-03-19T00:00:00Z 10kmE391N312 ## 10 Eriocheir sinensis 2008-10-28T00:00:00Z 10kmE389N311 ## geometry ## 1 POINT (143788.9 201487.1) ## 2 POINT (143788.9 201487.1) ## 3 POINT (114822.9 188235.3) ## 4 POINT (153743.2 191634.8) ## 5 POINT (143788.9 201487.1) ## 6 POINT (147138.3 199035.5) ## 7 POINT (35125.7 205808.5) ## 8 POINT (136197.7 197300.8) ## 9 POINT (136197.7 197300.8) ## 10 POINT (121593.8 190203.2)  Point in polygon with the sp package Instead of sf objects (= data.frames or tibbles with a geometry list-column), the sp package works with Spatial spatial data classes (which has many derived spatial data classes for points, polygons, \u0026hellip;).\nFirst, we need to convert the data.frame with point locations to a SpatialPointsDataFrame. We also need to ensure that the coordinate reference system (CRS) for both the point locations and the grid is the same. The data from GBIF are in WGS84 format.\ncrs_wgs84 \u0026lt;- CRS(\u0026#34;+init=epsg:4326\u0026#34;) coord \u0026lt;- invasive_species %\u0026gt;% select(decimalLongitude, decimalLatitude) invasive_spatial \u0026lt;- SpatialPointsDataFrame(coord, data = invasive_species, proj4string = crs_wgs84) The sp package has no native methods to read the Belgium 10 km x 10 km grid, but we can use rgdal::readOGR to connect with the sqlite/spatialite database and extract the Belgium 10 km x 10 km grid as a SpatialPolygonsDataFrame. Apart from the 10 km x 10 km grid, the database also contains 1 km x 1 km and 100 km x 100 km grids as raster or vector files.\nbe10grid \u0026lt;- readOGR(dsn = file.path(tempdir(), \u0026#34;Belgium.sqlite\u0026#34;), layer = \u0026#34;be_10km\u0026#34;) ## OGR data source with driver: SQLite ## Source: \u0026quot;C:\\Users\\hans_vancalster\\AppData\\Local\\Temp\\RtmpaaoJx1\\Belgium.sqlite\u0026quot;, layer: \u0026quot;be_10km\u0026quot; ## with 580 features ## It has 3 fields  We transform the 10 km x 10 km grid to the same CRS system:\nbe10grid \u0026lt;- spTransform(be10grid, crs_wgs84) Now we are ready to spatially join (overlay) the SpatialPointsDataFrame wth the 10 km x 10 km grid. This can be done using sp::over. The first two arguments of the function give, respectively, the geometry (locations) of the queries, and the layer from which the geometries or attributes are queried. See ?sp::over. In this case, when x = \u0026ldquo;SpatialPoints\u0026rdquo; and y = \u0026ldquo;SpatialPolygonsDataFrame\u0026rdquo;, it returns a data.frame of the second argument with row entries corresponding to the first argument.\ninvasive_be10grid \u0026lt;- over(x = invasive_spatial, y = be10grid) invasive_species_be10grid \u0026lt;- bind_cols(invasive_species, invasive_be10grid) To see what the result looks like, we can select the most relevant variables and print it (first ten rows).\ninvasive_species_be10grid %\u0026gt;% select(species, starts_with(\u0026#34;decimal\u0026#34;), eventDate, cellcode) %\u0026gt;% head(10) ## # A tibble: 10 x 5 ## species decimalLatitude decimalLongitude eventDate cellcode ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; ## 1 Eriocheir si~ 51.1 4.28 2008-09-17T00~ 10kmE392N~ ## 2 Eriocheir si~ 51.1 4.28 2008-06-03T00~ 10kmE392N~ ## 3 Eriocheir si~ 51.0 3.87 2008-03-20T00~ 10kmE389N~ ## 4 Eriocheir si~ 51.0 4.42 2008-07-03T00~ 10kmE393N~ ## 5 Eriocheir si~ 51.1 4.28 2008-09-17T00~ 10kmE392N~ ## 6 Eriocheir si~ 51.1 4.33 2008-04-10T00~ 10kmE392N~ ## 7 Eriocheir si~ 51.2 2.73 2008-03-13T00~ 10kmE381N~ ## 8 Eriocheir si~ 51.1 4.17 2008-03-19T00~ 10kmE391N~ ## 9 Eriocheir si~ 51.1 4.17 2008-03-19T00~ 10kmE391N~ ## 10 Eriocheir si~ 51.0 3.96 2008-10-28T00~ 10kmE389N~  What have we done? To wrap this up, we make a map that shows what we have done. We will use the results obtained with the sf package.\nFirst, we need to transform invasive_be10grid_sf back to WGS84 (the background maps in leaflet are in WGS84) (be10grid is already in WGS84 format).\ninvasive_be10grid_sf \u0026lt;- invasive_be10grid_sf %\u0026gt;% st_transform(crs = 4326) Zooming in on the point markers and hovering over a marker will show the reference grid identifier for the grid cell as it is joined to spatial points object invasive_be10grid. Clicking in a grid cell will bring up a popup showing the reference grid identifier for the grid cell as it is named in be10grid.\nleaflet(be10grid) %\u0026gt;% addTiles() %\u0026gt;% addPolygons(popup = ~cellcode) %\u0026gt;% addMarkers(data = invasive_be10grid_sf, label = ~cellcode) ","href":"/tutorials/tutorials/spatial_point_in_polygon/","title":"How to make spatial joins (point in polygon)?"},{"content":"During this workshop you learn how to turn a regular Rmarkdown file into a bookdown document using the INBO corporate identity. A lot of tips and trics use plain bookdown. So you can use them with other bookdown output formats.\nThe slides are available on the INBOmd examples website.\nThe source code is avalaible in the INBOmd example GitHub repository.\n","href":"/tutorials/tutorials/r_inbomd/","title":"Applying the INBO corporate identity to bookdown documents"},{"content":"","href":"/tutorials/tags/bookdown/","title":"bookdown"},{"content":"","href":"/tutorials/tags/e-book/","title":"e-book"},{"content":"","href":"/tutorials/tags/gitbook/","title":"gitbook"},{"content":"","href":"/tutorials/tags/inbomd/","title":"INBOmd"},{"content":"","href":"/tutorials/tags/rmarkdown/","title":"rmarkdown"},{"content":"","href":"/tutorials/categories/version-control/","title":"version control"},{"content":"","href":"/tutorials/tags/dplyr/","title":"dplyr"},{"content":"","href":"/tutorials/tags/ggplot2/","title":"ggplot2"},{"content":"","href":"/tutorials/tags/markdown/","title":"markdown"},{"content":"Doel van de cursus Hoe maak je van je ingezamelde gegevens een reproduceerbare analyse, visualisatie en rapportage, gebruik makend van de software R en Rstudio.\n Rstudio kunnen gebruiken (Les 1a) Commando's uitvoeren vanuit een script (Les 1b) Externe databestanden inlezen in R (Les 2a) Gegevens visualiseren (ggplot2) (Les 2b) Data manipuleren in een gewenste vorm (dplyr) (Les 3) Reproduceerbaar analyserapport maken (Rmarkdown) + algemene vragen (Les 4)  Bovenstaande topics worden gecombineerd in een opleiding van 4 workshops. Deze opleiding is bedoeld voor mensen die nog nooit met R gewerkt hebben. In de workshops wordt het materiaal in de handleidingen al doende uitgelegd, en afgewisseld met oefeningen. Na elke workshop wordt er altijd een huistaak aangeboden, die op vrijwillige basis ingediend kan worden, en van feedback voorzien.\nR en RStudio Introductie tot R en Rstudio\n Uitleg van alle vensters in Rstudio, en overlopen van de user installation instructions Werken met projecten Packages installeren en laden Coding basics in R Vectoren en dataframes  Handleiding\nCode\nHuistaak\nInlezen van gegevens Introductie tot het inlezen van externe databestanden\n readr readxl googlesheets  Handleiding\nData\nCode\nOefening\nggplot2 Introductie tot het maken van grafieken met ggplot2\n Basis syntax Geoms Aesthetics Facets Titels Plot bewaren  Handleiding\nData\nCode\nOefening\nHuistaak\ndplyr Introductie tot data manipulatie met dplyr\n piping filter, arrange, mutate, select group_by, summarise Tidy data: gather, spread  Handleiding\nData\nCode\nOefening\nHuistaak\nRmarkdown Introductie tot het maken van een reproduceerbaar document met Rmarkdown\n Markdown syntax Code chunks Chunk opties en globale opties YAML header Tabellen  Handleiding\nCode\nOefening\nFiguur voor oefening\nHuistaak\n","href":"/tutorials/tutorials/r_beginners/","title":"R voor beginners"},{"content":"","href":"/tutorials/tags/rstudio/","title":"rstudio"},{"content":"","href":"/tutorials/tags/inla/","title":"INLA"},{"content":"These workshops are a follow-up of the course on \u0026ldquo;Spatial, temporal and spatial-temporal models using R-INLA\u0026rdquo; by Alain Zuur and Elena Ieno (Highland Statistics Ltd.). The main goal is the get people using R-INLA with their own data in a workshop setting so they can tap into the knowledge of others. The workshops are not a copy of the Highstat course but elaborate certain topics. We also introduce the inlatools and inlabru.\nHackMD, a place to share your code related to these workshops.\nWorkshop 1 Fitting models with only fixed effects, random intercepts and first order random walk.\n slides source code and data  Workshop 2 inlabru versus INLA for fixed effects, random intercepts and random walks.\n slides source code and data  Workshop 3 Fitting models with spatial correlation\n slides source code and data  Literature  Bachl, F. et al, 2019 inlabru: an R package for Bayesian spatial modelling from ecological survey data https://doi.org/10.1111/2041-210X.13168 Blangiardo, M. and Cameletti, M. 2015 Spatial and Spatio-temporal Bayesian Models with R-INLA https://sites.google.com/a/r-inla.org/stbook/ ISBN: 978-1-118-32655-8 Gómez-Rubio, V. 2019 Bayesian inference with INLA and R-INLA https://becarioprecario.bitbucket.io/inla-gitbook/index.html Krainski, E. et al 2018 Advanced Spatial Modeling with Stochastic Partial Differential Equations Using R and INLA https://becarioprecario.bitbucket.io/spde-gitbook/index.html Wang, X. et al 2018 Bayesian Regression Modeling with INLA ISBN: 978-1-498-72725-9 Zuur, A. et al 2017 Beginner's Guide to Spatial, Temporal and Spatial-Temporal Ecological Data Analysis with R-INLA http://highstat.com/index.php/beginner-s-guide-to-regression-models-with-spatial-and-temporal-correlation ISBN: 978-0-957-17419-1 Zuur, A. and Ieno, E. 2018 Spatial, Temporal and Spatial-Temporal Ecological Data Analysis with R-INLA. Volume II: GAM and Zero-Inflated Models http://highstat.com/index.php/beginner-s-guide-to-regression-models-with-spatial-and-temporal-correlation ISBN: 978-0-957-17414-6  ","href":"/tutorials/tutorials/r_inla/","title":"INLA workshops"},{"content":"","href":"/tutorials/tags/inlabru/","title":"inlabru"},{"content":"At INBO, people do write code and require version control. As git is not the most straightforward environment to work with for non-IT specialists, we try to define our own sub-ecosystem with relevant practices and an achievable workflow.\nTo introduce the concept of version control with Git and Github, a separate git course website is available here.\nThe git course provides an introduction on main terminology of Git based on 5 important tasks:\n Tell the story of your project Travel back in time Experiment with changes Backup your work Collaborate on projects  The hands-on session git with Rstudio is part of the course. It can also be used as a reference to the indidivual steps when using git.\nWhen looking for a quick day-to-day workflow to use git(hub) with Rstudio, check the Rstudio workflow.\n","href":"/tutorials/tutorials/git_introduction/","title":"Git(hub) introduction"},{"content":"","href":"/tutorials/tags/version-control/","title":"version control"},{"content":"","href":"/tutorials/tags/reports/","title":"reports"},{"content":"","href":"/tutorials/tags/vmm/","title":"vmm"},{"content":"OPGELET: deze databank is alleen raadpleegbaar voor INBO-medewerkers, niet voor externen. Externen kunnen gebruik maken van het VMM-geoloket waterkwaliteit.\nHet INBO krijgt jaarlijks een (gedeeltelijke) kopie van de waterkwaliteitsdatabank van de Vlaamse Milieumaatschappij (VMM). De kopie omvat fysicochemische metingen en kwaliteitsindexen uit het VMM-meetnet oppervlaktewaters. Deze en andere waterkwaliteitsgevens zijn rechtstreeks opvraagbaar via het VMM-geoloket waterkwaliteit, maar de lokale kopie laat een veel vlottere raadpleging toe wanneer analyse van een groter aantal meetpunten gewenst is. De databank wordt ± jaarlijks geactualiseerd.\nMeer info over de aard van de gegevens en hoe ze kunnen geraadpleegd worden is te vinden op het intranet van het INBO: https://sites.google.com/a/inbo.be/intranet/ondersteuning/IT-en-data/datasystemen/vmm-oppervlaktewaters\n","href":"/tutorials/tutorials/vmm_surface_waters_quality_data/","title":"VMM surface waters - quality data"},{"content":"Introduction R code can become elaborate and consequently unclear or difficult to navigate. Yet, it is possible to introduce headers and navigate through them.\nCreating sections manually To create a header of a section, different methods can be applied. Any comment line which includes at least four trailing dashes (-), equal signs (=), or hash tags (#) automatically creates a code section.\n# 1. Header 1 #### # 2. Header 2 ---- # 3. Header 3 ==== On the right side of the code editor, nex to the buttons to run your code, a button with horizontal lines can be found. When you click it, the headers will be visible. As such, the structure of your code is visible and allows more easily to navigate through it.\nAnother way of navigation is via the button with the name of the selected header On the bottom of the code editor.\nCreating sections automatically It is also possible to add sections automatically by clicking on the tab Code and select Insert Section\u0026hellip;\nDrop down Note there is a drop down button next to each header, allowing to collapse or expand your code. Yet, there are shortcuts to this:\n Collapse — Alt+L Expand — Shift+Alt+L Collapse All — Alt+O Expand All — Shift+Alt+O  An example Now we will illustrate its use with an example of an analysis.\nRun tidyverse package\nlibrary(tidyverse) ## ── Attaching packages ─────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.0.9000 ✔ purrr 0.2.5 ## ✔ tibble 1.4.2 ✔ dplyr 0.7.6 ## ✔ tidyr 0.8.1 ✔ stringr 1.3.1 ## ✔ readr 1.1.1 ✔ forcats 0.3.0 ## ── Conflicts ────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag()  Now different manipulations will be performed on the dataset. To make navigation through the different manipulations more straightforward, we add sections.\n# 1. Plot hindfoot length over weight per year ---- surveys \u0026lt;- read_csv(\u0026#34;../data/20180222_surveys.csv\u0026#34;) %\u0026gt;% filter(!is.na(weight), # remove missing weight !is.na(hindfoot_length), # remove missing hindfoot_length !is.na(sex)) # remove missing sex ggplot(surveys, aes(x = weight, y = hindfoot_length)) + geom_point(aes(colour = species_id), alpha = 0.5) + ylab(\u0026#34;hindfoot length\u0026#34;) + scale_x_log10() + scale_color_discrete() + theme_dark() + facet_wrap(~year) # 2. Create a heatmap of the population growth in Ghent and its districts ---- tidy_bevolking \u0026lt;- read_csv(\u0026#34;../data/20180522_gent_groeiperwijk_tidy.csv\u0026#34;) ggplot(tidy_bevolking, aes(x = year, y = wijk)) + geom_tile(aes(fill = growth), color = \u0026#34;red\u0026#34;) + # fill = colour of content/pane; color = colour of edge # scale_fill_gradient(low = \u0026#34;white\u0026#34;, high = \u0026#34;steelblue\u0026#34;) + scale_fill_distiller(type = \u0026#34;div\u0026#34;) + theme(axis.title.x=element_blank(), axis.title.y=element_blank()) # 3. Place two plots in one window ---- install.packages(\u0026#34;cowplot\u0026#34;) devtools::install_github(\u0026#34;inbo/INBOtheme\u0026#34;) # install inbo theme library(cowplot) library(INBOtheme) weight_scatter \u0026lt;- ggplot(surveys, aes(x = weight, y = hindfoot_length)) + geom_point() + ylab(\u0026#34;hindfoot length\u0026#34;) weight_density \u0026lt;- ggplot(surveys, aes(x = weight, y = ..density..) ) + # the \u0026#39;..\u0026#39; refers to internal calculations of the density geom_histogram() + geom_density() # two plots in one window plot_grid(weight_scatter, weight_density, labels = c(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;)) ","href":"/tutorials/tutorials/r_script_sections/","title":"Headers and navigation in R code"},{"content":"Introduction The required packages are leaflet and sp.\nlibrary(leaflet) library(sp) Dummy data Let's create a dumy data.frame to play around, i.e. the three locations of INBO:\nnames \u0026lt;- c(\u0026#34;VAC HT\u0026#34;,\u0026#34;Geraardsbergen\u0026#34;,\u0026#34;Linkebeek\u0026#34;) lat \u0026lt;- c(50.865664, 50.760201, 50.767950) lon \u0026lt;- c(4.349944, 3.874300, 4.333044) data \u0026lt;- data.frame(names,lat,lon) We created three points:\nplot(data$lon, data$lat) Creating a map We need to convert the data.frame to a SpatialPointsDataFrame:\ncrs_wgs84 \u0026lt;- CRS(\u0026#34;+init=epsg:4326\u0026#34;) pts \u0026lt;- SpatialPointsDataFrame(data[c(\u0026#34;lon\u0026#34;,\u0026#34;lat\u0026#34;)], data[!(names(data) %in% c(\u0026#34;lon\u0026#34;,\u0026#34;lat\u0026#34;))], proj4string = crs_wgs84) The leaflet package is ideal to create a basic interactive map:\nlibrary(leaflet) leaf_map \u0026lt;- leaflet(pts) %\u0026gt;% addTiles(group = \u0026#34;OSM\u0026#34;) %\u0026gt;% addCircleMarkers() leaf_map Nice, no?!\nMore information is provided at the leaflet information website!\n","href":"/tutorials/tutorials/spatial_create_leaflet_map/","title":"Let's create an interactive map!"},{"content":"","href":"/tutorials/tags/tidyverse/","title":"tidyverse"},{"content":"library(dplyr) How to use piping in R Normally, you would do this:\nhead(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1  However, with piping, this would look different:\nmtcars %\u0026gt;% head() ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1  You may wonder, what's the point? If you need to apply multiple functions on one dataframe, piping saves you a lot of typing, and makes for tidy R code. An example:\nmtcars %\u0026gt;% mutate(dec = mpg/10) %\u0026gt;% select(mpg, dec, am) %\u0026gt;% filter(am == \u0026#34;1\u0026#34;) ## mpg dec am ## 1 21.0 2.10 1 ## 2 21.0 2.10 1 ## 3 22.8 2.28 1 ## 4 32.4 3.24 1 ## 5 30.4 3.04 1 ## 6 33.9 3.39 1 ## 7 27.3 2.73 1 ## 8 26.0 2.60 1 ## 9 30.4 3.04 1 ## 10 15.8 1.58 1 ## 11 19.7 1.97 1 ## 12 15.0 1.50 1 ## 13 21.4 2.14 1  What did we do:\n We created a new column \u0026lsquo;dec\u0026rsquo; using mutate(). This column \u0026lsquo;dec\u0026rsquo; consists of the values of column mpg divided by 10. We selected the columns \u0026lsquo;mpg\u0026rsquo;, \u0026lsquo;dec\u0026rsquo; and \u0026lsquo;am\u0026rsquo; using select(). We filtered for the value \u0026lsquo;1\u0026rsquo; in the column \u0026lsquo;am\u0026rsquo; using filter().  And all of this in just one step! Now what? We have created a new column, but this column is not part of our dataframe yet! We could do this:\nmtcars \u0026lt;- mtcars %\u0026gt;% mutate(dec = mpg/10) OR\u0026hellip; we could do this!\nlibrary(magrittr) mtcars %\u0026lt;\u0026gt;% mutate(dec = mpg/10) Soooo easy! This has been our first introduction to piping. There is however much more to learn! That is why you should definitely go to this link.\n","href":"/tutorials/tutorials/r_tidyverse_piping/","title":"Using `%\u003e%` pipes in R"},{"content":"","href":"/tutorials/tags/google/","title":"google"},{"content":"","href":"/tutorials/tags/installation/","title":"installation"},{"content":"","href":"/tutorials/categories/installation/","title":"installation"},{"content":"Insync is a thirth party tool that synchronises files with Google Drive. It has some nice features which are still not available in the sync tools provided by Google. For the remaining of this tutorial, \u0026ldquo;GoogleDrive\u0026rdquo; refers to the sync tools provided by Google.\nThe problem with GoogleDrive GoogleDrive doesn't work well in combination with RStudio projects or Git projects. We'll illustrate the problem with RStudio. RStudio has a performant auto save functionality, which limits data loss after an unexpected crash. As soon as the user changes a few characters in a script, the auto save kicks in. This functionality stores the backup information into a hidden subdirectory of the project (.Rproj.user). It writes very often to the files in this subdirectory.\nGoogleDrive is constantly monitoring the synchronised directory for new, changed or deleted files. As soon as it detects such file, it will lock the file, synchronise the file and unlock the file. The locking of the file pervents that changes are made to the file while it is being synchronised, because this would mess up the synchronisation. GoogleDrive synchronises all files within a synced directory, including those created by the RStudio auto save function. But as this function writes very often to those files, it often ends in trying to write to a file which still is locked by GoogleDrive. This results in a \u0026ldquo;cannot save to file\u0026rdquo; dialog box in RStudio, which has to be dismissed by the user. This happens so often that it becomes frustrating for the user.\nHow Insync solves this problem Insync is also constantly monitoring all files in the synchronised directories. Unlike GoogleDrive, Insync has an option to ignore directories or files when synchronising. When Insync is set to ignore .Rproj.user, the files within .Rproj.user are no longer synchronised and thus never locked, causing no problem with the RStudio auto save function.\nWait a minute, so these files will be no longer be available through the GoogleDrive website? Isn't that a problem? Indeed, they will not be available. And no, that is not a problem. Only your temporary changes are no longer synchronised. When you save your script file in RStudio, you are saving a file to a location which is not on the ignore list and will thus be synchronised. But after saving, this file will be locked during sync? Yes, but the time between two consecutive manual saves of a script is a lot larger that the time required to sync the script. So the file will be unlocked by the next time you save the file.\nHow to set the ignore list in Insync First of all, it is important to do this prior to syncing files to your computer. Once a file or directory has been synced between the computer and the cloud, Insync will keep syncing it. Even when a file or directory is afterward added to the ignore list.\nSet up  Open the Insync app Click on your avatar Choose ignore list Add the search pattern for the files/folders to ignore into the form fieldand click on the circled \u0026lsquo;+\u0026rsquo;  The default action is to exclude all matching files and directories (including their files and subdirectories) from syncing (\u0026ldquo;do not upload or download\u0026rdquo;). Local files will remain only local and files in the cloud will remain only in the cloud. You can change this behaviour via the drop down menu of the pattern. Other options are \u0026ldquo;do not upload\u0026rdquo;, \u0026ldquo;do not download\u0026rdquo; or \u0026ldquo;remove from this list\u0026rdquo;.\nWe recommend to add following patterns:\n .rproj.user *.git *.rcheck *_cache *_files _site  FAQ  Can I use the same local folder when switching from GoogleDrive to Insync  It is safer you use a different folder.   I've already synced an RStudio project with Insync without setting the ignore list  Create a new RStudio project in a different folder and copy your data an script to this new RStudio project    ","href":"/tutorials/installation/user/user_install_insync/","title":"Insync installation"},{"content":"","href":"/tutorials/tags/ci/","title":"ci"},{"content":"","href":"/tutorials/tags/development/","title":"development"},{"content":"Set up continuous integration with Wercker There are 2 major steps to set up continuous integration:\n create a wercker.yml file in the package add the application (package) to Wercker.com  To be able to add a package to Wercker, one must have administrator rights on the package repository on Github.\nThe Wercker test environment can only be set up if the file wercker.yml is commited to the repository, but Wercker is triggered to start checking when the application is added to wercker.com (giving an error if wercker.yml is not commited yet).\nwercker.yml Add a file \u0026ldquo;wercker.yml\u0026rdquo; in the root of the package with:\n box: reference to a package with a Docker image that is used as a test environment. If no specific version is specified, only the last master version is used. Which Docker image to use?  inbobmk/rstable which is an image with stable versions of R and a large number of packages (see the README). Most of the packages which are often used at INBO are available. The version of the packages is roughly fixed to the date on which the R version in the Docker image was upgraded. rocker/verse (https://hub.docker.com/r/rocker/verse/) which has the R, devtools and all tidyverse packages. The latest version of the image contains the latest version of the packages.   build:  different steps to pass  inbobmk/r-check: runs R CMD check but assumes that all dependencies are installed. Use this in combination with inbobmk/rstable in case you want to check your package against a stable set of packages. jimhester/r-check: installs all missing dependencies on the fly and then runs R CMD check. This will install the latest version of the dependencies. inbobmk/r-coverage: checks which lines in the code are covered by unit tests and which are not. See our page on code coverage for more details. This assumes that the covr package is installed. jimhester/r-coverage: installs covr and runs the code coverage inbobmk/r-lint: this check the style of your code. Good coding style is like using correct punctuation. You can manage without it, but it sure makes things easier to read. It assumes the lintr package is installed. jimhester/r-lint: installs the lintr package and checks the style of the code   steps are run following their order in the yaml file the execution will stop when a step fails if not all packages are available in the docker image, code to install packages has to be added as a first step   after a build pipe, one can also add a deploy-pipe  An example of a simple wercker.yml-file:\nbox: inbobmk/rstable build: steps: - script: code: Rscript -e \u0026quot;install.packages(c('DT','plotly'))\u0026quot; - inbobmk/r-check - inbobmk/r-coverage - inbobmk/r-lint  Wercker.com To add the application to www.wercker.com:\n log in on the website (easiest is to log in via github) and create a username click on the \u0026ldquo;+\u0026rdquo; button on the right top and choose \u0026ldquo;add application\u0026rdquo; select your username (next) select the repository of your package (next) choose the recommended option (next) you could choose to make the results publicly available and \u0026ldquo;create\u0026rdquo;  After creation, one can under \u0026ldquo;options\u0026rdquo;:\n pick a color for the package (useful when adding more than one package) read information on Webhook (ensures communication between github, Wercker and other services) status badge: markdown-code that allows you to add Wercker-results to your own code (e.g. copy this link to the README-file)  Hitting the avatar on the top right and choosing \u0026ldquo;Settings\u0026rdquo; allows to adjust if and when to receive email notifications.\n","href":"/tutorials/tutorials/development_wercker/","title":"Wercker"},{"content":"","href":"/tutorials/categories/literature/","title":"literature"},{"content":"","href":"/tutorials/tags/literature/","title":"literature"},{"content":"Sometimes we have a layer in one coordinate reference system (CRS) and need to transform it into another coordinate reference system. The first thing we need to do is identifying both coordinate reference systems. Let's create an example and identify the coordinate reference system with proj4string(). We used the coordinates posted on the contact page of NGI.\nlibrary(sp) library(leaflet) library(widgetframe) ## Loading required package: htmlwidgets  ngi \u0026lt;- data.frame(x = 650381.78, y = 667603.12) coordinates(ngi) \u0026lt;- ~x + y proj4string(ngi) ## [1] NA  NA indicates that the coordinate reference system isn't set. So we need to set it manually. In this case we know it is \u0026ldquo;Lambert 2008\u0026rdquo;. We need to know the related projection string. The projection string is often a rather long string of parameters. However, most coordinate reference systems have an EPSG number which you can find at http://epsg.io/. The EPSG number for \u0026ldquo;Lambert 2008\u0026rdquo; is 3812. Let's set this coordinate reference system to our dataset. CRS() defines the coordinate reference system based on a text string.\nproj4string(ngi) \u0026lt;- CRS(\u0026#34;+init=epsg:3812\u0026#34;) proj4string(ngi) ## [1] \u0026quot;+init=epsg:3812 +proj=lcc +lat_1=49.83333333333334 +lat_2=51.16666666666666 +lat_0=50.797815 +lon_0=4.359215833333333 +x_0=649328 +y_0=665262 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\u0026quot;  We could verify the correctness of this position by plotting in on a map. Here we use the leaflet package which requires the data to be in the \u0026ldquo;WGS84\u0026rdquo; coordinate reference system. Therefore we use spTransform to do this transformation. \u0026ldquo;WGS84\u0026rdquo; has EPSG number 4326. But here the coordinate reference system string itself is easier to memorise: \u0026ldquo;+proj=longlat\u0026rdquo;.\nngi_ll \u0026lt;- spTransform(ngi, CRS(\u0026#34;+proj=longlat\u0026#34;)) proj4string(ngi_ll) ## [1] \u0026quot;+proj=longlat +ellps=WGS84\u0026quot;  leaflet(ngi_ll) %\u0026gt;% addTiles() %\u0026gt;% addMarkers() # %\u0026gt;% #frameWidget()    CRS EPSG shortened_PROJ.4_string     WGS 84 4326 +init=epsg:4326   Belge 1972 / Belgian Lambert 72 31370 +init=epsg:31370   ETRS89 / Belgian Lambert 2008 3812 +init=epsg:3812   WGS 84 / Pseudo-Mercator 3857 +init=epsg:3857    ","href":"/tutorials/tutorials/spatial_transform_crs/","title":"Transforming spatial objects"},{"content":"WMS stands for Web Map Service. The service provides prerendered tiles at different scales. This makes it useful to include them as background images in maps.\nwms_grb links to the WMS of the GRB-basiskaart, the Flemish cadastral map. It depicts land parcels, buildings, watercourses, roads and railroads.\nwms_ortho contains a mosaic of recent orthophotos made during the winter. The layer Ortho contains the images, the layer Vliegdagcontour detail on the time when the pictures were taken.\nwms_inbo is a WMS providing several layers\nwms_hunting displays hunting grounds in Flanders\nwms_grb \u0026lt;- \u0026#34;https://geoservices.informatievlaanderen.be/raadpleegdiensten/GRB-basiskaart/wms\u0026#34; wms_ortho \u0026lt;- \u0026#34;https://geoservices.informatievlaanderen.be/raadpleegdiensten/OMWRGBMRVL/wms\u0026#34; wms_inbo \u0026lt;- \u0026#34;https://geoservices.informatievlaanderen.be/raadpleegdiensten/INBO/wms\u0026#34; wms_hunting \u0026lt;- \u0026#34;https://geoservices.informatievlaanderen.be/raadpleegdiensten/Jacht/wms\u0026#34; As background of interactive maps WMS layers can be added to a leaflet map using the addWMSTiles() function.\nlibrary(leaflet) leaflet() %\u0026gt;% setView(lng = 4.287638, lat = 50.703039, zoom = 15) %\u0026gt;% addWMSTiles( wms_grb, layers = \u0026#34;GRB_BSK\u0026#34;, options = WMSTileOptions(format = \u0026#34;image/png\u0026#34;, transparent = TRUE) ) leaflet() %\u0026gt;% setView(lng = 4.287638, lat = 50.703039, zoom = 15) %\u0026gt;% addWMSTiles( wms_ortho, layers = \u0026#34;Ortho\u0026#34;, options = WMSTileOptions(format = \u0026#34;image/png\u0026#34;, transparent = TRUE) ) leaflet() %\u0026gt;% setView(lng = 4.287638, lat = 50.703039, zoom = 15) %\u0026gt;% addWMSTiles( wms_inbo, layers = \u0026#34;PNVeg\u0026#34;, options = WMSTileOptions(format = \u0026#34;image/png\u0026#34;, transparent = TRUE) ) leaflet() %\u0026gt;% setView(lng = 4.287638, lat = 50.703039, zoom = 14) %\u0026gt;% addTiles(group = \u0026#34;OSM\u0026#34;) %\u0026gt;% addWMSTiles( wms_hunting, layers = \u0026#34;Jachtterr\u0026#34;, options = WMSTileOptions(format = \u0026#34;image/png\u0026#34;, transparent = TRUE) ) leaflet() %\u0026gt;% setView(lng = 4.287638, lat = 50.703039, zoom = 14) %\u0026gt;% addTiles(group = \u0026#34;OSM\u0026#34;) %\u0026gt;% addWMSTiles( wms_grb, layers = \u0026#34;GRB_BSK\u0026#34;, options = WMSTileOptions(format = \u0026#34;image/png\u0026#34;, transparent = TRUE), group = \u0026#34;GRB\u0026#34; ) %\u0026gt;% addWMSTiles( wms_hunting, layers = \u0026#34;Jachtterr\u0026#34;, options = WMSTileOptions(format = \u0026#34;image/png\u0026#34;, transparent = TRUE), group = \u0026#34;hunting\u0026lt;br\u0026gt;grounds\u0026#34; ) %\u0026gt;% addLayersControl( baseGroups = \u0026#34;OSM\u0026#34;, overlayGroups = c(\u0026#34;GRB\u0026#34;, \u0026#34;hunting\u0026lt;br\u0026gt;grounds\u0026#34;), options = layersControlOptions(collapsed = FALSE) ) ","href":"/tutorials/tutorials/spatial_wms_services/","title":"Using WMS service in R"},{"content":"Real life datasources seldom provide data in exactly the format you need for the analysis. Hence most of the time you need to manipulate the data after reading it into R. There are several ways to do this, each with their pros and cons. We highly recommend the tidyverse collection of packages. The command library(tidyverse) will actually load the following packages: ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr and forecats.\nWhere to find good information on these packages:  official tidyverse website the R for data science book (R4DS) by Garrett Grolemund and Hadley Wickham. Note that this book is freely available online. A printed version is available at the INBO library. video tutorials:  Data wrangling with R and RStudio: a good introduction on dplyr and tidyr by Garrett Grolemund dplyr tutorial at useR!2014 by Hadley Wickham (video part 1 and part 2) tidyverse, visualization, and manipulation basics: a high-level overview of tidyverse by Garrett Grolemund   Data Transformation Cheat Sheet: a two page document which covers the most important function for dplyr  ","href":"/tutorials/tutorials/r_tidyverse_info/","title":"Data wrangling with tidyverse"},{"content":"","href":"/tutorials/tags/data/","title":"data"},{"content":"Introduction (shamelessly taken from wikipedia)\nKerberos is a computer network authentication protocol that works on the basis of tickets to allow nodes communicating over a non-secure network to prove their identity to one another in a secure manner.\nWindows 2000 and later uses Kerberos as its default authentication method. Many UNIX and UNIX-like operating systems, including FreeBSD, Apple's Mac OS X, Red Hat Enterprise Linux, Oracle\u0026lsquo;s Solaris, IBM's AIX and Z/OS, HP's HP-UX and OpenVMS and others, include software for Kerberos authentication of users or services.\nHence, we can use the protocol to have an OS independent solution for authentication across different databases. In this document, the installation and configuration for linux/mac users is provided as well as an introduction to the usage of the authentication service to connect to databases. For windows users (in the domain) the authentication is provided by default.\nInstallation Libraries for authentication For debian/ubuntu users (make sure you belong to the sudo group):\nsudo apt-get install krb5-user libpam-krb5 libpam-ccreds auth-client-config sudo apt-get install openssl These libraries will be used later on. The following section is for interaction with MS SQL databases.\nModern Linux distributions use PAM to handle the authentication tasks of applications (services) on the system (PAM stands for Pluggable Authentication Modules, see man PAM). However we do not need that here. The above installation may have led to inserting a line into PAM configuration file /etc/pam.d/common-auth. The line looks like this (note the defining part pam_krb5.so):\nauth\t[success=4 default=ignore]\tpam_krb5.so minimum_uid=1000 This line makes every application that needs authentication on the system (like sudo, screensaver unlock, update manager, \u0026hellip;) first try the Kerberos connection to authenticate. This is overkill as we don't want to use Kerberos that way, and it can significantly slow down all other system authentications. Therefore, you should comment out the above line in /etc/pam.d/common-auth.\nMS SQL Server tools As most of the databases at INBO are SQL Server, an appropriate driver and the command line toolset is required to fully support database connections to SQL Server.\nODBC driver Download and install the Microsoft ODBC Driver for SQL Server. The installation instructions for different Linux flavours can be downloaded together with the ODBC driver. For Ubuntu 16.04 (and most distributions based on it), following instructions apply:\nsudo su apt-get install curl curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add - curl https://packages.microsoft.com/config/ubuntu/16.04/prod.list \u0026gt; /etc/apt/sources.list.d/mssqlrelease.list exit sudo apt-get update sudo ACCEPT_EULA=Y apt-get install msodbcsql=13.1.4.0-1 sudo apt-get install unixodbc-dev mssql-tools Install the MS SQL tools as well:\n sqlcmd: Command-line query utility. bcp: Bulk import-export utility.  The instructions for different platforms are explained here. In order to test the SQL connection later in this tutorial, add /opt/mssql-tools/bin/ to your PATH environment variable.\nYou could also decide to go for the binaries: download the debian package of mssql-tools and install with:\nsudo apt-get install libgss3 sudo dpkg -i mssql-tools_14.0.1.246-1_amd64.deb Configure Kerberos client (again, the commands assume root privileges)\nStart with the kerberos configuration dialogue:\ndpkg-reconfigure krb5-config Use INBO.BE as the realm (this is the realm of the kerberos servers): Make sure to use DNS to find these servers, so choose \u0026lsquo;NO\u0026rsquo; if you get the below question: Next, adapt the krb5.conf, probably available in the /etc directory. Add the following sections with configurations to the file:\n[realms] INBO.BE = { kdc = DNS_Name_DomainController1.domain.be kdc = DNS_Name_DomainController2.domain.be kdc = DNS_Name_DomainController3.domain.be kdc = DNS_Name_DomainController4.domain.be kdc = DNS_Name_DomainController5.domain.be default_domain = domain.be } [logging] default = FILE:/var/log/krblibs.log kdc = FILE:/var/log/krbkdc.log admin_server = FILE:/var/log/kadmind.log [libdefaults] default_realm = DOMAIN.BE dns_lookup_realm = false dns_lookup_kdc = false ticket_lifetime = 24h renew_lifetime = 7d forwardable= true Inbo staff can download a preconfigured krb5.conf file here:\u0026ldquo;https://drive.google.com/a/inbo.be/file/d/1q4MOWl3i-DDy1s3vwOeqPkpToa1S-3zE/view?usp=sharing\u0026quot;. In order to sync the timing of the domain controller server and client side, install ntp:\nsudo apt-get install ntp After installation, check if the following two files do exist:\n /etc/ntp.conf /etc/ntp.conf.dhcp (empty file, just amke sure there is a file)  Test installation Kerberos ticket system To check if the Kerberos configuration is successful, ask for a ticket by initiating with kinit:\nkinit your_user_name If no errors are prodused, check the existing tickets with klist:\nklist This should produce a list of successfully granted tickets, so something similar as:\nValid starting Expires Service principal 03/01/18 15:42:08 04/01/18 01:42:08 krbtgt/INBO.BE@INBO.BE renew until 10/01/18 15:42:08 SQL database connections When the ticketing is working, the next step is to use the authentication to connect to the databases itself. To test this, we'll use the sqlcmd command line tool. In a next section, we'll focus on the ODBC settings.\nTesting with sqlcmd (make sure you have an active ticket). Type quit to exit.\nInbo staff can consult a list of connection strings ( including server names ) for a server to query link\nsqlcmd -S DBServerName -E 1\u0026gt; Select top 10 name from sys.databases; 2\u0026gt; Go SQL ODBC connections To support database connections from other applications (e.g. GUI environments, but also R, Python,\u0026hellip;), the configuration of database drivers and connections should be provided in the /etc/odbc.ini and /etc/odbcinst.ini.\nMake sure the ODBC driver for SQL Server is available with a recognizable name in the /etc/odbcinst.ini file:\n[ODBC Driver 13 for SQL Server] Description=Microsoft ODBC Driver 13 for SQL Server Driver=/opt/microsoft/msodbcsql/lib64/libmsodbcsql-13.1.so.4.0 UsageCount=2 Connecting by explicitly providing the SQL connection string to ODBC libraries/packages Inbo staff can consult a list of connection strings here At this moment, you can actually connect using typical ODBC libraries/packages provided by R or Python:\nlibrary(DBI) connection \u0026lt;- dbConnect( odbc::odbc(), .connection_string = \u0026quot;Driver={ODBC Driver 13 for SQL Server};Server=DBServername;Database=DBName;Trusted_Connection=yes;\u0026quot; ) dbListTables(connection) import pyodbc conn = pyodbc.connect(\u0026#34;Driver={ODBC Driver 13 for SQL Server};Server=DBServername;Database=DBName;Trusted_Connection=yes;\u0026#34;) In RStudio, you can also make the connection with the GUI:\n Go to the Connections pane and click \u0026lsquo;New Connection\u0026rsquo;. In the window that opens, choose the ODBC Driver for SQL Server. In the Parameters field that comes next, add Server=DBServerName;Database=DBName;Trusted_Connection=yes;.  Note that the DBI connection statement is visible at the bottom field of the dialog window.   Click Test to verify successful connection.  If connection is unsuccessful, try again after explicitly adding your username to the connection string: User ID=your_username;   If the test is successful, click OK to make the connection.  Beside the fact that the connection has been made (see RStudio's R console), you also get a list of all databases (of the specific SQL Server) in the Connections pane. You can use this for exploratory purposes. Click here for more information on using RStudio's Connections pane.\nUNTESTED: Connecting after configuring odbc.ini However, it is probably easier to provide the configuration to specific databases directly, using the /etc/odbc.ini file. For example, the DBName database can be defined as follows:\n[nbn_ipt] Driver = ODBC Driver 13 for SQL Server Description = odbc verbinding naar db Trace = No Server = DBServername Database = DBName Port = 1433 Next, add the DBServername\nTODO: -\u0026gt; example in R/Python -\u0026gt; also available in Rstudio!\n","href":"/tutorials/installation/user/user_install_kerberos/","title":"Using Kerberos authentication for database connection"},{"content":"Scope This style guide is a recommendation for all R code written for the Research Institute for Nature and Forest (INBO). The goal of this style guide is twofold. First of all applying the guidelines will result in readable code. Secondly, it is much easier to work together on code when everyone is using the same guidelines. It is likely that applying these guidelines will have consequences on the current style used by many R users at INBO. Therefore this style guide should be applied within reason.\n Don't apply the style guide to existing code. R users are free to apply the style guide to new personal R code. Using the style guide is highly recommended for new or revised R code intended to be distributed and used by other R users. The style guide is mandatory for new or revised R packages distributed by INBO.  Please note that the RStudio editor has some handy features that automatically highlights errors against the code style in a non intrusive way. RStudio hints in this document are the instructions to activate these diagnostics in RStudio.\nSyntax RStudio hint: Tools \u0026gt; Global options \u0026gt; Code \u0026gt; Diagnostics: Check everything\nGeneral  lines should not exceed 80 characters  split the command over multiple lines if the command is longer than 80 characters RStudio hint: Tools \u0026gt; Global options \u0026gt; Code \u0026gt; Display: Check Show margin and set Margin column to 80   object names should be meaningful object names should not exceed 20 characters object names should be lowercase use _ to separate words in object names  function names with a single dot are allowed   use double quotes (\u0026quot;) around characters and not single quotes (') don't add commented code  use version control if you want to keep old versions of code    # Good example_text \u0026lt;- example_function( first_argument = \u0026#34;Some text\u0026#34;, second_argument = \u0026#34;More text\u0026#34; ) # Bad some.really.long.dot.separated.name \u0026lt;- MyCoolFunction(FirstArgument = \u0026#39;Some text\u0026#39;, second.argument = \u0026#39;More text\u0026#39;) Whitespace naturallanguagesusewhitespaceandpunctuationtomaketextsmorereadableprogramminglanguagesarenoexceptiontothisrule  Natural languages use whitespace and punctuation to make texts more readable. Programming languages are no exception to this rule.\n don't use tabs, use two spaces instead  RStudio hint: Tools \u0026gt; Global options \u0026gt; Code \u0026gt; Editing: Check Insert spaces for tab and set Tab width to 2   no space before a comma, one space after a comma one space before and after an infix operator (+, -, *, /, ^, \u0026amp;, |, %%, %/%, %*%, %in%, \u0026hellip;) no spaces a the end of a line  RStudio hint: Tools \u0026gt; Global options \u0026gt; Code \u0026gt; Saving: Check Strip trailing horizontal whitespace when saving   end the script file with a single blank line  RStudio hint: Tools \u0026gt; Global options \u0026gt; Code \u0026gt; Saving: Check Ensure that source files end with newline    Assignments  only create an object when you will use it later on always use \u0026lt;- for assignment always put the new variable on the left and never use -\u0026gt; use = only for passing arguments in a function at least one space before and at least one space after \u0026lt;- and =  use multiple spaces if it improves readability    # Good x \u0026lt;- data.frame(z = 1:10) summary(x) # Bad x=data.frame(z\u0026lt;-1:10) 5 -\u0026gt; a # Improved readability example a \u0026lt;- 5 ab \u0026lt;- 10 abc \u0026lt;- 7 d \u0026lt;- 245 Brackets R uses three types of brackets: round (...), square [...] and curly {...}.\n no spaces after opening a bracket no spaces before closing a bracket no spaces before opening a bracket except:  one space with control flow functions (if, else, for, while)   no spaces after closing a bracket except:  one space with control flow functions (if, else, for, while)   { should not start on a newline and is always the end of a line apply indentation when splitting long text inside brackets over multiple lines  # Good y \u0026lt;- seq(0, 2) if (max(y) \u0026lt;= 10) { x \u0026lt;- 1 } else { x \u0026lt;- 2 } sapply( y, function(x){ return(x) } ) # Bad y \u0026lt;- seq (0, 2 ) if( max( y ) \u0026lt;= 10 ) { x \u0026lt;- 1 } else { x \u0026lt;- 2 } sapply( y , function ( x ) { return(x)}) Special cases and exceptions  selecting rows with square brackets df[selection, ]  this results in two conflicting rules  a single space after a comma no space before a bracket   solution in case of a short command: add # nolint after the command  df[selection, ] # nolint   solution in case of a long command: split the command over several lines    # Good relevant_subset \u0026lt;- original_dataframe[ original_dataframe$x \u0026gt; some_value | original_dataframe$y \u0026lt; some_other_value, ] # Recommended dplyr alternative relevant_subset \u0026lt;- original_dataframe %\u0026gt;% filter(x \u0026gt; some_value | y \u0026lt; some_other_value) # Bad relevant_subset \u0026lt;- original_dataframe[original_dataframe$x \u0026gt; some_value | original_dataframe$y \u0026lt; some_other_value, ] # nolint  a really long text  text shorter than 80 characters but passed the 80 character limit due to the indentitation  solution: remove all indentation   text longer than 80 characters  solution: add # nolint at the end of the line     functions from other packages with names that don't comply with this style guide  solution: add # nolint at the end of the line    Important notice\nAdding # nolint at the end of a line excludes that line from the automatic checks for coding styles. Therefore use it only when you have no other options.\nFunctions  always explicitly mention return in functions  # Good sum \u0026lt;- function(x, y) { z \u0026lt;- x + y return(z) } sum \u0026lt;- function(x, y) { return(x + y) } # Bad sum \u0026lt;- function(x, y) { x + y } Validating syntax The code below validate the syntax for an R file, an RMarkdown file or an R package.\nRStudio hint: Running this code within RStudio will open a Markers pane, indicating the filename, line number and the kind of syntax error that occurs. Double clicking on the error will open the file at the correct location, making it easy to rectify the problem.\nExtra hint: start correcting for the last lines and work your way forward. This leaves the line numbers of the errors intact until you solve them.\nAn example to clarify this. Suppose you have an error at line 10 and an error at line 100. Both errors are lines should not be more than 80 characters, so to solve them we have to split the lines over multiple lines.\nLet say that we start with solving line 10 by splitting it over four lines. So the old line 10 becomes the new lines 10 to 13. Hence the old line 11 becomes the new line 14, and the old line 100 becomes the new line 103. When we now click on the marker for line 100, RStudio will go the current line 100 which is the old line 97. So you end up looking for an error at the wrong position.\nStarting at the back solved this issue. In the same example we would start by solving line 100. Let's assume we split this over two lines. So old line 100 because new line 100 and 101. Old line 101 becomes new line 102 but more importantly all line numbers before 100 are unchanged. So clicking on the marker for line 10 will take you the current line 10 which is the old line 10.\n# validate a single file lintr::lint(filename = \u0026#34;file.R\u0026#34;) lintr::lint(filename = \u0026#34;file.Rmd\u0026#34;) # validate a package lintr::lint_package(path = \u0026#34;.\u0026#34;) Documentation Functions  Add documentation above each function with Roxygen markup Add inline comments where relevant  Required Roxygen tags #\u0026#39; @title Title of the helpfile #\u0026#39; @description Description of the function in the helpfile #\u0026#39; @param define a parameter #\u0026#39; @export is the function exported by the package NAMESPACE #\u0026#39; @importFrom import a function from another package Optional Roxygen tags #\u0026#39; @seealso link to other functions #\u0026#39; @section section title #\u0026#39; @alias other name for the topic #\u0026#39; @keywords a set of standardised keywords. See file.path(R.home(\u0026#34;doc\u0026#34;), \u0026#34;KEYWORDS\u0026#34;) #\u0026#39; @inheritParams inherit the definition of parameters from another function #\u0026#39; @examples a working example of the function #\u0026#39; @return a description of the output from the function See http://r-pkgs.had.co.nz/man.html#roxygen-comments for more information on Roxygen\nINBO extra requirements for package DESCRIPTION  license: MIT or GPL-3? In case of MIT a LICENSE file should be added and License: MIT to the DESCRIPTION. In case of GPL-3 it is sufficient to add License: GPL-3 to the DESCRIPTION list of authors in Authors@R format INBO is listed as copyright holder one or more roles are atributed to each person  cre: package maintainer (only one person) aut: main author (at least one person) ctb: contributor (if relevant) cph: copyright holder (must be INBO)    Authors@R: c(person(\u0026ldquo;Els\u0026rdquo;, \u0026ldquo;Lommelen\u0026rdquo;, email = \u0026ldquo;els.lommelen@inbo.be\u0026rdquo;, role = c(\u0026ldquo;aut\u0026rdquo;, \u0026ldquo;cre\u0026rdquo;)), person(\u0026ldquo;Thierry\u0026rdquo;, \u0026ldquo;Onkelinx\u0026rdquo;, email = \u0026ldquo;thierry.onkelinx@inbo.be\u0026rdquo;, role = \u0026ldquo;aut\u0026rdquo;), person(\u0026ldquo;Anja\u0026rdquo;, \u0026ldquo;Leyman\u0026rdquo;, email = \u0026ldquo;anja.leyman@inbo.be\u0026rdquo;, role = \u0026ldquo;ctb\u0026rdquo;), person(family = \u0026ldquo;Research Institute for Nature and Forest (INBO)\u0026quot;, email = \u0026ldquo;info@inbo.be\u0026rdquo;, role = \u0026ldquo;cph\u0026rdquo;))\nHow-to's  Add one or more how-to's to a package Add them as RMarkdown vignettes  File structure R Package Functions  all generic R functions should be distributed as an R package use devtools::create() to start a new pacakge  RStudio hint: File \u0026gt; New project \u0026gt; New directory \u0026gt; R Package: Type the name in Package name   keep source files compact  create a separate file for each function, with the file name equal to the function name. This makes it easy to find the correct source file. exception: very short auxilary functions with related functionality  related functions can be bundled into one R script file name is either equal to the most important function or describes the related functionality     split large functions into several subfunctions  Scripts  place scripts in the inst folder  the scripts will be available for the user after installing the package the location of the scripts can be found with system.file(\u0026quot;script-name.R\u0026quot;, package = \u0026quot;yourpackage\u0026quot;) use a relevant folder structure when adding lots of files to inst    Unit tests  use the testthat package for unit tests  use devtools::use_testthat() to setup the test infrastructure   all unit tests are stored in tests/testthat all files should have either a test_ or helper_ prefix  files with helper_ prefix contain auxiliary function for the tests but no tests   the test files will be run in alphabetical order  setting the order of the files is easy by adding 3 letters to the prefix (eg. test_aaa_, test_baa, test_zzz_) 3 letters offers quite some flexibility to insert new files at the correct location without having to rename at lot of files. If the first file is test_aaa_ and the second test_baa, they you can 675 files between the two.   unit test files can be larger than source files a unit test file can contain tests for several functions in case the functions are strongly related (e.g. subfunctions) and reuse test cases each package should contain the unit for coding style as listed below  store this in a file tests/testthat/test_zzz_coding_style.R add this file to .gitignore  the coding style will be tested separately when using continuous integration      if (requireNamespace(\u0026#34;lintr\u0026#34;, quietly = TRUE)) { context(\u0026#34;lints\u0026#34;) test_that(\u0026#34;Package Style\u0026#34;, { lintr::expect_lint_free() }) } Data To make data available for users, they can be stored in a package in 3 different file types:\n plain text file (.txt, .csv,\u0026hellip;): use if  your package is under version control and the data often change you want to keep track on the changes using version control consider to keep row and column order fixed   binary file (.Rdata or .rda): use if  your dataset is large and data do not change between different versions you are not interested in keeping track on the exact changes you want to keep the exact format of the data (e.g. factors with levels) (possible in plain text with git2rdata)   code (.r) generating a table: use if the data can easily be generated with code  Data can be stored in 3 places:\n to make data available for loading and parsing examples, store them in the folder inst/extdata. Access this data with system.file(\u0026quot;xyz\u0026quot;, package = \u0026quot;abc\u0026quot;). Possible for all data types. to make data available to package users, store them in the data folder. Access this data with data(xyz). Possible only for data formats that can be handled by data(). Binary .rda-files can be stored by using usethis::use_data(xyz). to keep data internal for use by your functions, store them in the file R/Sysdata.rda by using usethis::use_data(xyz, internal = TRUE). Access this data with abc::xyz.  (In the above examples, xyz are data and abc is the package in which they are stored.)\nAdd scripts for generating these data in the folder data-raw and create this folder by using usethis::use_data_raw() (ignores folder during build).\nR script  group a long set of commands with similar functionality into a dedicated function  e.g. prepare_data(), do_analysis(), create_figure(), \u0026hellip;   place the user defined functions in a separate file which you source() into the main script  it is better to use the same file structure as an R package consider writing a simple package in case you have a lot of functions    RMarkdown  each chunk has only one output (figure, table, summary, \u0026hellip;) don't mix (heavy) calculations and output in the same chunk: this is more interesting for caching the results give chunks a relevant name: this make debugging easier and file name of figures and Bookdown label will be based on the chunk name avoid writing code that generates Markdown  use (parametrised) child documents instead   use the bookdown version for long reports: this makes it easy to split a long report into several child documents  Recommended packages Data import  readr: import text files readxl: import Excel files googlesheets: import Google Sheets DBI: connect to databases PostgreSQL, SQLite, MySQL, Oracle, \u0026hellip; RODBC: connect to databases SQL Server, Access  Data manipulation \u0026amp; transformation  dplyr:  subsetting observations subsetting variables changing variables aggregation combining dataframes   tidyr:  changing a dataframe from wide to long format and vice versa nesting and unnesting dataframes splitting a single variable into multiple variables    Graphics  ggplot2:all static graphics, charts and plots INBOtheme: INBO corporate identity for ggplot2 graphics  Quality control  lintr: checking coding style testthat: writing unit tests covr: check which part of the code is not covered by unit tests  ","href":"/tutorials/tutorials/styleguide_r_code/","title":"Styleguide R code"},{"content":"Na de eerste installatie  Start Rstudio Kies in het menu Tools -\u0026gt; Global Options In het tabblad General  Pas de Default working directory aan naar de locatie waar je R versie staat (C:/Program Files/R/R-3.x.y) 1 Restore .RData into workspace at startup: uitvinken Save workspace to.RData on exit: Never   In het tabblad Code  Subtab Editing  Insert spaces for tab: aanvinken Tab width: 2 Soft-wrap R source files: aanvinken   Subtab Saving  Default text encoding: UTF-8   Subtab Diagnostics  Alles aanvinken     In het tabblad Appearance  Stel in naar eigen smaak   In het tabblad Packages  CRAN mirror: wijzigen naar Global (CDN) - RStudio   In het tabblad Sweave  Weave Rnw files using: knitr Typeset LaTeX into PDF using: XeLaTex   Klik op OK en herstart RStudio  Configuratie van RStudio na een upgrade van R  Start RStudio Kies in het menu Tools -\u0026gt; Global Options Indien niet de laatste versie vermeld staat bij R version: klik op Change om het aan te passen. Klik op OK als je een waarschuwing krijgt dat je RStudio moet herstarten. Wijzig de initial working directory in C:/Program Files/R/R-3.x.y 1 Klik op OK Herstart RStudio    x en y verwijzen naar het versienummer. Dus bij R-3.1.0 is x = 1 en y = 0. De working directory is in dat geval C:/Progam Files/R/R-3.1.0 \u0026#x21a9;\u0026#xfe0e;\n   ","href":"/tutorials/installation/user/user_install_rstudio/","title":"Rstudio installation"},{"content":"Na de eerste installatie Bij een nieuwe R installatie hoeft de gebruiker geen bijkomende stappen te ondernemen.\nNa elke upgrade Voor onderstaande instructies uit telkens een nieuwe R versie geïnstalleerd werd. Je kan dit, indien gewenst, ook frequenter uitvoeren.\nWindows  Zorg dat de computer verbonden is met het internet. Zorg dat er geen enkele R sessie actief is op de computer. Start R x64 via het menu start. Tik het commando update.packages(ask = FALSE, checkBuilt = TRUE) gevolgd door enter. Er zullen nu packages waarvan een nieuwe versie beschikbaar is gedownload en geïnstalleerd worden. Dit duurt een hele poos, afhankelijk van het aantal te upgraden packages. Wacht tot de installatie volledig afgelopen is. Tik vervolgens het commando q() gevolgd door enter. R zal nu afgesloten worden.  Linux  Zorg dat de computer verbonden is met het internet. Zorg dat er geen enkele R sessie actief is op de computer. Open een terminal venster (Ctrl + Alt + T). Voer het commando Rscript -e 'update.packages(ask = FALSE, checkBuilt = TRUE)' uit. Er zullen nu packages waarvan een nieuwe versie beschikbaar is gedownload en geïnstalleerd worden. Dit duurt een hele poos, afhankelijk van het aantal te upgraden packages. Wacht tot de installatie volledig afgelopen is. Sluit de terminal met exit.  ","href":"/tutorials/installation/user/user_install_r/","title":"R language installation"},{"content":"Introduction You notice you have done something wrong on your branch. No worries, erroneous commits can be undone or reverted.\nFirst, check your history of commits to see which commits are the faulty ones.\ngit log --oneline Example output\nb7119f2 Continue doing crazy things 872fa7e Try something crazy a1e8fb5 Make some important changes to hello.py 435b61d Create hello.py 9773e52 Initial import Git checkout The git checkout command serves three distinct functions: checking out files, checking out commits, and checking out branches. In this part, only the first two configurations are addressed. Checking out a commit makes the entire working directory match that commit. This can be used to view an old state of your project without altering your current state in any way. Checking out a file lets you see an old version of that particular file, leaving the rest of your working directory untouched. (This will put you in a detached HEAD state.)\nYou can use git checkout to view the “Make some import changes to hello.py” commit as follows:\ngit checkout a1e8fb5 This makes your working directory match the exact state of the a1e8fb5 commit. You can look at files, and even edit files without worrying about losing the current state of the project. Nothing you do in here will be saved in your repository. Checking out an old commit is a read-only operation. It’s impossible to harm your repository while viewing an old revision. To continue developing, you need to get back to the “current” state of your project (assuming your master branch is the head of the project):\ngit checkout master If you’re only interested in a single file, you can also use git checkout to fetch an old version of it. For example, if you only wanted to see the hello.py file from the old commit, you could use the following command:\ngit checkout a1e8fb5 hello.py Remember, unlike checking out a commit, this does affect the current state of your project. The old file revision will show up as a “Change to be committed,” giving you the opportunity to revert back to the previous version of the file. If you decide you don’t want to keep the old version, you can check out the most recent version with the following:\ngit checkout HEAD hello.py This concludes the part on checking your previous commits. In the following part, some methods of rollback to a previous state will be elucidated.\ngit revert The git revert command undoes a committed snapshot. But, instead of removing the commit from the project history, it figures out how to undo the changes introduced by the commit and appends a new commit with the resulting content. This prevents Git from losing history, which is important for the integrity of your revision history and for reliable collaboration.\nUsage:\ngit revert \u0026lt;commit\u0026gt; Generate a new commit that undoes all of the changes introduced in , then apply it to the current branch.\nReverting should be used when you want to remove an entire commit from your project history. This can be useful, for example, if you’re tracking down a bug and find that it was introduced by a single commit. Instead of manually going in, fixing it, and committing a new snapshot, you can use git revert to automatically do all of this for you.\nIt's important to understand that git revert undoes a single commit—it does not “revert” back to the previous state of a project by removing all subsequent commits. In Git, this is actually called a reset, not a revert.\nReverting has two important advantages over resetting. First, it doesn’t change the project history, which makes it a “safe” operation for commits that have already been published to a shared repository. For details about why altering shared history is dangerous, please see the git reset page.\nSecond, git revert is able to target an individual commit at an arbitrary point in the history, whereas git reset can only work backwards from the current commit. For example, if you wanted to undo an old commit with git reset, you would have to remove all of the commits that occurred after the target commit, remove it, then re-commit all of the subsequent commits. Needless to say, this is not an elegant undo solution.\nExample\nThe following example is a simple demonstration of git revert. It commits a snapshot, then immediately undoes it with a revert.\n// edit some tracked files // commit a snapshot git commit -m 'make some changes that will be undone' // revert the commit we have just created git revert HEAD This can be visualized as the following:\nNote that the 4th commit is still in the project history after the revert. Instead of deleting it, git revert added a new commit to undo its changes. As a result, the 3rd and 5th commits represent the exact same code base, and the 4th commit is still in our history just in case we want to go back to it down the road.\ngit reset If git revert is a “safe” way to undo changes, you can think of git reset as the dangerous method. When you undo with git reset(and the commits are no longer referenced by any ref or the reflog), there is no way to retrieve the original copy—it is a permanent undo. Care must be taken when using this tool, as it’s one of the only Git commands that has the potential to lose your work.\nLike git checkout, git reset is a versatile command with many configurations. It can be used to remove committed snapshots, although it’s more often used to undo changes in the staging area and the working directory. In either case, it should only be used to undo local changes—you should never reset snapshots that have been shared with other developers.\nUsage:\ngit reset \u0026lt;file\u0026gt; Remove the specified file from the staging area, but leave the working directory unchanged. This unstages a file without overwriting any changes.\ngit reset Reset the staging area to match the most recent commit, but leave the working directory unchanged. This unstages all files without overwriting any changes, giving you the opportunity to re-build the staged snapshot from scratch.\ngit reset --hard Reset the staging area and the working directory to match the most recent commit. In addition to unstaging changes, the \u0026ndash;hard flag tells Git to overwrite all changes in the working directory, too. Put another way: this obliterates all uncommitted changes, so make sure you really want to throw away your local developments before using it.\ngit reset \u0026lt;commit\u0026gt; Move the current branch tip backward to , reset the staging area to match, but leave the working directory alone. All changes made since will reside in the working directory, which lets you re-commit the project history using cleaner, more atomic snapshots.\ngit reset --hard \u0026lt;commit\u0026gt; Move the current branch tip backward to and reset both the staging area and the working directory to match. This obliterates not only the uncommitted changes, but all commits after , as well.\nDiscussion\nAll of the above invocations are used to remove changes from a repository. Without the \u0026ndash;hard flag, git reset is a way to clean up a repository by unstaging changes or uncommitting a series of snapshots and re-building them from scratch. The \u0026ndash;hard flag comes in handy when an experiment has gone horribly wrong and you need a clean slate to work with.\nWhereas reverting is designed to safely undo a public commit, git reset is designed to undo local changes. Because of their distinct goals, the two commands are implemented differently: resetting completely removes a changeset, whereas reverting maintains the original changeset and uses a new commit to apply the undo.\nDon’t Reset Public History\nYou should never use git reset when any snapshots after have been pushed to a public repository. After publishing a commit, you have to assume that other developers are reliant upon it.\nRemoving a commit that other team members have continued developing poses serious problems for collaboration. When they try to sync up with your repository, it will look like a chunk of the project history abruptly disappeared. The sequence below demonstrates what happens when you try to reset a public commit. The origin/master branch is the central repository’s version of your local master branch.\nAs soon as you add new commits after the reset, Git will think that your local history has diverged from origin/master, and the merge commit required to synchronize your repositories is likely to confuse and frustrate your team.\nThe point is, make sure that you’re using git reset on a local experiment that went wrong—not on published changes. If you need to fix a public commit, the git revert command was designed specifically for this purpose.\nExamples: unstaging a file The git reset command is frequently encountered while preparing the staged snapshot. The next example assumes you have two files called hello.py and main.py that you’ve already added to the repository.\n# Edit both hello.py and main.py # Stage everything in the current directory git add . # Realize that the changes in hello.py and main.py # should be committed in different snapshots # Unstage main.py git reset main.py # Commit only hello.py git commit -m \u0026quot;Make some changes to hello.py\u0026quot; # Commit main.py in a separate snapshot git add main.py git commit -m \u0026quot;Edit main.py\u0026quot; As you can see, git reset helps you keep your commits highly-focused by letting you unstage changes that aren’t related to the next commit.\nremoving local commits The next example shows a more advanced use case. It demonstrates what happens when you’ve been working on a new experiment for a while, but decide to completely throw it away after committing a few snapshots.\n# Create a new file called `foo.py` and add some code to it # Commit it to the project history git add foo.py git commit -m \u0026quot;Start developing a crazy feature\u0026quot; # Edit `foo.py` again and change some other tracked files, too # Commit another snapshot git commit -a -m \u0026quot;Continue my crazy feature\u0026quot; # Decide to scrap the feature and remove the associated commits git reset --hard HEAD~2 The git reset HEAD~2 command moves the current branch backward by two commits, effectively removing the two snapshots we just created from the project history. Remember that this kind of reset should only be used on unpublished commits. Never perform the above operation if you’ve already pushed your commits to a shared repository.\nUsage statement The content of this Rmarkdown tutorial is a transformation from this source and is licensed under a Creative Commons Attribution 2.5 Australia License.\n","href":"/tutorials/tutorials/git_undo_commit/","title":"Undo a git commit"},{"content":"When working off line, two Git tasks cannot be performed: fetching/pulling updates from the server, and pushing changes to the server. All other commands still work.\nOne can commit changes, branch off, revert and reset changes, the same as when there exists an internet connection.\nExample workflow: start offline mode\nwhile(notBored): commit changes add files branch off new features end offline mode\nupdate master branch\ngit fetch origin push changes to the server\ngit push \u0026lt;branch-name\u0026gt; it is possible, that during your down-time, a pull request got accepted in that case, perform the following steps\ngit fetch origin git checkout \u0026lt;branch-name\u0026gt; git rebase master when necessary: solve merge conflicts, and rebase again.\nYour feature branch can now be pushed to the server, and a pull request can be made\n","href":"/tutorials/tutorials/git_no_internet/","title":"Git without internet"},{"content":"BEFORE I START WORKING   STEP 1: Update the master branch on my PC to make sure it is aligned with the remote master\ngit fetch origin git checkout master git merge --ff-only origin/master   STEP 2: Choose your option:\n  OPTION 2A: I already have a branch I want to continue working on:\nSwitch to existing topic branch:\ngit checkout name_existing_branch git fetch origin git rebase origin/master   OPTION 2B: I'll make a new branch to work with: Create a new topic branch from master(!):\ngit checkout master git checkout -b name_new_branch     WHILE EDITING  STEP 3.x: adapt in tex, code,\u0026hellip; (multiple times)   New files added\ngit add .   Adaptation\ngit commit -am \u0026quot;clear and understandable message about edits\u0026quot;     EDITS ON BRANCH READY   STEP 4: Pull request to add your changes to the current master. Choose your option:\n  OPTION 2A CHOSEN:\ngit push origin name_existing_branch   OPTION 2B CHOSEN:\ngit push origin name_new_branch     STEP 5: Code review!\nGo to your repo on Github.com and click the create pull request block. You and collaborators can make comments about the edits and review the code.\nIf everything is ok, click the Merge pull request, followed by confirm merge. (all online actions on GitHub). Delete the online branch, since obsolete.\nYou're work is now tracked and added to master! Congratulations.\nIf the code can't be merged automatically (provided by a message online), go to STEP 6.\n  PULL REQUEST CANNOT BE MERGED BY GITHUB   STEP 6: master has changed and there are conflicts: update your working branch with rebase\ngit checkout name_existing_branch git fetch origin git rebase origin/master # fix conflicts local git add file_with_conflict git rebase --continue git push -f origin name_existing_branch   ","href":"/tutorials/tutorials/git_workflow/","title":"Git workflow using the command line"},{"content":"The list below contains all R related software which should be installed on the computers of useRs at INBO. Note that the installation process of most software requires administrator rights.\n","href":"/tutorials/installation/administrator/","title":"Administrator installation notes"},{"content":"Here a some pages which describe the steps that the users stil needs to do after an installation or upgrade.\n","href":"/tutorials/installation/user/","title":"User installation notes"},{"content":"Fix merge conflict with a pull request You have made some changes to a feature branch. Make a pull request on the server. The standard case of automatic merge is not possible. Push your latest changes from the feature branch to the server.\nLocally on your computer:\ngit fetch origin Rebase your feature branch with your master\ngit rebase origin/master Git will now state that there are merge conflicts. These will look like this:\n\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD:\u0026lt;some git nonsense\u0026gt; This part is from a version of this file ===== This is from another version of a file \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; blahdeblahdeblah:\u0026lt;some more git nonsense\u0026gt; The \u0026laquo;\u0026laquo;\u0026lt;, ===== and \u0026raquo;\u0026raquo;\u0026gt; markers show which lines were changed simultaneously. In order to remove the conflict, choose which line you want to keep (first or second), remove the other line and the markers, and finally commit the result.\nAdd your files that you fixed.\ngit add \u0026lt;fixed files\u0026gt; Continue with your rebase\ngit rebase --continue If more troubles occur, fix them, add them, and do a git rebase continue\nForce push your branch to the server. (force because you changed the commit)\ngit push -f origin branchname On the server, you can now automatically close your Pull Request.\n","href":"/tutorials/tutorials/git_conflict/","title":"Handle conflicts"},{"content":"De installatiebestanden voor de stabiele versies zijn beschikbaar via http://www.rstudio.com/products/rstudio/download/. De preview versie is beschikbaar via https://www.rstudio.com/products/rstudio/download/preview/\nWindows Nieuwe installatie en upgrade van RStudio RStudio upgraden doe je door de nieuwe versie te installeren over de oude.\n Zorg dat eerst R geïnstalleerd is. Voer het 64-bit installatiebestand uit. Welkom bij de installatie: klik op volgende. Geef de doelmap en klik op volgende. Je mag de standaard gebruiken. Klik op installeren. Klik op voltooien.  RStudio mag niet met admininstratorrechten gestart worden. Anders worden een aantal R packages met administrator rechten geïnstalleerd waardoor de gebruiker ze niet meer kan updaten.\nTest de configuratie door RStudio te starten als een gewone gebruiker.\nAfwijkingen t.o.v. default installatie  Geen  ","href":"/tutorials/installation/administrator/admin_install_rstudio/","title":"RStudio Desktop installation"},{"content":"Windows Installatie en upgrade De installatiebestanden zijn beschikbaar via https://cloud.r-project.org/bin/windows/Rtools/\nBij een upgrade dient eerst de vorige versie te worden verwijderd.\n Kies Nederlands als taal voor de installatie en klik volgende. Klik volgende. Kies C:\\Rtools (standaard) als installatiemap en klik volgende. Kies uit dropdown menu Tools for building R packages from source (standaard) en klik volgende. Vink Add rtools to system path aan en klik volgende. Klik volgende. Klik installeren. Klik voltooien.  Afwijkingen t.o.v. default installatie  Aanvinken van Add rtools to system path  ","href":"/tutorials/installation/administrator/admin_install_rtools/","title":"Rtools installation"},{"content":"","href":"/tutorials/tags/windows/","title":"windows"},{"content":"Windows De installatiebestanden zijn beschikbaar via http://git-scm.com/downloads\n Voer het installatiebestand uit Welkom bij de installatie: klik op Next Aanvaard de licentievoorwaarden door Next te klikken Installeer git in de voorgesteld standaard directory. Gebruik de standaard componenten door Next te klikken Kies use Git from the Windows Command Prompt en klik Next Kies Checkout Windows-style, commit Unix-style line endings en klik Next Kies use Windows' default console window en klik Next Klik op Finish  Afwijkingen t.o.v. default installatie  use Git from the Windows Command Prompt use Windows' default console window  Ubuntu sudo apt-get update sudo apt-get install git ","href":"/tutorials/installation/administrator/admin_install_git/","title":"Git installation"},{"content":"","href":"/tutorials/tags/pandoc/","title":"pandoc"},{"content":"Windows Pandoc wordt automatisch geïnstalleerd als je RStudio installeert.\nUbuntu  Kijk op https://github.com/jgm/pandoc/releases wat de laatste versie is. Pas het versienummer in onderstaande code aan en voer ze uit in een terminalvenster  wget https://github.com/jgm/pandoc/releases/download/1.19/pandoc-1.19-1-amd64.deb sudo dpkg -i pandoc-1.19-1-amd64.deb rm pandoc-1.19-1-amd64.deb ","href":"/tutorials/installation/administrator/admin_install_pandoc/","title":"Pandoc installation"},{"content":"Windows Installatiebestand beschikbaar via https://cloud.r-project.org/bin/windows/base/\u0026lt;/a\u0026gt;\nIn de onderstaande tekst moet je in R-3.x.y zowel x als y vervangen door een cijfer om zo het huidige versienummer te krijgen. Dus voor versie R-3.0.0 is x = 0 en y = 0.\nNieuwe installatie van R  Voer het bestand R-3.x.y-win.exe uit. Kies Nederlands als taal voor de installatie en klik op OK. klik op Volgende. Aanvaard de licentievoorwaarden door op Volgende te klikken. Gebruik de standaarddoelmap en klik op Volgende. Selecteer de gewenste componenten en klik op Volgende. Je MOET deze standaardwaarden laten staan. Opstartinstelling aanpassen: Kies Nee en klik op Volgende. Geef de map voor het start menu en klik op Volgende. Je mag de standaardwaarde gebruiken. Vink de gewenste extra snelkoppelingen aan (default is ok), alle register entries aan en klik op Volgende. R wordt nu geïnstalleerd. Klik op Voltooien als de installatie afgelopen is. Ga naar Start en tik “Omgevingsvariabelen” in het veld Programma's en variabelen zoeken. Selecteer De omgevingsvariabelen van het systeem bewerken. Selecteer het tabblad Geavanceerd en klik op de knop Omgevingsvariabelen. Ga na of er een systeemvariabele R_LIBS_USER met waarde C:/R/library bestaat[1]. Indien niet, maak deze aan met de knop Nieuw. Sluit al deze schermen via de OK knop. Kopieer het bestand Rprofile.site naar etc in de doelmap waar je R geïnstalleerd hebt (C:\\Program Files\\R\\R-3.x.y) Hierbij moet je het bestaande bestand overschrijven. Zorg dat de gebruiker schrijfrechten heeft voor C:\\Program Files\\R\\R-3.x.y\\library en C:\\R\\library  Afwijkingen t.o.v. default installatie  alle gebruikers moeten volledige rechten hebben in  C:\\R\\library C:\\Program Files\\R\\R-3.x.y\\library   Systeemvariable R_LIBS_USER instellen op C:/R/library (verplicht forward slashes) Rprofile.site in C:\\Program Files\\R\\R-3.x.y\\etc overschrijven  R mag niet met admininstratorrechten gestart worden. Anders worden een aantal packages met administrator rechten geïnstalleerd waardoor de gebruiker ze niet meer kan updaten.\nStart R als een gewone gebruiker om de configuratie te testen.\nUpgrade van een bestaande R installatie Deze instructies veronderstellen dat R en RStudio in het verleden reeds geïnstalleerd werden volgens de bovenstaande instructies. Indien dan niet het geval is, volg dan de instructies voor een nieuwe installatie.\n Voer het bestand R-3.x.y-win.exe uit. Kies Nederlands als taal voor de installatie en klik op OK. klik op Volgende. Aanvaard de licentievoorwaarden door op Volgende te klikken. Gebruik de standaarddoelmap en klik op Volgende. Selecteer de gewenste componenten en klik op Volgende. Je MOET deze standaardwaarden laten staan. Opstartinstelling aanpassen: Kies Nee en klik op Volgende. Geef de map voor het start menu en klik op Volgende. Je mag de standaardwaarde gebruiken. Vink de gewenste extra snelkoppelingen aan (default is ok), alle register entries aan en klik op Volgende. R wordt nu geïnstalleerd. Klik op Voltooien als de installatie afgelopen is. Kopieer het bestand Rprofile.site naar etc in de doelmap waar je R geïnstalleerd hebt (C:\\Program Files\\R\\R-3.x.y) Hierbij moet je het bestaande bestand overschrijven. Zorg dat de gebruiker schrijfrechten heeft voor C:\\Program Files\\R\\R-3.x.y\\library De nieuwe R versie is klaar voor gebruik. De gebruiker moet RStudio bijwerken.  R mag niet met admininstratorrechten gestart worden. Anders worden een aantal packages met administrator rechten geïnstalleerd waardoor de gebruiker ze niet meer kan updaten.\nStart R als een gewone gebruiker om de configuratie te testen.\nInhoud Rprofile.site options( papersize = \u0026quot;a4\u0026quot;, tab.width = 2, width = 80, help_type = \u0026quot;html\u0026quot;, stringsAsFactors = TRUE, keep.source.pkgs = TRUE, xpinch = 300, ypinch = 300, yaml.eval.expr = TRUE, usethis.full_name = \u0026quot;Research Institute for Nature and Forest\u0026quot;, usethis.description = list( `Authors@R` = \u0026quot;c(\\n person(\\n \\\u0026quot;Voornaam\\\u0026quot;, \\\u0026quot;Achternaam\\\u0026quot;, role = c(\\\u0026quot;aut\\\u0026quot;, \\\u0026quot;cre\\\u0026quot;), \\n email = \\\u0026quot;voornaam.achternaam@inbo.be\\\u0026quot;, \\n comment = c(ORCID = \\\u0026quot;9999-9999-9999-9999\\\u0026quot;)),\\n person(\\n \\\u0026quot;Research Institute for Nature and Forest\\\u0026quot;,\\n role = c(\\\u0026quot;cph\\\u0026quot;, \\\u0026quot;fnd\\\u0026quot;), email = \\\u0026quot;info@inbo.be\\\u0026quot;))\u0026quot;, License = \u0026quot;GPL-3\u0026quot;, Language = \u0026quot;en-GB\u0026quot;, Encoding = \u0026quot;UTF-8\u0026quot;, Roxygen = \u0026quot;list(markdown = TRUE)\u0026quot; ), repos = c( RStudio = \u0026quot;https://cloud.r-project.org/\u0026quot;, INLA = \u0026quot;https://inla.r-inla-download.org/R/stable\u0026quot; ), install.packages.check.source = \u0026quot;no\u0026quot;, install.packages.compile.from.source = \u0026quot;never\u0026quot; ) # display fortune when starting new interactive R session if (interactive()) { if (length(find.package(\u0026quot;fortunes\u0026quot;, quiet = TRUE)) == 0) { utils::install.packages(\u0026quot;fortunes\u0026quot;) } tryCatch( print(fortunes::fortune()), error = function(e){ invisible(NULL) } ) } # required for RStan and brms Sys.setenv(BINPREF = \u0026quot;C:/Rtools/mingw_$(WIN)/bin/\u0026quot;)  Ubuntu sudo sh -c 'echo \u0026quot;deb http://cloud.r-project.org/bin/linux/ubuntu xenial/\u0026quot; \u0026gt;\u0026gt; /etc/apt/sources.list' sudo gpg --keyserver keyserver.ubuntu.com --recv-key E084DAB9 sudo gpg -a --export E084DAB9 | apt-key add - sudo apt-get update sudo apt-get install -y r-base r-base-dev libcurl4-openssl-dev libssl-dev libssh2-1-dev libxml2-dev  [1] Het moeten forward slashes zijn.\n","href":"/tutorials/installation/administrator/admin_install_r/","title":"Install R"},{"content":"R CMD check has a large set of generic quality tests on a package. It is impossible to create generic tests that check the content of the package. E.g. does each function return sensible results. However, R CMD check does run a set unit tests. These are small pieces of code written by the package developer which test the output of a specific function under specific circumstances. We highly recommend the testthat framework for writing unit tests.\nCombining code coverage and Wercker A useful tool to visualise the coverage of the package by unit tests, is codecov. It can be added to the Wercker application by:\n login to http://www.codecov.io (via GitHub) and copy the token add it to the tab Environment on http://www.wercker.com/: Key = CODECOV_TOKEN, Value = (paste the token) and tick \u0026lsquo;Protected\u0026rsquo; to prevent it from being viewed. This makes it secure.  Note that it only makes sense when the wercker.yaml has a inbobmk/r-coverage or jimhester/r-coverage step.\n","href":"/tutorials/tutorials/development_codecov/","title":"Code coverage"},{"content":"Intro R is known to have difficulties handling large data files. Here we will explore some tips that make working with such files in R less painfull.\ntl;dr  If you can comfortably work with the entire file in memory, but reading the file is rather slow, consider using the data.table package and read the file with its fread function. If your file does not comfortably fit in memory:  Use sqldf if you have to stick to csv files. Use a SQLite database and query it using either SQL queries or dplyr. Convert your csv file to a sqlite database in order to query    Downloading the example files While you can directly test this tutorial on your own large data files, we will use bird tracking data from the LifeWatch bird tracking network for the examples. We have made two versions of some tracking data available for download: a .csv file (text data) and a .db file (sqlite data). Both contain processed log files; for more information on the processing, see the BirdTrackingEtl package.\ncsv.name \u0026lt;- \u0026#34;2016-04-20-processed-logs-big-file-example.csv\u0026#34; db.name \u0026lt;- \u0026#34;2016-04-20-processed-logs-big-file-example.db\u0026#34; The evaluation of the next code chunk is ignored by default as the downloading and unzipping of the files results in more than 3 GB of data. If you do want to download the files yourself and test the other chunks, run the code and download the csv and sqlite examples. Make sure you have the R.utils package available (for unzipping the downloaded files). If not, use the command install.packages(\u0026quot;R.utils\u0026quot;) in your R console to download the package.\nlibrary(\u0026#34;R.utils\u0026#34;) # download the CSV file example csv.url \u0026lt;- paste(\u0026#34;https://s3-eu-west-1.amazonaws.com/lw-birdtracking-data/\u0026#34;, csv.name, \u0026#34;.gz\u0026#34;, sep = \u0026#34;\u0026#34;) if (!file.exists(csv.name)) { download.file(csv.url, destfile = paste0(csv.name, \u0026#34;.gz\u0026#34;)) gunzip(paste0(csv.name, \u0026#34;.gz\u0026#34;)) } # download the sqlite database example db.url \u0026lt;- paste(\u0026#34;https://s3-eu-west-1.amazonaws.com/lw-birdtracking-data/\u0026#34;, db.name, \u0026#34;.gz\u0026#34;, sep = \u0026#34;\u0026#34;) if (!file.exists(db.name)) { download.file(db.url, destfile = paste0(db.name, \u0026#34;.gz\u0026#34;)) gunzip(paste0(db.name, \u0026#34;.gz\u0026#34;)) } Loading a large dataset: use fread() or functions from readr instead of read.xxx(). library(\u0026#34;data.table\u0026#34;) library(\u0026#34;readr\u0026#34;) If you really need to read an entire csv in memory, by default, R users use the read.table method or variations thereof (such as read.csv). However, fread from the data.table package is a lot faster. Furthermore, the readr package also provides more optimized reading functions (read_csv, read_delim,\u0026hellip;). Let's measure the time to read in the data using these three different methods.\nread.table.timing \u0026lt;- system.time(read.table(csv.name, header = TRUE, sep = \u0026#34;,\u0026#34;)) readr.timing \u0026lt;- system.time(read_delim(csv.name, \u0026#34;,\u0026#34;, col_names = TRUE)) data.table.timing \u0026lt;- system.time(allData \u0026lt;- fread(csv.name, showProgress = FALSE)) data \u0026lt;- data.frame(method = c(\u0026#39;read.table\u0026#39;, \u0026#39;readr\u0026#39;, \u0026#39;fread\u0026#39;), timing = c(read.table.timing[3], readr.timing[3], data.table.timing[3])) data ## method timing ## 1 read.table 362.819 ## 2 readr 34.016 ## 3 fread 24.785  fread and read_delim are indeed much faster then the default read.table. However, the result of fread is a data.table and the result of read_delim is a tibble. Both are not a data.frame. The data.table package describes the data.table object as a more performant replacement for the data.frame. This means that selecting, filtering and aggregating data is much faster on a data.table compared to the standard data.frame but it requires you to use a slightly different syntax. A tibble is very similar to a data.frame, but provides more convenience when printing or subsetting the data table.\nYou can find the data.table package on CRAN. A good place to learn this package are the package vignettes. The introduction to data.table should be enough to get started. The readr package is also on CRAN. It belongs to a suite of R packages aiming to improve data manipulation in R, called tidyverse. More examples and explanation about readr is provided on the readr website.\nData files that don't fit in memory If you are not able to read in the data file, because it does not fit in memory (or because R becomes too slow when you load the entire dataset), you will need to limit the amount of data that will actually be stored in memory. There are a couple of options which we will investigate:\n limit the number of lines you are trying to read for some exploratory analysis. Once you are happy with the analysis you want to run on the entire dataset, move to another machine. limit the number of columns you are reading to reduce the memory required to store the data. limit both the number of rows and the number of columns using sqldf. stream the data.  1. Limit the number of lines you read (fread) Limiting the number of lines you read is easy. Just use the nrows and/or skip option (available to both read.table and fread). skip can be used to skip a number of rows, but you can also pass a string to this parameter causing fread to only start reading lines from the first line matching that string. Let's say we only want to start reading lines after we find a line matching the pattern 2015-06-12 15:14:39. We can do that like this:\nsprintf(\u0026#34;Number of lines in full data set: %s\u0026#34;, nrow(allData)) ## [1] \u0026quot;Number of lines in full data set: 3761058\u0026quot;  subSet \u0026lt;- fread(csv.name, skip = \u0026#34;2015-06-12 15:14:39\u0026#34;, showProgress = FALSE) sprintf(\u0026#34;Number of lines in data set with skipped lines: %s\u0026#34;, nrow(subSet)) ## [1] \u0026quot;Number of lines in data set with skipped lines: 9998\u0026quot;  Skipping rows this way is obviously not giving you the entire dataset, so this strategy is only useful for doing exploratory analysis on a subset of your data. Note that also read_delim provides a n_max argument to limit the number of lines to read. If you want to explore the whole dataset, limiting the number of columns you read can be a more useful strategy.\n2. Limit the number of columns you read (fread) If you only need 4 columns of the 21 columns present in the file, you can tell fread to only select those 4. This can have a major impact on the memory footprint of your data. The option you need for this is: select. With this, you can specify a number of columns to keep. The opposite - specifying the columns you want to drop - can be accomplished with the drop option.\nfourColumns = fread(csv.name, select = c(\u0026#34;device_info_serial\u0026#34;, \u0026#34;date_time\u0026#34;, \u0026#34;latitude\u0026#34;, \u0026#34;longitude\u0026#34;), showProgress = FALSE) sprintf(\u0026#34;Size of total data in memory: %s MB\u0026#34;, utils::object.size(allData)/1000000) ## [1] \u0026quot;Size of total data in memory: 1434.074264 MB\u0026quot;  sprintf(\u0026#34;Size of only four columns in memory: %s MB\u0026#34;, utils::object.size(fourColumns)/1000000) ## [1] \u0026quot;Size of only four columns in memory: 365.90692 MB\u0026quot;  The difference might not be as large as you would expect. R objects claim more memory than needed to store the data alone, as they keep pointers, and other object attributes. But still, the difference could save you.\n3. Limiting both the number of rows and the number of columns using sqldf The sqldf package allows you to run SQL-like queries on a file, resulting in only a selection of the file being read. It allows you to limit both the number of lines and the number of rows at the same time. In the background, this actually creates a sqlite database on the fly to execute the query. Consider using the package when starting from a csv file, but the actual strategy boils down to making a sqlite database file of your data. See this section below to learn how to interact with those and create a SQlite database from a CSV-file.\n4. Streaming data Short: streaming a file in R is a bad idea. If you are interested why, read the rest of this section.\nStreaming a file means reading it line by line and only keeping the lines you need or do stuff with the lines while you read through the file. It turns out that R is really not very efficient in streaming files. The main reason is the memory allocation process that has difficulties with a constantly growing object (which can be a dataframe containing only the selected lines).\nIn the next code block, we will read parts of our data file once using the freadfunction, and once line by line. You'll see the performance issue with the streaming solution.\nlibrary(ggplot2) allowedDevices = c(753, 801, 852) minDate = strptime(\u0026#39;1/3/2014\u0026#39;,format = \u0026#39;%d/%m/%Y\u0026#39;) maxDate = strptime(\u0026#39;1/10/2014\u0026#39;,format = \u0026#39;%d/%m/%Y\u0026#39;) streamFile \u0026lt;- function(limit) { con \u0026lt;- file(csv.name, open = \u0026#34;r\u0026#34;) selectedRecords \u0026lt;- list() i \u0026lt;- 0 file.streaming.timing \u0026lt;- system.time( while (i \u0026lt; limit) { oneLine \u0026lt;- readLines(con, n = 1, warn = FALSE) vec = (strsplit(oneLine, \u0026#34;,\u0026#34;)) selectedRecords \u0026lt;- c(selectedRecords, vec) i \u0026lt;- i + 1 } ) close(con) return(file.streaming.timing[[3]]) } freadFile \u0026lt;- function(limit) { file.fread.timing = system.time( d \u0026lt;- fread(csv.name, showProgress = FALSE, nrows = limit) ) return(file.fread.timing[[3]]) } maxLines \u0026lt;- c(5000, 10000, 15000, 20000, 25000, 30000) streamingTimes \u0026lt;- sapply(maxLines, streamFile) freadTimes \u0026lt;- sapply(maxLines, freadFile) data \u0026lt;- data.frame(n = maxLines, streaming = streamingTimes, fread = freadTimes) pdata \u0026lt;- melt(data, id = c(\u0026#34;n\u0026#34;)) colnames(pdata) \u0026lt;- c(\u0026#34;n\u0026#34;, \u0026#34;algorithm\u0026#34;, \u0026#34;execTime\u0026#34;) qplot(n, execTime, data = pdata, color = algorithm, xlab = \u0026#34;number of lines read\u0026#34;, ylab = \u0026#34;execution time (s)\u0026#34;) The database file strategy Working with SQLite databases SQLite databases are single file databases meaning you can simply download them, store them in a folder or share them with colleagues. Similar to a csv. They are however more powerful than csv's because of two important features:\n Support for SQL: this allows you to execute intelligent filters on your data, similar to the sqldf package or database environments you are familiar with. That way, you can reduce the amount of data that's stored in memory by filtering out rows or columns. Indexes: SQLite databases contain indexes. An index is something like an ordered version of a column. When enabled on a column, you can search through the column much faster. We will demonstrate this below.  We have downloaded a second file 2016-04-20-processed-logs-big-file-example.db that contains the same data as the 2016-04-20-processed-logs-big-file-example.csv file, but as a sqlite database. Furthermore, the database file contains indexes which will dramatically drop the time needed to perform search queries. If you do not have a SQLite database containing your data, you can first convert your csv into a SQlite as described further in this tutorial.\nLet's first connect to the database and list the available tables.\nlibrary(RSQLite) db \u0026lt;- dbConnect(SQLite(), dbname = db.name) # show the tables in this database dbListTables(db) ## [1] \u0026quot;SpatialIndex\u0026quot; ## [2] \u0026quot;geom_cols_ref_sys\u0026quot; ## [3] \u0026quot;geometry_columns\u0026quot; ## [4] \u0026quot;geometry_columns_auth\u0026quot; ## [5] \u0026quot;geometry_columns_field_infos\u0026quot; ## [6] \u0026quot;geometry_columns_statistics\u0026quot; ## [7] \u0026quot;geometry_columns_time\u0026quot; ## [8] \u0026quot;processed_logs\u0026quot; ## [9] \u0026quot;spatial_ref_sys\u0026quot; ## [10] \u0026quot;spatialite_history\u0026quot; ## [11] \u0026quot;sql_statements_log\u0026quot; ## [12] \u0026quot;sqlite_sequence\u0026quot; ## [13] \u0026quot;vector_layers\u0026quot; ## [14] \u0026quot;vector_layers_auth\u0026quot; ## [15] \u0026quot;vector_layers_field_infos\u0026quot; ## [16] \u0026quot;vector_layers_statistics\u0026quot; ## [17] \u0026quot;views_geometry_columns\u0026quot; ## [18] \u0026quot;views_geometry_columns_auth\u0026quot; ## [19] \u0026quot;views_geometry_columns_field_infos\u0026quot; ## [20] \u0026quot;views_geometry_columns_statistics\u0026quot; ## [21] \u0026quot;virts_geometry_columns\u0026quot; ## [22] \u0026quot;virts_geometry_columns_auth\u0026quot; ## [23] \u0026quot;virts_geometry_columns_field_infos\u0026quot; ## [24] \u0026quot;virts_geometry_columns_statistics\u0026quot;  Let's try to select rows where the device id matches a given value (e.g. 860), and the date time is between two given timestamps. For our analysis, we only need date_time, latitude, longitude and altitude so we will only select those.\nsqlTiming \u0026lt;- system.time(data \u0026lt;- dbGetQuery(conn = db, \u0026#34;SELECT date_time, latitude, longitude, altitude FROM processed_logs WHERE device_info_serial = 860 AND date_time \u0026lt; \u0026#39;2014-07-01\u0026#39; AND date_time \u0026gt; \u0026#39;2014-03-01\u0026#39;\u0026#34; )) print(sqlTiming[3]) ## elapsed ## 13.48  This provides a convenient and fast way to request subsets of data from our large data file. We could do the same analysis for each of the serial numbers, each time only loading that subset of the data. As an example, consider the calculation of the average altitude over the specified period for each of the bird serial identifiers in the list serial_id_list. By using a for loop, the calculation is done for each of the birds separately and the amount of data loaded into memory at the same time is lower:\nserial_id_list \u0026lt;- c(853, 860, 783) print(\u0026#34;Average altitude between 2014-03-01 and 2014-07-01:\u0026#34;) ## [1] \u0026quot;Average altitude between 2014-03-01 and 2014-07-01:\u0026quot;  for (serialid in serial_id_list) { data \u0026lt;- dbGetQuery(conn = db, sprintf(\u0026#34;SELECT date_time, latitude, longitude, altitude FROM processed_logs WHERE device_info_serial = %d AND date_time \u0026lt; \u0026#39;2014-07-01\u0026#39; AND date_time \u0026gt; \u0026#39;2014-03-01\u0026#39;\u0026#34;, serialid)) print(sprintf(\u0026#34;serialID %d: %f\u0026#34;, serialid, mean(data$altitude))) } ## [1] \u0026quot;serialID 853: NA\u0026quot; ## [1] \u0026quot;serialID 860: 23.550518\u0026quot; ## [1] \u0026quot;serialID 783: 14.900030\u0026quot;  Remark that we use the sprintf function to dynamically replace the serial id in the sqlite query we will execute. For each loop, the %d is replaced by the value of the serial id of the respective loop. Read the manual of the sprintf function for more information and options.\nInteracting with SQLite databases using dplyr If you're not comfortable with writing queries in SQL, R has a great alternative: dplyr. dplyr can connect to a SQLite database and you can perform the same operations on it that you would do on a dataframe. However, dplyr will translate your commands to SQL, allowing you to take advantage of the indexes in the SQLite database.\nlibrary(dplyr) my_db \u0026lt;- src_sqlite(db.name, create = FALSE) bird_tracking \u0026lt;- tbl(my_db, \u0026#34;processed_logs\u0026#34;) results \u0026lt;- bird_tracking %\u0026gt;% filter(device_info_serial == 860) %\u0026gt;% select(date_time, latitude, longitude, altitude) %\u0026gt;% filter(date_time \u0026lt; \u0026#34;2014-07-01\u0026#34;) %\u0026gt;% filter(date_time \u0026gt; \u0026#34;2014-03-01\u0026#34;) head(results) ## # Source: lazy query [?? x 4] ## # Database: sqlite 3.19.3 ## # [/home/stijn_vanhoey/githubs/inbo_tutorials/content/tutorials/r_large_data_files_handling/2016-04-20-processed-logs-big-file-example.db] ## date_time latitude longitude altitude ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2014-03-10 12:43:37 44.0 -7.59 626 ## 2 2014-03-10 12:58:32 44.1 -7.67 405 ## 3 2014-03-10 13:13:52 44.1 -7.69 326 ## 4 2014-03-10 13:28:57 44.1 -7.71 250 ## 5 2014-03-10 13:43:54 44.1 -7.72 174 ## 6 2014-03-10 13:59:06 44.1 -7.73 23  Dplyr provides the ability to perform queries as above without the need to know SQL. If you want to learn more about how to use dplyr with a SQLite database, head over to this vignette.\nCreate a SQLite databases from a CSV file In the case you have a CSV file available and you would like to query the data using SQL queries or with dplyr as shown in the previous sections, you can decide to convert the data to a SQlite database. The conversion will require some time, but once available, it provides the opportunity to query the data using SQL queries or with dplyr as shown in the previous sections. Moreover, you can easily add additional tables with related information to combine the data with.\nIf you already loaded the CSV file into memory, the creation of a SQLITE database is very straighforward and can be achieved in two steps:\ndb \u0026lt;- dbConnect(SQLite(), dbname = \u0026#34;example.sqlite\u0026#34;) dbWriteTable(db, \u0026#34;birdtracks\u0026#34;, allData) dbDisconnect(db) The first command creates a new database when the file example.sqlite does not exist already. The command dbWriteTable writes the table in the database. Hence, we can rerun the query from the previous section, but now on the newly created SQlite database, with the single created table birdtracks:\nmy_db \u0026lt;- src_sqlite(\u0026#34;example.sqlite\u0026#34;, create = FALSE) bird_tracking \u0026lt;- tbl(my_db, \u0026#34;birdtracks\u0026#34;) results \u0026lt;- bird_tracking %\u0026gt;% filter(device_info_serial == 860) %\u0026gt;% select(date_time, latitude, longitude, altitude) %\u0026gt;% filter(date_time \u0026lt; \u0026#34;2014-07-01\u0026#34;) %\u0026gt;% filter(date_time \u0026gt; \u0026#34;2014-03-01\u0026#34;) head(results) ## # Source: lazy query [?? x 4] ## # Database: sqlite 3.19.3 ## # [/home/stijn_vanhoey/githubs/inbo_tutorials/content/tutorials/r_large_data_files_handling/example.sqlite] ## date_time latitude longitude altitude ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 2014-03-10 12:43:37 44.0 -7.59 626 ## 2 2014-03-10 12:58:32 44.1 -7.67 405 ## 3 2014-03-10 13:13:52 44.1 -7.69 326 ## 4 2014-03-10 13:28:57 44.1 -7.71 250 ## 5 2014-03-10 13:43:54 44.1 -7.72 174 ## 6 2014-03-10 13:59:06 44.1 -7.73 23  However, when working with really large CSV files, you do not want to load the entire file into memory first (this is the whole point of this tutorial). An alternative strategy is to load the data from the CSV file in chunks (small sections) and write them step by step to the SQlite database.\nThis can be implemented by reading the CSV file in small sections (let's say 50000 lines each time) and move all sections to a given table in a sqlite database. As this is a recurrent task, we will provide the transformation in a custom written function, called csv_to_sqlite. The function is available within the inborutils package. Check the function documentation online or by typing ?csv_to_sqlite after installing and loading the inborutils package. As SQlite does not natively support date and datetime representations, the function converts those columns to an appropriate string representation before copying the dates to sqlite. To check for the date handling, the lubridate package is used.\nAs an example, let's convert the processed bird logs csv file to a sqlite database, called example.sqlite as a table birdtracks. Using the default values for the preprocessing number of lines and the chunk size, the conversion is as follows:\nlibrary(inborutils) sqlite_file \u0026lt;- \u0026#34;example2.sqlite\u0026#34; table_name \u0026lt;- \u0026#34;birdtracks\u0026#34; inborutils::csv_to_sqlite(csv_file = csv.name, sqlite_file, table_name, pre_process_size = 1000, chunk_size = 50000, show_progress_bar = FALSE) Hence, this approach will work for large files as well and is an ideal first step when doing this kind of analysis. Once performed, the SQlite database is available to query, similar to the previous examples:\nmy_db \u0026lt;- src_sqlite(\u0026#34;example2.sqlite\u0026#34;, create = FALSE) bird_tracking \u0026lt;- tbl(my_db, \u0026#34;birdtracks\u0026#34;) results \u0026lt;- bird_tracking %\u0026gt;% filter(device_info_serial == 860) %\u0026gt;% select(date_time, latitude, longitude, altitude) %\u0026gt;% filter(date_time \u0026lt; \u0026#34;2014-07-01\u0026#34;) %\u0026gt;% filter(date_time \u0026gt; \u0026#34;2014-03-01\u0026#34;) head(results) ## # Source: lazy query [?? x 4] ## # Database: sqlite 3.19.3 ## # [/home/stijn_vanhoey/githubs/inbo_tutorials/content/tutorials/r_large_data_files_handling/example2.sqlite] ## date_time latitude longitude altitude ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 2014-03-10 12:43:37 44.0 -7.59 626 ## 2 2014-03-10 12:58:32 44.1 -7.67 405 ## 3 2014-03-10 13:13:52 44.1 -7.69 326 ## 4 2014-03-10 13:28:57 44.1 -7.71 250 ## 5 2014-03-10 13:43:54 44.1 -7.72 174 ## 6 2014-03-10 13:59:06 44.1 -7.73 23  Remark that the dates are properly handled, by making sure the date representation inside SQlite is the converted string version.\n","href":"/tutorials/tutorials/r_large_data_files_handling/","title":"Reading large data files in R"},{"content":"Connection to INBO database Here we provide the approach using the package DBI, which is also used by RStudio.[1]. This package enables the link between R and the (remote) database. After installation of the needed packages (install.packages(c(\u0026quot;DBI\u0026quot;, \u0026quot;glue\u0026quot;, \u0026quot;tidyverse\u0026quot;))), the packages can be loaded:\nlibrary(DBI) library(glue) library(tidyverse) To create a database connection, different approaches are available as explained in the next section:\n Use the dedicated connect_inbo_dbase function in the inbodb package Setup the required parameters yourself Reuse existing registered connections on your computer  Inbodb function To support the connection to INBO databases, a dedicated function is available in the inbodb package, called connect_inbo_dbase. The function provides support to connect to INBO databases and uses the Connections pane in the Rstudio interface:\nTo use the connect_inbo_dbase function, make sure to install the inbodb package, following the installation instructions. After a successfull installation, load the library and create a database connection:\nlibrary(inbodb) my_connection \u0026lt;- connect_inbo_dbase(\u0026#34;D0021_00_userFlora\u0026#34;) Once the connection is successfully established, the database can be queried.\nRemark for Linux users: When working in Linux, this setup (using Trusted connection) requires an active kerberos session. More information about the setup and functionality is to be found in the tutorial on kerberos installation.\nSetting up the connection yourself In case you want to setup the connection yourself (e.g. errors using inbodb), you can do so by providing the necessary parameters:\nmy_connection \u0026lt;- DBI::dbConnect(odbc::odbc(), driver = \u0026#34;SQL Server\u0026#34;, server = \u0026#34;inbo-sql07-prd.inbo.be\u0026#34;, # or inbo-sql08-prd.inbo.be port = 1433, database = \u0026#34;D0021_00_userFlora\u0026#34;, # or your database of interest trusted_connection = \u0026#34;Yes\u0026#34;) The most important parameters are server and database, the others should normally be kept as such. For the first one, remember that database names starting with M, S or W can be accessed using the inbo-sql08-prd.inbo.be server and others (mostly with D) use the inbo-sql07-prd.inbo.be server. The database name is the name of the database (if you can’t remember the name, connect with a database you do know, e.g. D0021_00_userFlora and you’ll see an overview of the existing databases on that server after connecting.)\nUse existing MSAccess connection name When you query data from a SQL database that is already accessible using MSAccess, such a database is also accessible from R. For Windows users, the most important element is to know the so-called DSN (i.e. a registered Data Source Name). Actually, it is just the name of the database as it is known by your computer (and MS Access). The easiest way to check the DSN is to check the registered ODBC connections in the administrator tools menu.\nFor Dutch-speaking Windows 7 users:\n\u0026gt; Kies in het Configuratiescherm van Windows de optie Systeembeheer \u0026gt; Gegevensbronnen (ODBC). De optie Systeembeheer verschijnt in de categorie Systeem en onderhoud.  You should see a list similar to the list underneath, with the names of the available DSN names enlisted:\nAn alternative way to check the DSN name of a database already working on with Access, is to check the DSN inside MS Access (in dutch, check menu item Koppelingsbeheer):\nFor example, the DSN name UserFlora or Cydonia-prd can be used to query these databases and extract data from it with similar queries to the one used in MSAccess. First of all, the connection with the database need to be established, by using the odbcConnect function, providing the DSN name as argument:\nFor Windows users:\nmy_connection \u0026lt;- odbcConnect(\u0026#34;UserFlora\u0026#34;) Once the connection is successfully established, the database can be queried.\nGet a complete table from the database The function dbReadTable can be used to load an entire table from a database. For example, to extract the tblTaxon table from the flora database:\nrel_taxa \u0026lt;- dbReadTable(my_connection, \u0026#34;relTaxonTaxonGroep\u0026#34;) head(rel_taxa) %\u0026gt;% knitr::kable()    ID TaxonGroepID TaxonID     1 4 1   2 4 2   3 4 3   4 4 4   5 4 5   6 4 6    The connection my_connection, made earlier, is used as the first argument. The table name is the second argument.\nRemark: If you have no idea about the size of the table you’re trying to load from the database, this could be rather tricky and cumbersome. Hence, it is probably better to only extract a portion of the table using a query.\nExecute a query to the database The function dbGetQuery provides more flexibilty as it can be used to try any SQL-query on the database. A complete introduction to the SQL language is out of scope here. We will focus on the application and the reusage of a query.\nmeting \u0026lt;- dbGetQuery(my_connection, paste(\u0026#34;SELECT TOP 10 * FROM dbo.tblMeting\u0026#34;, \u0026#34;WHERE COR_X IS NOT NULL\u0026#34;)) head(meting) %\u0026gt;% knitr::kable()    ID WaarnemingID TaxonID MetingStatusCode Cor_X Cor_Y CommentaarTaxon CommentaarHabitat CREATION_DATE CREATION_USER UPDATE_DATE UPDATE_USER     2 21748 3909 GDGA 109948 185379 NA NA NA NA NA NA   14 45523 3909 GDGA 127708 179454 NA NA NA NA NA NA   15 124394 3909 GDGA 109424 192152 NA NA NA NA NA NA   23 38561 3909 GDGA 128290 179297 NA NA NA NA NA NA   24 126500 3909 GDGA 98714 178373 NA NA NA NA NA NA   173 73725 3909 GDGA 102612 189891 NA NA NA NA NA NA    Create and use query templates When you regularly use similar queries, with some minimal alterations, you do not want to copy/paste each time the entire query. It is prone to errors and you’re script will become verbose. It is advisable to create query templates, that can be used within the dbGetQuery function.\nConsider the execution of the following query. We are interested in those records with valid X and Y coordinates for the measurement, based on a given dutch name:\nsubset_meting \u0026lt;- dbGetQuery(my_connection, \u0026#34;SELECT meet.COR_X , meet.Cor_Y , meet.MetingStatusCode , tax.NaamNederlands , tax.NaamWetenschappelijk , waar.IFBLHokID FROM tblMeting AS meet LEFT JOIN tblTaxon AS tax ON tax.ID = meet.TaxonID LEFT JOIN tblWaarneming AS waar ON waar.ID = meet.WaarnemingID WHERE meet.Cor_X IS NOT NULL AND meet.Cor_X != 0 AND tax.NaamNederlands LIKE \u0026#39;Wilde hyacint\u0026#39;\u0026#34;) head(subset_meting) %\u0026gt;% knitr::kable()    COR_X Cor_Y MetingStatusCode NaamNederlands NaamWetenschappelijk IFBLHokID     88720 208327 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 11195   24106 199925 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 9601   103111 190915 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 6990   118123 183942 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 5672   106107 182343 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 5217   105765 180785 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 4830    If we need this query regularly, but each time using a different tax.NaamNederlands (only the name changes), it is worthwhile to invest some time in the creation of a small custom function that uses this query as a template. Let’s create a function flora_records_on_dutch_name that takes a valid database connection and a given Dutch name and returns the relevant subset of the data for this query:\nflora_records_on_dutch_name \u0026lt;- function(dbase_connection, dutch_name) { dbGetQuery(dbase_connection, glue_sql( \u0026#34;SELECT meet.Cor_X , meet.COr_Y , meet.MetingStatusCode , tax.NaamNederlands , tax.NaamWetenschappelijk , waar.IFBLHokID FROM dbo.tblMeting meet LEFT JOIN dbo.tblTaxon AS tax ON tax.ID = meet.TaxonID LEFT JOIN dbo.tblWaarneming AS waar ON waar.ID = meet.WaarnemingID WHERE meet.Cor_X IS NOT NULL AND meet.Cor_X != 0 AND tax.NaamNederlands LIKE {dutch_name}\u0026#34;, dutch_name = dutch_name, .con = dbase_connection)) } Hence, instead of copy-pasting the whole query each time (which could be error-prone), we can reuse the function for different names:\nhyacint \u0026lt;- flora_records_on_dutch_name(my_connection, \u0026#34;Wilde hyacint\u0026#34;) head(hyacint) %\u0026gt;% knitr::kable()    Cor_X COr_Y MetingStatusCode NaamNederlands NaamWetenschappelijk IFBLHokID     88720 208327 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 11195   24106 199925 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 9601   103111 190915 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 6990   118123 183942 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 5672   106107 182343 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 5217   105765 180785 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 4830    bosanemoon \u0026lt;- flora_records_on_dutch_name(my_connection, \u0026#34;Bosanemoon\u0026#34;) head(bosanemoon) %\u0026gt;% knitr::kable()    Cor_X COr_Y MetingStatusCode NaamNederlands NaamWetenschappelijk IFBLHokID     119247 204936 GDGA Bosanemoon Anemone nemorosa L. 10234   73658 199081 GDGA Bosanemoon Anemone nemorosa L. 8823   72752 199010 GDGA Bosanemoon Anemone nemorosa L. 8824   72921 198828 GDGA Bosanemoon Anemone nemorosa L. 8824   72874 198735 GDGA Bosanemoon Anemone nemorosa L. 8824   72887 198660 GDGA Bosanemoon Anemone nemorosa L. 8824    Remark: Do not forget to close your connection when finished.\ndbDisconnect(my_connection) The glue_sql function In order to accomplish the re-usage of a query for different input names (dutch_name), the glue_sql function is used from the glue package. The glue_sql function (and the more general glue function) provides the ability to combine text and variable values in a single charactor string (i.e. the query to execute). For each variable name required in the query (any part of your query you want to have interchangeable), a representation in the query is given by the variable name you use in R, put in between curly brackets. For example, if you have the dutch_name variable in R, you can use it inside the query as {dutch_name}:\ndutch_name \u0026lt;- \u0026#39;Jan\u0026#39; an_integer \u0026lt;- 3 a_float \u0026lt;- 2.8 glue(\u0026#39;This prints a combination of a name: {dutch_name}, an integer: {an_integer} and a float value: {a_float}\u0026#39;) ## This prints a combination of a name: Jan, an integer: 3 and a float value: 2.8  Whereas the glue function is a general function for strings, the glue_sql function is specifically created to setup queries to databases. More information is provided here and here.\n[1] Formerly, connections were made using the package RODBC\n","href":"/tutorials/tutorials/r_database_access/","title":"Read data from INBO databases (SQL Server) with R"},{"content":"library(googlesheets) library(dplyr) library(ggplot2) Setup The R package googlesheets provides the functionality to retrieve data from a google sheet. Once the registration as a user is done and the permissions are granted, it enables to read and write to google sheets. Initiation of the authentification can be done with the gs_auth command.\ngs_auth() Google will ask to grant the package the permission to access your drive. This token is saved to in a file .httr-oauth in your current working directory. Make sure this is not part of your version control system. However, as we want to be able to make the running independent from the user authentification or without needing an interactive environment. Therefore, we can store the token in a file and get the authentification from there. The code loads the token from the file if the file exists. Otherwise a new token is created and stored in the file.\nif (file_test(\u0026#34;-f\u0026#34;, \u0026#34;googlesheets_token.rds\u0026#34;)) { gs_auth(token = \u0026#34;googlesheets_token.rds\u0026#34;) } else { # first get the token, but not caching it in a local file token \u0026lt;- gs_auth(cache = FALSE) # save the token into a file in the current directory saveRDS(token, file = \u0026#34;googlesheets_token.rds\u0026#34;) } Make sure the token is savely stored within your project folder without sharing it or putting it into the version control history. If you need more power (e.g. necessity of integration services such as Travis CI), check the encryption options in this manual.\nOnce registered, an overview of your available google sheets is provided as follows, with the option to provide a (regular) expression to search for in the names:\ngs_ls(\u0026#34;tutorial_example\u0026#34;) ## # A tibble: 2 x 10 ## sheet_title author perm version updated sheet_key ws_feed ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 tutorial_e… stijn… rw old 2018-12-13 16:37:58 1O9W_c0A… https:… ## 2 tutorial_e… stijn… rw old 2018-12-13 16:36:56 124Od346… https:… ## # ... with 3 more variables: alternate \u0026lt;chr\u0026gt;, self \u0026lt;chr\u0026gt;, alt_key \u0026lt;chr\u0026gt;  I just want to load a google spreadsheet Consider you have the authentification. You know the name, URL or key of the spreadheet and want to read this in, well use one of the functions gs_title(), gs_url() or gs_key().\nAs an example, I want to work on the sheet called TraitsRedListSpecies. Finding the spreadheet is done as follows:\ngoose \u0026lt;- gs_title(\u0026#34;tutorial_example_spreadsheet_2\u0026#34;) ## Sheet successfully identified: \u0026quot;tutorial_example_spreadsheet_2\u0026quot;  An overview of the different sheets provides sufficient information to retrieve the dataset and work with them:\ngs_ws_ls(goose) ## [1] \u0026quot;Tbl1 Ringgegevens\u0026quot; \u0026quot;Tbl2 Koppels\u0026quot; \u0026quot;Tbl3 Waarnemingen\u0026quot; ## [4] \u0026quot;Tbl4 Nesten\u0026quot; \u0026quot;Tbl5 Nestcontroles\u0026quot; \u0026quot;Tbl6 Uitkipsucces\u0026quot; ## [7] \u0026quot;Tbl7 Kuikenoverleving\u0026quot; \u0026quot;Tbl8 Datums\u0026quot;  So, getting the BreedingBirds sheet into a native R object can be done by reading the specific worksheet:\ngoose_data \u0026lt;- goose %\u0026gt;% gs_read(ws = \u0026#34;Tbl1 Ringgegevens\u0026#34;) ## Accessing worksheet titled 'Tbl1 Ringgegevens'. ## Parsed with column specification: ## cols( ## Ringnummer = col_character(), ## Kleurring = col_character(), ## Soort = col_character(), ## Leeftijdscategorie = col_character(), ## Geslacht = col_character(), ## `Datum vangst` = col_character(), ## `Locatie vangst` = col_character(), ## Vangtype = col_character(), ## Gestorven = col_character() ## )  Inspecting the data of the sheet:\nclass(goose_data) ## [1] \u0026quot;tbl_df\u0026quot; \u0026quot;tbl\u0026quot; \u0026quot;data.frame\u0026quot;  head(goose_data) ## # A tibble: 6 x 9 ## Ringnummer Kleurring Soort Leeftijdscatego… Geslacht `Datum vangst` ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 B1634 162 Bran… Adult Vrouw 7/20/2000 ## 2 B3204 181 Bran… Adult \u0026lt;NA\u0026gt; 8/9/2009 ## 3 K23913 636 Bran… Adult \u0026lt;NA\u0026gt; 8/9/2009 ## 4 K46701 \u0026lt;NA\u0026gt; Bran… Adult Onbekend 7/2/2012 ## 5 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 6 K46702 \u0026lt;NA\u0026gt; Bran… Adult Onbekend 7/2/2012 ## # ... with 3 more variables: `Locatie vangst` \u0026lt;chr\u0026gt;, Vangtype \u0026lt;chr\u0026gt;, ## # Gestorven \u0026lt;chr\u0026gt;  We can now start working on the data, e.g. make a plot of the number of species in each location:\nggplot(data = goose_data %\u0026gt;% filter(!is.na(`Locatie vangst`)), aes(x = `Locatie vangst`)) + geom_bar() ","href":"/tutorials/tutorials/r_google_sheets/","title":"Read data from google sheet"},{"content":"","href":"/tutorials/tags/spreadsheet/","title":"spreadsheet"},{"content":"","href":"/tutorials/authors/","title":"Authors"},{"content":"","href":"/tutorials/search/","title":"Search"}]
