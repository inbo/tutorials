[{"content":"Introduction Welcome to the tutorials website of the Research Institute for Nature and Forest (INBO). It contains a collection of guides, tutorials and further reading material on the installation, use and development of (research) software at our institute.\nGetting started When this is your first time checking this website, welcome! If you are looking for something specific, the website provides 3 options to search for relevant topics:\n Categories: A (limited) set of general categories to classify the tutorials. For example, installation for support during installation of software or r for tutorials about the R programming language. Click on a category to get the relevant tutorials. Tags: Check the cloud of tags listed on each page of the website and pick an item that would be relevant for your topic. By clicking on a specific tag, you will get an overview of the tutorials with this tag. Search: Type any word in the search form and you will get a dropdown list of the pages which contain the word you are looking for.  Website menu In addition to the navigation options explained in the previous section, pages are divided into 3 main section accessible in the menu:\n Tutorials: The main bulk of pages, providing tutorials on a variety of topics. Articles: Provides useful background information about topics, links to literature, etc. Installation: Installation instructions for both system administrators and users.  The absence of a more granular menu is a deliberate decision to overcome the requirement to recreate a website menu when new tutorials are written. By using categories and tags, more flexibility is provided. The tags list, categories overview and search options are automatically updated when a new tutorial is added to the website.\nWant to contribute? Great news! By sharing information, we can effectively help each other. Moreover, a tutorial is useful for your future self as well ;-). Discovered something that could be useful to your colleagues? Instead of keeping a note or document on your own computer or drive, make it a tutorial and share it. Note that links/references to other resources are also useful to add, as this will enable others to find these resources.\nCheck the create tutorial page for more information on how to contribute to this website!\nTechnical information can be found in the Readme of the code repository.\nQuestions? If you have any questions, do not hesitate to contact the website maintainers (main contributors listed here), create a new issue if you already have a GitHub account or email the IT helpdesk for more information.\n","href":"/","title":"Home"},{"content":"","href":"/articles/","title":"Articles"},{"content":"","href":"/tutorials/","title":"Tutorials"},{"content":"Introduction First of all, thanks to consider making a new tutorial! By providing a tutorial, you are actively supporting your colleagues and the wider community and making work more efficient.\nIn this page, the roadmap towards a new tutorial will be explained.\nWriting a tutorial Each tutorial is setup in a subfolder of the content/tutorials folder. Within this folder, different files and formats may exist as you create them, but a file with the name index.md will be used for the tutorials website. So, you can create the tutorial directly in markdown or create it based on a Rmarkdown, a Jupyter notebook, quarto or any other format, as long as there is a single markdown file with the name index.md in your tutorial folder.\nThere are different ways to create this file. We will provide in this document specific instructions for markdown an Rmarkdown based tutorials. Still, if you got useful information or text in another format or you got stuck with the description, do not hesitate to describe your submission in a new issue . If you do not have a Github account, ask the IT helpdesk. We will try to support you as soon as possible.\n1. writing a markdown tutorial If you are directly writing your documentation in markdown syntax, you can either work online using Github or work on your own computer while using git.\nOnline submission Although providing less functionalities (adding custom images is not directly supported), the Github interface provides already a powerfull interface to add new content.\nTo write a new tutorial online, go to the INBO-tutorials Github repository and navigate to the /content/tutorials page, or use this link.\nNext, click the Create new file button\nYou will be directed to a new page and asked to Name your file\u0026hellip;\nProviding this name is very important, so make sure:\n provide a folder name + / + index.md the folder name needs to be all lowercase the folder name should not have spaces, but you can use _ to separate words provide a meaningful name without dates or names  For example: r_tidy_data/index.md, database_query_inboveg/index.md or statistics_regression/index.md\nNotice: The moment you type the /, Github will guide you by translating this to a folder name.\nNext, in the edit field you can start typing your tutorial. The header has an agreed format and you should copy paste this to start with:\n--- title: \u0026quot;YOUR TITLE\u0026quot; description: \u0026quot;SHORT DESCRIPTION ON TUTORIAL\u0026quot; authors: [YOUR_AUTHOR_ID] date: YYYY-MM-DD categories: [\u0026quot;YOUR_CATEGORY\u0026quot;] tags: [\u0026quot;FIRST_TAG\u0026quot;, \u0026quot;SECOND_TAG\u0026quot;, \u0026quot;...\u0026quot;] --- # your text starts here... Replace all the CAPITAL words with appropriate information:\n a short clear title a single or two line description your author id: for each author on INBO tutorials, there\u0026rsquo;s a AUTHOR_ID.toml file under data/authors. If it\u0026rsquo;s your first contribution, please choose an author id (suggestion: your full name, lowercase, with no space) and add a file about yourself there. Start by copying the content from a preexisting author file. If a tutorial is written by multiple authors, use the following syntax in the header: authors: [1ST_AUTHOR_ID, 2ND_AUTHOR_ID, ...] the creation date, in the specified format (year-month-day), e.g. 2019-01-04 or 2018-04-02 one or two categories from the specified list available here multiple tags you can pick yourself, all lowercase words. Have a look at the current wordcloud to check which would be useful for your submission.  Underneath the last triple dahs (---), you can write the tutorial as you like using markdown syntax. Use the Preview button to check how it would look like rendered on a website.\nNotice: You can add images from online source by using the URL of the image, e.g. ![short image description](http://.../my_image.jpg). For example, https://www.inbo.be/sites/all/themes/bootstrap_inbo/img/inbo/logo_nl.png will include the INBO logo into your document:\nIf you are ready, commit your file to the website maintainers by filling in the boxes:\n Create new file: exchange this by a short message about the additions, e.g. Add tutorial to explain tidy data in R or Add tutorial about database queries in inboveg,\u0026hellip; Add an optional extended description: If you think more background info is suitable, add that in this box. yourgithubnam-patch you can replace this by the same name as your folder name above (e..g. r_tidy_data) to clarify your submission.  (the checkbox will always be on create a new branch, this is also the required option)\nNext, click commit new file and your submission will be reviewed by the website maintainers. If accepted, the tutorial will be automatically integrated in the tutorials website.\nUsing git (or Github Desktop, Rstudio,\u0026hellip;) When you ever used git or Github before, either using the command line, rstudio, Github Desktop,\u0026hellip; you can work on your own computer and submit the tutorial using git. In this section, we assume you are familiar to the git command or interface and have some basic knowledge. If not, no worries, we have a dedicated course to get you up to speed, see the INBO git course.\nNotice: The links in the different steps will refer to the Rstudio steps in the INBO git tutorial, but can be done using other interfaces or the command line as well. Pick the one you prefer.\nIf it is your first submission using your computer, clone the INBO tutorials repository (so use the clone button on the https://github.com/inbo/tutorials page!).\nNext, we use the git workflow to submit the new tutorial:\n Update your code and create a new branch, cf. STEP 1 of the workflow. Provide a useful name for your branch related to the topic of your tutorial, e.g. r_tidy_data or database_query_inboveg and not my_tutorial. Navigate to the subfolder content/tutorials and create a new subfolder in this directory. This will be the directory of your tutorial. Again, use a representative name for the directory name similar to the branch name. Within the folder, create a new file markdown index.md. The header of this file has an agreed format and you should copy paste this to start with:  --- title: \u0026quot;YOUR TITLE\u0026quot; description: \u0026quot;SHORT DESCRIPTION ON TUTORIAL\u0026quot; authors: [YOUR_AUTHOR_ID] date: YYYY-MM-DD categories: [\u0026quot;YOUR_CATEGORY\u0026quot;] tags: [\u0026quot;FIRST_TAG\u0026quot;, \u0026quot;SECOND_TAG\u0026quot;, \u0026quot;...\u0026quot;] --- # your text starts here... Replace all the CAPITAL words with appropriate information:\n a short clear title a single or two line description your author id: for each author on INBO tutorials, there\u0026rsquo;s a AUTHOR_ID.toml file under data/authors. If it\u0026rsquo;s your first contribution, please choose an author id (suggestion: your full name, lowercase, with no space) and add a file about yourself there. Start by copying the content from a preexisting author file. If a tutorial is written by multiple authors, use the following syntax in the header: authors: [1ST_AUTHOR_ID, 2ND_AUTHOR_ID, ...] the creation date, in the specified forma (year-month-day), e.g. 2019-01-04 or 2018-04-02 one or two categories from the specified list available here multiple tags you can pick yourself, all lowercase words. Have a look at the current wordcloud to check which would be useful for your submission.  Underneath the last triple dahs (---), you can write the tutorial as you like using markdown syntax.\n add/commit the file to git, cf. STEP 2 of the workflow. You can commit the tutorial all in once or split the commits in different steps, that is up to you. To make sure your work is updated online as well, push the tutorial as in STEP 3 of the workflow. When ready, push your tutorial a last time and create a Pull request to the website maintainers as explained in STEP 4 of the workflow.  After you pull request, your submission will be checked and reviewed. When accepted and merged, you tutorial will be online.\n2. Writing an Rmarkdown tutorial As you are writing the tutorial in Rmarkdown, we assume you are using Rstudio to write the tutorial. In this section, we assume you are familiar to the Rstudio git interface and have some basic knowledge. If not, no worries, we have a dedicated course to get you up to speed with git in Rstudio, see the INBO git course.\nIf it is your first submission using your computer, clone the INBO tutorials repository (so use the clone button on the https://github.com/inbo/tutorials page!).\nNext, we use the git workflow to submit the new tutorial:\n Update your code and create a new branch, cf. STEP 1 of the workflow. Provide a useful name for your branch related to the topic of your tutorial, e.g. r_tidy_data or database_query_inboveg and not my_tutorial. Navigate to the subfolder content/tutorials and create a new subfolder in this directory. This will be the directory of your tutorial. Again, use a representative name for the directory name similar to the branch name. Within the folder, create a new file markdown index.Rmd. The header of this file has an agreed format and you should copy paste this to start with:  --- title: \u0026quot;YOUR TITLE\u0026quot; description: \u0026quot;SHORT DESCRIPTION ON TUTORIAL\u0026quot; authors: [YOUR_AUTHOR_ID] date: YYYY-MM-DD categories: [\u0026quot;YOUR_CATEGORY\u0026quot;] tags: [\u0026quot;FIRST_TAG\u0026quot;, \u0026quot;SECOND_TAG\u0026quot;, \u0026quot;...\u0026quot;] output: md_document: preserve_yaml: true variant: gfm+footnotes --- # your text starts here... Replace all the CAPITAL words with appropriate information:\n a short clear title a single or two line description your author id: for each author on INBO tutorials, there\u0026rsquo;s a AUTHOR_ID.toml file under data/authors. If it\u0026rsquo;s your first contribution, please choose an author id (suggestion: your full name, lowercase, with no space) and add a file about yourself there. Start by copying the content from a preexisting author file. If a tutorial is written by multiple authors, use the following syntax in the header: authors: [1ST_AUTHOR_ID, 2ND_AUTHOR_ID, ...] the creation date, in the specified forma (year-month-day), e.g. 2019-01-04 or 2018-04-02 one or two categories from the specified list available here multiple tags you can pick yourself, all lowercase words. Have a look at the current wordcloud to check which would be useful for your submission. leave the output section as it is  Underneath the last triple dash (---), you can write the tutorial as you like using markdown syntax and add code chunks to run R code.\nAs the Rmarkown file will not be part of the website, make sure to click the knit button to create the equivalent markdown file (index.md) in the same directory:\nNotice: always knit the Rmarkdown file before you start committing the changes!\n add/commit both files to git, cf. STEP 2 of the workflow. You can commit the tutorial all in once or split the commits in different steps, that is up to you. To make sure your work is updated online as well, push the tutorial as in STEP 3 of the workflow. When ready, push your tutorial a last time and create a Pull request to the website maintainers as explained in STEP 4 of the workflow.  After you pull request, your submission will be checked and reviewed. When accepted and merged, you tutorial will be online.\nTips In the rare cases where your tutorial includes a link to a raw HTML document (to be delivered as-is, without any visual integration in the rest of the INBO tutorials website), you can proceed like this:\n Place your HTML document in static/html (in the top directory of the site, outside of tutorials, articles, \u0026hellip;) To link it, use a relative URL (.. can be used to point to the parent directory). The content of the static directory is accessible at the root URL.  This should be more clear with a practical example:\nThe file at content/tutorials/r_beginners/index.md has a link to a static HTML file in static/html/Rmarkdown_oefening_resultaat.html.\nSince that tutorial page will be visible at the tutorials/r_beginners/ relative URL and that the static file will be accessible at html/Rmarkdown_oefening_resultaat.html, you need to create a link that goes up twice to reach the site root before going down in the html directory, for example:\n[Oefening](../../html/Rmarkdown_oefening_resultaat.html)\n","href":"/create_tutorial/","title":"Create tutorial"},{"content":"Check the administrator installation or user installation pages in function of the administrator rights on your computer.\n","href":"/installation/","title":"Installation notes"},{"content":"","href":"/categories/","title":"Categories"},{"content":"","href":"/authors/","title":"Authors"},{"content":"","href":"/authors/dirkmaes/","title":"dirkmaes"},{"content":"","href":"/authors/elslommelen/","title":"elslommelen"},{"content":"","href":"/authors/fredericpiesschaert/","title":"fredericpiesschaert"},{"content":"","href":"/tags/inbodb/","title":"inbodb"},{"content":"See the vignette/tutorial at https://inbo.github.io/inbodb/articles/get_data_meetnetten.html\n","href":"/tutorials/vignette_inbodb_get_data_meetnetten/","title":"inbodb: How to retrieve data from the Meetnetten database"},{"content":"See the vignette/tutorial at https://inbo.github.io/inbodb/articles/get_data_taxonlijsten.html\n","href":"/tutorials/vignette_inbodb_get_data_taxonlijsten/","title":"inbodb: How to retrieve data from the Taxonlijsten database"},{"content":"","href":"/tags/r/","title":"r"},{"content":"","href":"/categories/r/","title":"r"},{"content":"","href":"/tags/","title":"Tags"},{"content":"","href":"/authors/toonwestra/","title":"toonwestra"},{"content":"","href":"/tags/vignette/","title":"vignette"},{"content":"Windows Installatiebestand beschikbaar via cloud.r-project.org\nIn de onderstaande tekst moet je in R-4.x.y zowel x als y vervangen door een cijfer om zo het huidige versienummer te krijgen. Dus voor versie R-4.0.0 is x = 0 en y = 0.\nNieuwe installatie van R  Voer het bestand R-4.x.y-win.exe uit. Kies Nederlands als taal voor de installatie en klik op OK. klik op Volgende. Aanvaard de licentievoorwaarden door op Volgende te klikken. Wijzig de standaarddoelmap naar C:\\R\\R-4.x.y en klik op Volgende. Selecteer de gewenste componenten en klik op Volgende. Je MOET deze standaardwaarden laten staan. Opstartinstelling aanpassen: Kies Nee en klik op Volgende. Geef de map voor het start menu en klik op Volgende. Je mag de standaardwaarde gebruiken. Vink de gewenste extra snelkoppelingen aan (default is ok), alle register entries aan en klik op Volgende. R wordt nu geïnstalleerd. Klik op Voltooien als de installatie afgelopen is. Ga naar Start en tik “Omgevingsvariabelen” in het veld Programma's en variabelen zoeken. Selecteer De omgevingsvariabelen van het systeem bewerken. Selecteer het tabblad Geavanceerd en klik op de knop Omgevingsvariabelen. Ga na of er een systeemvariabele R_LIBS_USER met waarde C:/R/library bestaat1. Indien niet, maak deze aan met de knop Nieuw. Sluit al deze schermen via de OK knop. Kopieer het bestand Rprofile.site naar etc in de doelmap waar je R geïnstalleerd hebt (C:\\R\\R-4.x.y). Hierbij moet je het bestaande bestand overschrijven. Zorg dat de gebruiker schrijfrechten heeft voor C:\\R\\R-4.x.y\\library en C:\\R\\library.  Afwijkingen t.o.v. default installatie  Wijzig de standaarddoelmap naar C:\\R\\R-4.x.y alle gebruikers moeten volledige rechten hebben in  C:\\R\\library C:\\R\\R-4.x.y\\library   Systeemvariable R_LIBS_USER instellen op C:/R/library (verplicht forward slashes) Rprofile.site in C:\\R\\R-4.x.y\\etc overschrijven  R mag niet met admininstratorrechten gestart worden. Anders worden een aantal packages met administrator rechten geïnstalleerd waardoor de gebruiker ze niet meer kan updaten.\nStart R als een gewone gebruiker om de configuratie te testen.\nUpgrade van een bestaande R installatie Deze instructies veronderstellen dat R en RStudio in het verleden reeds geïnstalleerd werden volgens de bovenstaande instructies. Indien dat niet het geval is, volg dan de instructies voor een nieuwe installatie.\n Voer het bestand R-4.x.y-win.exe uit. Kies Nederlands als taal voor de installatie en klik op OK. klik op Volgende. Aanvaard de licentievoorwaarden door op Volgende te klikken. Wijzig de standaarddoelmap naar C:\\R\\R-4.x.y en klik op Volgende. Selecteer de gewenste componenten en klik op Volgende. Je MOET deze standaardwaarden laten staan. Opstartinstelling aanpassen: Kies Nee en klik op Volgende. Geef de map voor het start menu en klik op Volgende. Je mag de standaardwaarde gebruiken. Vink de gewenste extra snelkoppelingen aan (default is ok), alle register entries aan en klik op Volgende. R wordt nu geïnstalleerd. Klik op Voltooien als de installatie afgelopen is. Kopieer het bestand Rprofile.site naar etc in de doelmap waar je R geïnstalleerd hebt (C:\\R\\R-4.x.y). Hierbij moet je het bestaande bestand overschrijven. Zorg dat de gebruiker schrijfrechten heeft voor C:\\R\\R-4.x.y\\library De nieuwe R versie is klaar voor gebruik. De gebruiker moet RStudio bijwerken.  R mag niet met admininstratorrechten gestart worden. Anders worden een aantal packages met administrator rechten geïnstalleerd waardoor de gebruiker ze niet meer kan updaten.\nStart R als een gewone gebruiker om de configuratie te testen.\nInhoud Rprofile.site options( papersize = \u0026quot;a4\u0026quot;, tab.width = 2, width = 80, help_type = \u0026quot;html\u0026quot;, keep.source.pkgs = TRUE, xpinch = 300, ypinch = 300, yaml.eval.expr = TRUE, repos = c( CRAN = \u0026quot;https://cloud.r-project.org/\u0026quot;, INLA = \u0026quot;https://inla.r-inla-download.org/R/stable\u0026quot;, inbo = \u0026quot;https://inbo.r-universe.dev\u0026quot; ), pkgType = \u0026quot;both\u0026quot;, install.packages.check.source = \u0026quot;no\u0026quot;, inbo_required = c(\u0026quot;checklist\u0026quot;, \u0026quot;fortunes\u0026quot;, \u0026quot;remotes\u0026quot;, \u0026quot;INBOmd\u0026quot;, \u0026quot;INBOtheme\u0026quot;), qgisprocess.detect_newer_qgis = TRUE ) # display fortune when starting new interactive R session if (interactive() \u0026amp;\u0026amp; \u0026quot;fortunes\u0026quot; %in% rownames(utils::installed.packages())) { tryCatch( print(fortunes::fortune()), error = function(e) { invisible(NULL) } ) } if (\u0026quot;checklist\u0026quot; %in% rownames(utils::installed.packages())) { options( lintr.linter_file = system.file(\u0026quot;lintr\u0026quot;, package = \u0026quot;checklist\u0026quot;) ) } if ( interactive() \u0026amp;\u0026amp; !all(getOption(\u0026quot;inbo_required\u0026quot;) %in% rownames(utils::installed.packages())) ) { warning( c( \u0026quot;\\n\u0026quot;, rep(\u0026quot;^\u0026quot;, getOption(\u0026quot;width\u0026quot;)), \u0026quot;\\nThis R installation lacks some required INBO packages.\u0026quot;, \u0026quot;\\nPlease install them using the code below:\\n\u0026quot;, \u0026quot;\\ninstall.packages(c(\u0026quot;, paste0( \u0026quot;\\\u0026quot;\u0026quot;, getOption(\u0026quot;inbo_required\u0026quot;)[ !getOption(\u0026quot;inbo_required\u0026quot;) %in% rownames(utils::installed.packages()) ], \u0026quot;\\\u0026quot;\u0026quot;, collapse = \u0026quot;, \u0026quot; ), \u0026quot;))\\n\\n\u0026quot;, rep(\u0026quot;^\u0026quot;, getOption(\u0026quot;width\u0026quot;)) ) ) }  Ubuntu Instructies om R te installeren onder Ubuntu zijn beschikbaar op https://cran.r-project.org/bin/linux/ubuntu. Na de installatie kopier je Rprofile.site naar /etc/R.\n  Het moeten forward slashes zijn.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","href":"/installation/administrator/admin_install_r/","title":"Install R"},{"content":"","href":"/tags/installation/","title":"installation"},{"content":"","href":"/categories/installation/","title":"installation"},{"content":"","href":"/authors/thierryo/","title":"thierryo"},{"content":"","href":"/tags/git/","title":"git"},{"content":"Fix merge conflict with a pull request Sometimes a pull request warns about a merge conflict. A merge conflict occurs when one or more lines were altered in the base branch so that a merge conflict arises in the feature branch. The warning looks like the image below.\nYou can fix most merge conflicts in the browser. First push the Resolve conflict button in the warning. The website sends you to a page with a list of all merge conflicts. The webpage highlights them by a red vertical line. Every merge conflict inserts three delimiters:\n \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; feature branch name: the start of the merge conflict ======: the separator between the content of both branches \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; base branch name: the end of the merge conflict  Fix the merge conflict by changing the source. Often you can fix it by simply deleting the content of one of the branches within the conflict. Potentially you need to keep a mix of both. Don\u0026rsquo;t forget to also delete the three delimiters when you\u0026rsquo;re ready. Should fixing the merge conflict be more complicated, then you probably better fix them via the command line. In our example we solved the conflict by keeping the content of the feature branch.\nOnce you have fixed all merge conflicts, go to the top of the page. There hit the Mark as resolved button.\nFinally you store the fixes in a new commit by hitting the Commit merge button.\n","href":"/tutorials/git_merge_conflict/","title":"Handle Git conflicts via GitHub.Com"},{"content":"","href":"/tags/version-control/","title":"version control"},{"content":"","href":"/categories/version-control/","title":"version control"},{"content":"","href":"/tags/brms/","title":"brms"},{"content":"","href":"/tags/generalized-linear-regression/","title":"generalized linear regression"},{"content":"","href":"/tags/mixed-models/","title":"mixed models"},{"content":"","href":"/tags/stan/","title":"stan"},{"content":"","href":"/categories/statistics/","title":"statistics"},{"content":"In the fall of 2023, a tutorial on Bayesian statistics with the brms packages was organised at INBO. Before you start the tutorial, please follow the instructions in this R script to install brms properly. For more advanced use cases, we added an example of how to directly use stan via the rstan package. All course material can be found in Dutch and English on the following pages:\n The English tutorial with all course material to test the examples yourself. The Dutch tutorial with all course material to test the examples yourself. General files useful for both tutorials.  ","href":"/tutorials/r_brms/","title":"Tutorial Bayesian statistics with brms"},{"content":"","href":"/categories/databases/","title":"databases"},{"content":"","href":"/categories/development/","title":"development"},{"content":"","href":"/tags/open-science/","title":"open science"},{"content":"","href":"/tags/packages/","title":"packages"},{"content":"","href":"/tags/python/","title":"python"},{"content":"At the Research Institute for Nature and Forest (INBO), we are eager to sustain, promote and develop open-source software that is relevant to biodiversity researchers! This page lists R and Python packages which INBO developed or made a significant contribution to. Several of these packages continue being developed.\nPlease, feel free to try out packages! If you encounter a problem or if you have a suggestion, we encourage you to post an issue on the package’s code repository. You can also directly contribute improvements with a pull request.\nThe package links below refer to the package’s documentation website, if available. When there is no documentation website, often one or more vignettes are available within the package, describing the package’s purpose and demonstrating its use.\nThe following table gives a quick overview:\n   Research stage Related INBO packages     Study design grts, protocolhelper   Retrieve data: general frictionless, inbodb, inbospatial   Retrieve data: environmental pydov, wateRinfo, watina, inbolims   Retrieve data: biological bioRad, camtrapdp, etn, forrescalc, gulltracking, n2khab, pyinaturalist, rgbif   Store data frictionless, git2rdata   Validate data pywhip   Analyze data: geoprocessing qgisprocess, inbospatial   Analyze data: graphs inboggvegan   Analyze data: models dhcurve, effectclass, gwloggeR, inlatools, multimput, niche_vlaanderen   Analyze data: indices LSVI   Publish INBOmd, INBOtheme, checklist   Miscellaneous (mixed content) inborutils, checklist    Study design  R package grts: draw a sample from a sampling frame with the Generalized Random Tessellation Stratified (GRTS) sampling strategy. R package protocolhelper: provides templates for protocols and helper functions to start developing a new protocol or move an existing protocol to the INBO protocols repository  Retrieve data General  R package frictionless: read and write Frictionless Data Packages. A Data Package is a simple container format and standard to describe and package a collection of (tabular) data. It is typically used to publish FAIR and open datasets. R package inbodb: connect to and retrieve data from databases on the INBO server, with dedicated functions to query some of these databases. R package inbospatial: provides functions to retrieve data from web feature, web mapping and web coverage services.  Environmental data  Python package pydov: to query and download data from Databank Ondergrond Vlaanderen (DOV). DOV aggregates data about soil, subsoil and groundwater of Flanders and makes them publicly available. Interactive and human-readable extraction and querying of the data is provided by a web application, whereas the focus of this package is to support machine-based extraction and conversion of the data. R package wateRinfo: facilitates access to waterinfo.be, a website managed by the Flanders Environment Agency (VMM) and Flanders Hydraulics Research. The website provides access to real-time water and weather related environmental variables for Flanders (Belgium), such as rainfall, air pressure, discharge, and water level. The package provides functions to search for stations and variables, and download time series. R package watina: provides functions to query and process data from the Watina database (mainly groundwater data). R package inbolims: provides functions to query and process data from the INBO Laboratory Information Management System (LIMS), e.g. soil physical and chemical analyses.  Biological data  R package bioRad: extract, visualize and summarize aerial movements of birds and insects from weather radar data. R package camtrapdp: aims to experiment and test visualization functionalities for camera trap data formatted following the Camera Trap Data Package standard. R package etn: provides functionality to access and process data from the European Tracking Network (ETN) database hosted by the Flanders Marine Institute (VLIZ) as part of the Flemish contribution to LifeWatch. R package forrescalc: provides aggregated values on dendrometry, regeneration and vegetation of the Flemish forest reserve monitoring network, and functions to derive these data starting from individual tree measurements in Fieldmap. R package gulltracking: provides functionality to annotate GPS tracking data of gulls stored in Movebank. These data are collected by the LifeWatch GPS tracking network for large birds. R package n2khab: provides preprocessed reference data (including checklists, spatial habitat distribution, administrative \u0026amp; environmental layers, GRTSmaster_habitats) and preprocessing functions, supporting reproducible and transparent analyses on Flemish Natura 2000 (n2k) habitats (hab) and regionally important biotopes (RIBs). Python package pyinaturalist: Python client for the iNaturalist APIs. R package rgbif: provides an R interface to the Global Biodiversity Information Facility API.  Store data  R package frictionless: read and write Frictionless Data Packages. A Data Package is a simple container format and standard to describe and package a collection of (tabular) data. It is typically used to publish FAIR and open datasets. R package git2rdata: an R package for writing and reading dataframes as plain text files. Important information is stored in a metadata file, which allows to maintain the classes of variables. git2rdata is ideal for storing R dataframes as plain text files under version control, as it strives to minimize row based diffs between two consecutive commits. The package is intended to facilitate a reproducible and traceable workflow.  Validate data  Python package pywhip: a package to validate data against whip specifications, a human and machine-readable syntax to express specifications for data.  Analyze data Geoprocessing  R package qgisprocess: R interface to the geoprocessing algorithms of QGIS and other providers that can interface with QGIS (e.g. GDAL, GRASS GIS, SAGA and WhiteboxTools). R package inbospatial: provides a collection of useful R functions for spatial data retrieval and analysis  Make graphs  R package inboggvegan: provides R functions for multivariate plots. More specifically, extended biplot and screeplot functionality is offered for the vegan package.  Fit models and make model predictions  R package dhcurve: an R package to predict tree height for a given girth, based on a model and data on tree height, tree girth, tree species and location (in Dutch). R package effectclass: an R package to classify and visualize modelled effects by comparing their confidence interval with thresholds. R package gwloggeR: an R package to detect anomalous observations in timeseries of groundwater loggerdata (water pressure and air pressure). Additive outliers, temporal changes and level shifts are detected. R package inlatools: provides a set of functions which can be useful to diagnose INLA models: calculating Pearson residuals, simulation based checks for over- or underdispersion, simulation based checks for the distribution, visualising the effect of the variance or precision on random effects (random intercept, first order random walk, second order random walk). The functions can be useful to choose sensible priors and diagnose the fitted model. R package multimput: an R package that assists with analysing datasets with missing values using multiple imputation. Python package niche_vlaanderen: Python package to run the NICHE Vlaanderen model. Based on calculated abiotic properties of the location, NICHE Vlaanderen determines whether certain vegetation types can develop. An additional flooding module allows the user to test whether the predicted vegetations are compatible with a particular flooding regime. The package is a redevelopment of an existing ArcGIS plugin in Python, without external non-open source dependencies.  Calculate indices  R package LSVI: bundles a number of functions to support researchers in determining the local conservation status (‘LSVI’) of Natura 2000 habitats in Flanders. Several functions retrieve the criteria and/or associated species lists for determining the LSVI. A specific function allows to calculate the LSVI. The package is written in Dutch.  Publish your workflow and discuss your results  R package INBOmd: provides several styles for rmarkdown files and several templates to generate reports, presentations and posters. The styles are based on the corporate identity of INBO and the Flemish government. All templates are based on bookdown, which is an extension of rmarkdown. bookdown is taylored towards writing books and technical documentation. R package INBOtheme: contains ggplot2 themes for INBO, the Flemish government and Elsevier journals. The documentation website includes a set of example figures for each available theme. R package checklist: provides an elaborate and strict set of checks for R packages and R code, which includes setting up GitHub actions to publish to Zenodo.  Last but not least: miscellaneous!  R package inborutils: provides a collection of useful R utilities and snippets that we consider recyclable for multiple projects. The functions are either out of scope or just not mature enough to include as extensions to existing packages. R package checklist: provides an elaborate and strict set of checks for R packages and R code.  ","href":"/articles/inbo_software/","title":"Software by INBO: packages for environmentalists and ecologists!"},{"content":"","href":"/authors/florisvdh/","title":"florisvdh"},{"content":"","href":"/tags/gis/","title":"gis"},{"content":"","href":"/categories/gis/","title":"gis"},{"content":"","href":"/tags/grids/","title":"grids"},{"content":"","href":"/tags/mgrs/","title":"mgrs"},{"content":"Summary Grids are widely used in biodiversity monitoring to define the basic spatial units for data collection, data processing or mapping. In this post I explain differences between the UTM and MGRS grids, provide references to their full definition and refer to some software implementations for MGRS.\nMain points:\n The UTM grid and the MGRS grid outside the polar regions are both derived from the UTM (Universal Transverse Mercator) map projection system. Both grids have a world-wide coverage, but the polar regions are only covered by the MGRS grid. In the polar regions, the MGRS grid is derived from the UPS (Universal Polar Stereographic) map projection. The major divisions of the UTM grid are the UTM zones, defined by meridians that are 6° apart. The understanding of the term ‘UTM grid’ in this post only involves grid lines tied to the UTM coordinate system. In the area where UTM applies, the major partitions of the MGRS grid are the grid zones, which are a further division of UTM zones according to specific parallels, further amended by shifting the meridian borders of some grid zones. Grid zone borders generate extra cell clipping compared to the UTM grid. Apart from deviations and extra clips as a consequence of the grid zones, the MGRS grid cell pattern outside the polar regions matches that of the UTM grid. UTM grid references consist solely of digits (Arabic numerals), while MGRS grid references are alphanumeric (combination of digits \u0026amp; Latin characters). To illustrate this, following examples refer to the Belgian point location used in the text:  An UTM grid reference looks like 31.577019.5661520 (meter precision) or 31.577.5661 (kilometer precision) and separators are needed (point, slash, space) if not applying localized simplifications. Quite some variations exist. An MGRS grid reference looks like 31U ES 77019 61520 (meter precision) or 31U ES 77 61 (kilometer precision), and the spacing is optional.   When decreasing the precision of a position, coordinates are to be rounded while in (UTM and MGRS) grid references the numeric parts representing easting and northing are to be truncated. Projected coordinate reference systems (CRSs) specify a geodetic datum so that coordinates can be linked to a physical location on the Earth’s surface. Likewise, you need to specify (and be aware of) the geodetic datum when using a (UTM or MGRS) grid reference for positioning.  Terminological notes The NGA (National Geospatial-Intelligence Agency) Office of Geomatics (https://earth-info.nga.mil), part of the US Department of Defense, maintains definitions of the UTM map projection system and the MGRS grid (National Geospatial-Intelligence Agency, 2014a), as did its predecessor organisations since the inception of these systems in the 1940s (Buchroithner \u0026amp; Pfahlbusch, 2017; Department of the Army, 1956, 2001; Palombo, 2021; Snyder, 1987; Stott, 1977). Their usage of the term ‘UTM grid’ is not distinguished from the UTM map projection system, hence comes down to using numeric CRS coordinates in meters.\nAt least in the past, a ‘UTM grid’ reference system was implemented on civil maps, distinct from mere UTM CRS coordinates and with grid referencing rules to allow different levels of precision (see section UTM grid). Hence I prefer to maintain the distinction between the terms ‘UTM projection’ and ‘UTM grid’, also because the term ‘UTM grid’ is by some sources not distinguished from the ‘MGRS grid’, to which it is related.\nConfusion between UTM and MGRS is compounded by the fact that some sources distinguish these grids but describe the division of UTM zones by 8° wide latitude bands as being part of the UTM map projection system, while others attribute the latitude bands to MGRS only. In this account I use the straightforward interpretation of the UTM grid—directly tied to the UTM map projection and a corresponding CRS—which has been implemented in civil maps (Stott, 1977) and which does not involve latitude bands nor alphanumeric grid references.\nThe entwinement between UTM and MGRS is clear from the fact that MGRS was first called ‘UTMREF’ (UTM reference) in the early 1940s and this went hand in hand with the implementation of the UTM map projection by military forces (first German Army, then US Army; Buchroithner \u0026amp; Pfahlbusch (2017)). To serve NATO purposes, a few years later the referencing system was extended to the polar regions, for which the UPS map projection was used. Later on, US Army sources make a clear distinction between UTM and MGRS (e.g. Department of the Army, 1956).\nIntroduction In biodiversity monitoring the Military Grid Reference System (MGRS) is used a lot, although sometimes it is referred to as ‘the UTM grid’ (see Terminological notes). Example projects that use this spatial reference system are the Atlas Florae Europaea (Lampinen, 2013) and the European Invertebrate Survey (van Nieukerken, 1991; van Veen, 2000). In Belgium various faunistic inventories use this grid.\nIn this post I explain differences between the UTM and MGRS grid concept as adopted here, provide references to their full definition and refer to some software implementations for MGRS. The full definitions are not given here since the primary aim is to draw attention to important properties and differences. For the same reason, the UPS portion of MGRS is not further explained here. This post results from a limited literature study on these topics (see Bibliography).\nCoordinate reference systems (CRSs) that are widely used, e.g. on regional, national or global level, have obtained standardized definitions in a coordinated manner (e.g. in the EPSG dataset). However, grid reference systems—associated with a projected CRS, a map projection 1, or sets of these—seem not as well globally coordinated.\nA ‘grid’ in the context of maps refers to the horizontal and vertical lines chosen in the cartesian coordinate system of a projected CRS, usually in a regular manner (equal distance between all lines). It may also refer to the rectangular cells that emerge from these lines (Iliffe \u0026amp; Lott, 2008; Stott, 1977). The distance between the grid lines on a map depends on the used map scale.\nA map of Europe in the projected CRS ‘EPSG:3035’ (‘ETRS89-extended / LAEA Europe’). The grid is from the same CRS.  The same map of Europe in the projected CRS ‘EPSG:3035’. The grid however is from ‘EPSG:3034’ (‘ETRS89-extended / LCC Europe’), reprojected in ‘EPSG:3035’.  The lines that represent parallels and meridians of a geographic CRS are called the graticule. This map still uses the projected CRS ‘EPSG:3035’ but displays the graticule of geographic CRS ‘EPSG:4258’ (‘ETRS89’), projected in CRS ‘EPSG:3035’.  UTM projection, UTM grid and MGRS grid UTM projection UTM (Universal Transverse Mercator) was developed during World War II (Buchroithner \u0026amp; Pfahlbusch, 2017).\nUTM is a map projection system for the Earth, composed of 120 single Transverse Mercator map projections (conformal, transverse cylindrical projection) for 60 ‘UTM zones’ (numbered 1 - 60), each 6 degrees of longitude wide and reaching from 80°S to 84°N (originally 80°N; Department of the Army (1956)). 120 map projections are defined since for each UTM zone a separate map projection is needed for the northern and the southern hemisphere, the only difference being the false northing value. For example, the largest part of Belgium (and the whole of Flanders) is situated in UTM zone 31, and there the map projection ‘UTM zone 31N’ applies 2, with ‘N’ referring to the northern hemisphere.\nA map centered on Greenwich, showing how meridians define the borders of UTM zones.  UTM zones in the European area. The UTM zones are narrower in the north.  UTM projects geographical coordinates (degrees) to cartesian coordinates (meters), so the result is XY coordinates (eastings and northings) like in other map projections, often resulting in high numbers, especially for Y as it covers the whole range of 0° to 80° or 84° latitude.\nIn the case of a lower precision, coordinates are rounded (not truncated) following the usual rounding rules of science and engineering (National Geospatial-Intelligence Agency, 2014a), as would happen in other coordinate systems.\nAs an example, we calculate the projected easting (X) and northing (Y) of a point in Belgium using the sf package in R, at the millimeter precision. The CRS uses the geodetic datum ‘ETRS89’ 3 and the ‘UTM zone 31N’ map projection (CRS ‘EPSG:25831’).\nlibrary(sf) point \u0026lt;- st_sfc(st_point(c(4.1, 51.1)), crs = \u0026#34;EPSG:4258\u0026#34;) st_transform(point, \u0026#34;EPSG:25831\u0026#34;) |\u0026gt; st_coordinates() |\u0026gt; format(nsmall = 3, trim = TRUE) ## X Y ## [1,] \u0026quot;577019.527\u0026quot; \u0026quot;5661520.775\u0026quot;  A map in the Belgian CRS ‘EPSG:3812’ (‘ETRS89 / Belgian Lambert 2008’), showing provinces of Flanders. The point with geographical coordinates ‘51.1°N 4.1°E’, projected in ‘EPSG:3812’, is shown in red. It is situated in the province ‘Oost-Vlaanderen’. The superposed UTM grid lines and UTM coordinates are from the UTM CRS ‘EPSG:25831’ (‘ETRS89 / UTM zone 31N’)—reprojected in ‘EPSG:3812’—with lines every 5 km.  UTM grid A UTM grid refers to the grid lines and the resulting grid cells portrayed on a map, using the cartesian coordinate system(s) obtained by UTM projection. These lines can for example be shown every 1, 2 or 10 km, depending on the map scale. This concept complies with the general description of a ‘grid’ in the context of mapping (see Introduction).\nThe UTM grid lines are labelled with the corresponding coordinate values, often expressed as kilometers except in the SE corner of the map where full coordinates are given in meters, as explained by Stott (1977), and by National Geospatial-Intelligence Agency (2014b) in the context of the MGRS system. In the case of kilometer grid labels, the (anterior) digits before the last two (principal) digits are printed in superscript as they generally do not change within the map. This approach matches general rules of printing the various rectangular grids that emerged since World War I.\nSince the UTM projection system is composed of multiple Transverse Mercator projections, this pattern returns in the UTM grid. First of all, UTM grid cells are clipped alongside UTM zone borders and along the parallels of 80°S, 0° and 84°N, which define the borders of individual Transverse Mercator projections. Various smaller ‘grid cells’ 4 result from this, even though the majority of grid cells within a single Transverse Mercator projection are regularly ordered squares. Although each Transverse Mercator projection has a regular grid according to its cartesian coordinate system, any map projection that spans multiple UTM zones demonstrates that there is angle between the grids of different UTM zones, together constituting the UTM grid.\nBoth the occurrence of clipped grid cells and the angles between the constituting single grids make the UTM grid a relatively complex grid to use for various analytical purposes, especially when including clipped cells and when crossing the borders of a single Transverse Mercator projection.\nUTM grid in the European area, using cells of 100 km. As UTM zones (in one hemisphere) each have their own map projection, an angle arises between grid lines in two different UTM zones.  UTM grid in the area around Belgium, using cells of 100 km.  The UTM grid referencing system to identify grid cells at a specific precision follows the following numerical syntax, as explained on civil maps that use it (Stott, 1977) 5:\n[UTM zone number] [Truncated integer X-coordinate] [Truncated integer Y-coordinate]  Separators like points, slashes or spaces are needed since the number of digits can differ between easting (X) and northing (Y). The integers can be truncated at the required precision, hence representing 10p meters. As the coordinates are always truncated (not rounded) to integers, this effectively means that a UTM grid reference represents a square cell with a size that reflects the chosen precision, potentially clipped by the UTM zone border. For cells not clipped by the UTM zone border, this also means that their truncated coordinates match the UTM coordinates of their southwest corner; this holds for any precision.\nHence for the Belgian point location referred above with UTM coordinates 577019.527, 5661520.775, the UTM grid reference at 10-meter precision is 31 57701 566152.\nFor localized usage, the above grid reference can be further shortened by dropping the UTM zone number and anterior digits, e.g. if the context is within the area of one map. In many areas such simplifications lead to eastings and northings of equal length, so that separators are sometimes dropped (for the example at 10-meter precision you could have: 77016152). Clearly, you can encounter variations of the UTM grid reference syntax.\nWhen not dropping prefixes, the UTM grid reference is unique at the global level if it is also clear which hemisphere—i.e. which of the two map projections per UTM zone—applies.6\nThe concepts of UTM grid portrayal on maps and UTM grid referencing with anterior and principal digits match those of other rectangular grids that have also long been in use on civil maps, and these are all named ‘civil’ grids by Stott (1977; see also Snyder, 1987).\nNote that the UTM grid as explained here defines no subdivisions of the cartesian coordinate system by means of latitude bands, nor does it describe an alphanumeric geocoding system to uniquely identify further subdivisions. These topics are part of the Military Grid Reference System (MGRS) specification.\nMGRS grid (Military Grid Reference System) The Military Grid Reference System (MGRS) was designed during the 1940s in close association with the development of the UTM map projection system (see Terminological notes). It is used by NATO militaries for locating positions worldwide up to 1 m precision7. The MGRS specification is maintained by the NGA Office of Geomatics 8 (National Geospatial-Intelligence Agency, 2014a). Best use the foregoing reference when referring to the MGRS. Other resources describe the MGRS too, including Wikipedia contributors (2023).\nBiodiversity monitoring and perhaps other projects sometimes claim to use UTM grid references, while it is more appropriate to speak about MGRS grid references. Reasons for this confusion are given in the Terminological notes.\nThe coordinate system obtained by UTM projection is used in the MGRS definition between 80°S and 84°N, and MGRS combines this with a UPS (Universal Polar Stereographic) projection to define its grid in the polar regions. In what follows, only the UTM portion of MGRS is considered.\nSince the MGRS grid between 80°S and 84°N is based on the UTM grid, it inherits the complexities of the UTM grid already. The MGRS grid pattern between 80°S and 84°N has two important differences with the pattern of the UTM grid—apart from using a different referencing (geocoding) system:\n In MGRS the UTM grid cell pattern is further intersected by latitude bands of generally 8° wide. Intersecting UTM zones with latitude bands results in large cells called MGRS grid zones of generally 6° longitude by 8° latitude large. This intersection splits the UTM grid cells that overlap the dividing parallels, resulting in extra clipped cells compared to the UTM grid. In MGRS, some of the MGRS grid zones are wider or narrower than 6 longitudinal degrees. This has been done to avoid grid zone junctions in specific areas, e.g. Bergen and Oslo (Norway) belong to the same grid zone.  The intersection of the UTM grid by latitude bands of 8° wide forms the basis of the MGRS. This map does not include the subsequent changes to some MGRS grid zones in the north, e.g. in Norway.  Below the grid zone level, it is still the UTM grid pattern that drives the further subdivision in MGRS between 80°S and 84°N, down to the required precision, just as in other grid systems. Hence, with the exception of grid zones with shifted meridian borders, the MGRS grid still matches the UTM grid within the borders of one grid zone.\nAn important difference with UTM grid referencing however is that MGRS uses an alphanumeric grid reference instead of a numeric one as in civil grids. In MGRS, Latin characters are used in referencing both the applicable grid zone and the first subdivision thereof, as follows.\nAn MGRS ‘grid reference’ (sometimes called MGRS coordinate) is defined as:\n[Grid Zone Designation] [Grid Square ID] [Truncated integer X-coordinate] [Truncated integer Y-coordinate]  As explained further, for the Belgian point location referred earlier this comes down to 31U ES 77019 61520 at the 1 m precision, which resolves to 31U ES 77 61 at 1 km precision.\nThe integer coordinates are truncated depending on the required precision, up to zero digits (precision 100 km), with the maximum9 of five digits per coordinate corresponding to a precision of 1 meter (i.e. the unit of the UTM coordinate system). So we get:\n   precision (meter) number of digits per coordinate MGRS grid reference     100 000 0 31U ES   10 000 1 31U ES 7 6   1 000 2 31U ES 77 61   100 3 31U ES 770 615   10 4 31U ES 7701 6152   1 5 31U ES 77019 61520    Consequently, MGRS grid references need less digits per coordinate for 1 m precision than UTM grid references, and this is seen as an advantage of MGRS.10 Furthermore, the number of digits in both coordinates are equal and fixed for each precision—leading zeros need to be added at low values. Because both coordinates have an equal number of digits, the spaces can also be dropped; this is advised in a computational environment.\nThe ‘U’ latitude band goes from 48°N to 56°N. Two resulting MGRS grid zones are fully shown here: grid zones 31U (with most of Belgium) and 32U (with most of Germany).  In the UTM area of MGRS (between 80°S and 84°N), an MGRS grid reference is built as follows:\n The Grid Zone Designation (GZD), e.g. 31U, combines the UTM zone (31) with a letter code (U) that corresponds to a latitude band of generally 8° wide; hence each intersection defines the MGRS grid zone. The letter codes in the GZD represent the latitude bands and run from ‘C’ in the south to ‘X’ in the north, dropping ‘I’ and ‘O’ in order to not confuse with numeric digits. The X band is 12° high while all other bands are 8° high. Each grid zone is further subdivided into grid squares (cells) of size 100 km, using the UTM coordinate system to define the cell border coordinates as multiples of 100 000 m. As grid zones get wider towards the equator, more grid squares are present in zones near the equator than in zones located much further north or south. The Grid Square ID is a code of 2 characters that is unique within the grid zone, e.g. ES, with ‘E’—the ‘easting letter’— referring to a column and with ‘S’—the ‘northing letter’—referring to a row. These letters follow the Latin alphabet (excluding ‘I’ and ‘O’) in eastern and northern direction within a UTM zone, neglecting grid zone latitude borders and repeating the sequence as needed in northern direction. The starting letters at UTM coordinates 0, 0 determine the easting and northing letter sequences in a UTM zone, but depend on the UTM zone number as specified in the rules of the lettering scheme 11. To specify locations with precision \u0026lt; 100 km, the truncated integer coordinates of the UTM grid reference are added, thereby dropping the digits that represent the hundred-thousands and higher. At this level, a precision can be chosen of 1, 10, 100, 1000 or 10 000 meters.  Since coordinates are always truncated (not rounded), resulting in 0 to 5 integers per coordinate, this effectively means that an MGRS grid reference represents a square cell with its size reflecting the chosen precision, although potentially clipped by the UTM zone border or the latitude band. E.g. in 31U ES 7 6 X = 7 refers to the half-open interval [70, 80) km.\nWhat is the relation to coordinate reference systems (CRSs)? Projected CRSs combine a map projection (such as ‘UTM 31N’) with a geodetic datum that relates actual positions on the Earth’s surface to the (unprojected, ellipsoidal) geodetic coordinate system, which also implies defining the ellipsoid and prime meridian. The geodetic datum is a property of the geodetic CRS associated with the projected CRS.\nThis also means that without a geodetic datum, you cannot determine the physical location on the Earth’s surface that corresponds to a pair of UTM coordinates or to a UTM or MGRS grid reference 12. See the CRS tutorial for more information.\nAlthough the aim of MGRS is positioning and although it needs a compound CRS for its implementation, MGRS is a geocoding standard, not a map projection or a CRS as it adds complexity on top of a compound CRS.\nIn Belgium, the most relevant UTM CRSs for UTM zone 31 are:\n ETRS89 / UTM zone 31N (EPSG:25831) WGS 84 / UTM zone 31N (EPSG:32631) ED50 / UTM zone 31N (EPSG:23031) which was applied in older topographic maps  Run sf::st_crs(\u0026quot;EPSG:25831\u0026quot;) in R to inspect the WKT string.\nDistribution of the MGRS grid The NGA Office of Geomatics distributes worldwide WGS 84 vector layers of MGRS grid polygons up to 1 km precision.\nThe UF Geoplan Center provides an MGRS-data site that serves as a library for MGRS data and information, including geospatial layers.\nThe match of these layers with any WGS 84 UTM CRS and with the WGS 84 graticule can be easily verified post-hoc.\nCriticisms about the MGRS Criticisms about the MGRS are mainly related to the complexity of the system itself.\n The geocoding system is quite excentric compared to CRSs, especially in current times where computing power is available and handling long numeric coordinates is automated and mathematically simpler. The complexity may be a source of mistakes. A renowned geodesist has advocated on the PROJ mailing list to not use the MGRS outside of the United States.  The National Geospatial-Intelligence Agency (2014a) advises:\n Because various characteristics of MGRS are unhelpful to analytical work (see Subsection 11.8), this document suggests (but does not mandate) the following division of labor between MGRS and UTM/UPS when both are under consideration. UTM/UPS should be be used for calculations, analytical work, and storage \u0026amp; retrieval of geographic information; MGRS should be limited to notations on maps and charts, displays on end-user devices and person-to-person or person-to-machine communication.\n Software implementations MGRS and corresponding conversion methods have not been implemented in PROJ because of the technical non-fit.\nThey have been implemented in geospatial software though. The following list provides open-source implementations and may be incomplete:\n the GEOTRANS executables and C libraries of the NGA Office of Geomatics. They provide various coordinate conversions and transformations. the C++ library GeographicLib. It grew from a desire to improve on the GEOTRANS package for transforming between geographic and MGRS coordinates. GeographicLib also provides utilities that can be run in a shell environment, such as GeoConvert for converting coordinates to or from MGRS. These utilities also have an online implementation. a silent (unexposed) implementation as C code inside the GDAL driver for NITF, an imagery format of the NGA Geospatial-Intelligence Standards Working Group (GWG). the R package mgrs which uses GDAL’s MGRS implementation in the NITF-driver. the Python package mgrs (PyPi / conda-forge) which uses the GEOTRANS C library. the QGIS plugin Lat Lon Tools by the US National Security Agency (code repo). It provides several geoprocessing algorithms, which can also be accessed outside the QGIS GUI, e.g. from R 13. The plugin also provides a GUI for easy coordinate conversion based on interaction with the map.  Note that some of these implementations do not require a geodetic datum (hence CRS), but of course (converted) coordinates still need a geodetic datum to refer to an actual position on the Earth’s surface and to use them with other geospatial data.\nSince it has not been implemented in PROJ, regular geospatial R packages cannot convert to or from MGRS grid references.\nVarious online web applications allow to interactively determine MGRS coordinates, or display the MGRS grid. One example is map.army.\nAcknowledgements I am grateful to Ward Langeraert, Amber Mertens, Hans Van Calster, Toon Westra (all from INBO), Jeffrey Verbeurgt and Robson Nascimento (from NGI—National Geographic Institute) for their useful comments on earlier versions of the text.\nBibliography Buchroithner M.F. \u0026amp; Pfahlbusch R. (2017). Geodetic grids in authoritative maps – new findings about the origin of the UTM Grid. Cartography and Geographic Information Science 44 (3): 186–200. https://doi.org/10.1080/15230406.2015.1128851.\n Department of the Army (1956). Map reading. Field Manual, No. 21–26. Washington, DC, 256 p. https://ciehub.info/ref/FM/21-26.html.\n Department of the Army (2001). Map reading and land navigation. Field Manual, No. 3–25.26. Washington, DC, 209 p. https://ciehub.info/ref/FM/3-25x26.html.\n Iliffe J. \u0026amp; Lott R. (2008). Datums and Map Projections. For Remote Sensing, GIS and Surveying. 2nd edn. Whittles Publishing, Caithness, UK, 208 p.\n Lampinen R. (2013). UTM \u0026amp; MGRS – Atlas Florae Europaeae | LUOMUS. https://www.luomus.fi/en/utm-mgrs-atlas-florae-europaeae (accessed August 24, 2023).\n National Geospatial-Intelligence Agency (2014a). The Universal Grids and the Transverse Mercator and Polar Stereographic Map Projections. NGA Standardization Document - Implementation Practice, No. NGA.SIG.0012_2.0.0_UTMUPS. Washington, DC, 86 p. https://nsgreg.nga.mil/doc/view?i=4056.\n National Geospatial-Intelligence Agency (2014b). Universal grids and reference systems. NGA Standardization Document, No. NGA.STND.0037_2.0.0_GRIDS. Washington, DC, 101 p. https://nsgreg.nga.mil/doc/view?i=4057.\n Palombo S. (2021). Military Grid Reference System (MGRS). In: ArcGIS StoryMaps. https://storymaps.arcgis.com/stories/842edf2b4381438b9a4edefed124775b (accessed August 24, 2023).\n Snyder J.P. (1987). Map Projections - A Working Manual. U.S. Geological Survey Professional Paper, No. 1395. United States Government Printing Office, Washington, 397 p.\n Stott P.H. (1977). The UTM Grid Reference System. Journal of the Society for Industrial Archeology 3 (1): 1–14. https://www.jstor.org/stable/40967933.\n UF GeoPlan Center (2009). MGRS. 4 p. http://mgrs-data.org/data/documents/nga_mgrs_doc.pdf.\n van Nieukerken E.J. (1991). UTM grid: een voorschot op de toekomst. Nieuwsbrief EIS Kenniscentrum Insecten en andere ongewervelden 20: 9–14. https://natuurtijdschriften.nl/pub/1000930.\n van Veen M. (2000). EIS-nieuws: Software. Nieuwsbrief EIS Kenniscentrum Insecten en andere ongewervelden 30: 11–12. https://natuurtijdschriften.nl/pub/1001490.\n Wikipedia contributors (2023). Military Grid Reference System. In: Wikipedia, The Free Encyclopedia. https://en.wikipedia.org/w/index.php?title=Military_Grid_Reference_System\u0026amp;oldid=1140475364 (accessed August 25, 2023).\n    A map projection defines the conversion from geodetic to projected coordinates (geodetic coordinate system (CS, not CRS) + the projection (conversion) method + numeric projection (conversion) parameters), but without specifying the geodetic datum.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n OGC URN identifier of map projection UTM zone 31N: urn:ogc:def:coordinateOperation:EPSG::16031. Using projinfo, you can compare with urn:ogc:def:coordinateOperation:EPSG::16131 for map projection UTM zone 31S. More information about OGC URN identifiers can be found in the OGC URN policy and more extensively in the OGC Best Practice ‘Definition identifier URNs in OGC namespace’, the latest version of which can be found on the OGC Best Practices webpage.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n OGC URN identifier of the ETRS89 ensemble datum: urn:ogc:def:ensemble:EPSG::6258.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Perhaps these clipped grid cells are better called ‘polygons’ instead (Wikipedia contributors, 2023).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n This UTM grid reference notation is also followed in the 2001 version of US Army’s Field Manual 3-25.26 (Map Reading and Land Navigation) for the notation of UTM coordinates (Department of the Army, 2001), of course without truncation.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Some sources (e.g. UF GeoPlan Center, 2009) therefore mention the possibility to extend the UTM zone with ‘N’ or ‘S’ depending on the hemisphere, resulting in e.g. 31N 57701 566152. However this approach is now officially obsoleted, and full (i.e. globally unique) UTM coordinates should contain an MGRS grid zone prefix (see MGRS section) instead (National Geospatial-Intelligence Agency (2014b): appendix A).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Some software implementations—e.g. GeoConvert—go beyond the 1 m precision by adding more digits to the MGRS grid reference (Robson Nascimento, Jeffrey Verbeurgt, pers. comm.). Such implementations extend the official MGRS specification (National Geospatial-Intelligence Agency, 2014a), which stops at 1 m precision.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n URL: https://earth-info.nga.mil.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Some software implementations—e.g. GeoConvert—go beyond the 1 m precision by adding more digits to the MGRS grid reference (Robson Nascimento, Jeffrey Verbeurgt, pers. comm.). Such implementations extend the official MGRS specification (National Geospatial-Intelligence Agency, 2014a), which stops at 1 m precision.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Moreover, the use of Latin characters (letters) in MGRS made Morse code communication in former times less error-prone (Robson Nascimento, pers. comm.).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Moreover, two different lettering schemes are in use (called ‘AA’ and ‘AL’), and it is the ellipsoid used in the geodetic datum that defines which one will be used!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Older series of Belgian maps distributed by the National Geographical Institute (https://www.ngi.be) applied the ED50 geodetic datum (European Datum 1950), which affects the position of coordinates (hence MGRS grid) by about 90 m in E-W direction and about 200 m in N-S direction compared to the ETRS89 and WGS 84 datums. OGC URN identifiers of these datums:\nED50 datum: urn:ogc:def:datum:EPSG::6230\nETRS89 ensemble datum: urn:ogc:def:ensemble:EPSG::6258\nWGS 84 ensemble datum: urn:ogc:def:ensemble:EPSG::6326.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Beside R’s mgrs package, another possibility to translate to/from MGRS grid references in R is by accessing the ‘Lat Lon Tools’ QGIS plugin using the qgisprocess package, e.g. by calling the algorithm latlontools:point2mgrs.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","href":"/articles/spatial_mgrs/","title":"The UTM grid and the MGRS grid: not quite the same"},{"content":"","href":"/tags/utm/","title":"utm"},{"content":"","href":"/authors/hansvancalster/","title":"hansvancalster"},{"content":"","href":"/tags/maps/","title":"maps"},{"content":"What is WFS? In computing, the Open Geospatial Consortium (OGC) Web Feature Service (WFS) Interface Standard provides an interface allowing requests for geographical features across the web using platform-independent calls. One can think of geographical features as the “source code” behind a map, whereas the Web Mapping Service (WMS) interface or online tiled mapping portals like Google Maps return only an image, which end-users cannot edit or spatially analyze.\nThe standard output format for reading or downloading data from a WFS is the XML-based Geography Markup Language (GML), but other formats like shapefiles or geojson are also available. In early 2006 the OGC members approved the OpenGIS GML Simple Features Profile. This profile is designed both to increase interoperability between WFS servers and to improve the ease of implementation of the WFS standard. (Source: Wikipedia)\nBefore we embark on using a WFS service in R, we would like to draw your attention to the following issue when using a WFS service in a workflow that needs to be reproducible in the longer term. A distributed, stable storage of the GIS data used in a workflow is necessary to make that workflow reproducible on a longer term. When using dynamic databases and web services, where the data they present are expected to evolve (e.g. by always pointing at the latest release in these WFS-cases), this is not guaranteed if you just use the WFS service to directly read GIS data from it. Instead of directly reading the GIS data, it is also possible to download the data from the WFS service to disk (and possibly archive on, e.g., Zenodo) in order to obtain a stable version that can be re-used later by reading it from disk. Also, in case it takes a long time to get the GIS data from the WFS service, downloading instead of directly reading is often a better choice. On the other hand, some types of workflows might instead require the most up to date GIS data that are available to be retrieved while no workflow reproducibility is needed. Do note that publishing the workflow is always recommended for published work. It serves the scientific community and it is also a way of proving scientific quality - hence reproducibility is often the best choice for published work.\nSome of the material presented in this tutorial benefitted from a tutorial presented at the Use of R in Official Statistics conference in The Hague, September 2018 Spatial Analysis in R with Open Geodata, from Lovelace, Nowosad, and Muenchow (2020) and from this Dutch wordpress blogpost.\nUseful overviews of web feature services WFS (and WM(T)S) services for Belgium and regions in Belgium:\n overview of WFS services for Flanders region overview compiled by Michel Stuyts, which is also available on gitlab overview maintained by DOV Vlaanderen Geoportal of the Belgian federal institutions, notably NGI: several of these services can be viewed interactively at the NGI TopoMapViewer, including the CartoWeb.be WMS/WMTS service KMI meteo data  European portals:\n inspire geoportal: European portal for spatial data - some of which have a WFS service environmental data for Europe: many of the products listed have either a WMS or a WFS service  Worldwide coverage:\n spatineo directory  Used packages library(sf) # simple features packages for handling vector GIS data library(httr) # generic webservice package library(tidyverse) # a suite of packages for data wrangling, transformation, plotting, ... library(ows4R) # interface for OGC webservices In this tutorial, we take a coding approach to show how to get data from a web feature service. You may also be interested in the inbospatial package, which wraps some of the material presented here in more user-friendly functions. The inbospatial package has a separate tutorial which explains how to get data from web feature and (some) web coverage services using these functions.\nGet to know what the service can do with GetCapabilities First of all we need the URL of the service.\nwfs_bwk \u0026lt;- \u0026#34;https://geo.api.vlaanderen.be/BWK/wfs\u0026#34; Next, we append information to the URL address with the aid of httr::parse_url and httr::build_url. The former function parses an URL into a list for easier programmatic addition of information to the URL. The latter function does the reverse and builds the URL from the list object.\nThe url$query slot is where you instruct the WFS what information it should return. It is constructed as a list with name-value pairs. For now, we only need to specify the GetCapabilities request. Other information such as passing version = 2.0.0 can be added, but is not required (by default, the latest version of the WFS service will be chosen).\nurl \u0026lt;- parse_url(wfs_bwk) url$query \u0026lt;- list(service = \u0026#34;wfs\u0026#34;, #version = \u0026#34;2.0.0\u0026#34;, # facultative request = \u0026#34;GetCapabilities\u0026#34; ) request \u0026lt;- build_url(url) request ## [1] \u0026quot;https://geo.api.vlaanderen.be/BWK/wfs?service=wfs\u0026amp;request=GetCapabilities\u0026quot;  With GetCapabilities, we obtain a complete overview of all metadata for the web service.\nTo see all capabilities, you can visit the request in the webbrowser. For instance opening the page in the webbrowser and searching for “Filter_Capabilities” allows you to see all possible ways to filter the data from a WFS layer (e.g. restrict the downloaded data to a specified bounding box with SpatialOperator name=\u0026quot;BBOX\u0026quot;).\nInstead of searching the page on the web, there are several ways to access specific pieces of information programmatically. We will show here how to do this using functions in the ows4R package. The first thing we need to do is generate a connection to the WFS with the aid of WFSClient$new().\nbwk_client \u0026lt;- WFSClient$new(wfs_bwk, serviceVersion = \u0026#34;2.0.0\u0026#34;) #serviceVersion must be provided here The resulting object bwk_client is an R6 object. If you are not familiar with R6 object, you might want to read the R6 chapter in Advanced R.\nPrinting `bwk_client looks like this:\nbwk_client ## \u0026lt;WFSClient\u0026gt; ## Inherits from: \u0026lt;OWSClient\u0026gt; ## Public: ## attrs: list ## capabilities: WFSCapabilities, OWSCapabilities, OGCAbstractObject, R6 ## clone: function (deep = FALSE) ## defaults: list ## describeFeatureType: function (typeName) ## element: AbstractObject ## encode: function (addNS = TRUE, geometa_validate = TRUE, geometa_inspire = FALSE, ## ERROR: function (text) ## getCapabilities: function () ## getCASUrl: function () ## getClass: function () ## getClassName: function () ## getConfig: function () ## getFeatures: function (typeName, ...) ## getFeatureTypes: function (pretty = FALSE) ## getHeaders: function () ## getNamespaceDefinition: function (recursive = FALSE) ## getPwd: function () ## getToken: function () ## getUrl: function () ## getUser: function () ## getVersion: function () ## INFO: function (text) ## initialize: function (url, serviceVersion = NULL, user = NULL, pwd = NULL, ## isFieldInheritedFrom: function (field) ## logger: function (type, text) ## loggerType: NULL ## namespace: OWSNamespace, R6 ## reloadCapabilities: function () ## url: https://geo.api.vlaanderen.be/BWK/wfs ## verbose.debug: FALSE ## verbose.info: FALSE ## version: 2.0.0 ## WARN: function (text) ## wrap: FALSE ## Private: ## cas_url: NULL ## config: request ## fromComplexTypes: function (value) ## headers: NULL ## pwd: NULL ## serviceName: WFS ## system_fields: verbose.info verbose.debug loggerType wrap element names ... ## token: NULL ## user: NULL ## xmlElement: AbstractObject ## xmlExtraNamespaces: NULL ## xmlNamespacePrefix: OWS ## xmlNodeToCharacter: function (x, ..., indent = \u0026quot;\u0026quot;, tagSeparator = \u0026quot;\\n\u0026quot;)  The features listed can be accessed using $. We can see that some of them are again functions. As a first example, the following code will list all available layers for that WFS.\nbwk_client$getFeatureTypes(pretty = TRUE) ## name title ## 1 BWK:Bwkhab BWK 2 - BWK-zone en Natura 2000 Habitat ## 2 BWK:Bwkfauna BWK 2 - Faunistisch belangrijke gebieden ## 3 BWK:Hab3260 BWK 2 - Habitattype 3260  Another way of accessing this information is:\nbwk_client$getFeatureTypes() %\u0026gt;% map_chr(function(x){x$getName()}) ## [1] \u0026quot;BWK:Bwkhab\u0026quot; \u0026quot;BWK:Bwkfauna\u0026quot; \u0026quot;BWK:Hab3260\u0026quot;  bwk_client$getFeatureTypes() %\u0026gt;% map_chr(function(x){x$getTitle()}) ## [1] \u0026quot;BWK 2 - BWK-zone en Natura 2000 Habitat\u0026quot; ## [2] \u0026quot;BWK 2 - Faunistisch belangrijke gebieden\u0026quot; ## [3] \u0026quot;BWK 2 - Habitattype 3260\u0026quot;  We are using purrr::map() functionalities here, because the output of bwk_client$getFeatureTypes() is a list and each element of the list contains information about a layer.\nLet’s turn back our attention to getCapabilities() and see what information we can extract from it.\nbwk_client$getCapabilities() ## \u0026lt;WFSCapabilities\u0026gt; ## Inherits from: \u0026lt;OWSCapabilities\u0026gt; ## Public: ## attrs: list ## clone: function (deep = FALSE) ## defaults: list ## element: Capabilities ## encode: function (addNS = TRUE, geometa_validate = TRUE, geometa_inspire = FALSE, ## ERROR: function (text) ## findFeatureTypeByName: function (expr, exact = TRUE) ## getClass: function () ## getClassName: function () ## getClient: function () ## getFeatureTypes: function (pretty = FALSE) ## getNamespaceDefinition: function (recursive = FALSE) ## getOperationsMetadata: function () ## getOWSVersion: function () ## getRequest: function () ## getService: function () ## getServiceIdentification: function () ## getServiceProvider: function () ## getServiceVersion: function () ## getUrl: function () ## INFO: function (text) ## initialize: function (url, version, logger = NULL, ...) ## isFieldInheritedFrom: function (field) ## logger: function (type, text) ## loggerType: NULL ## namespace: NULL ## setClient: function (client) ## verbose.debug: FALSE ## verbose.info: FALSE ## WARN: function (text) ## wrap: FALSE ## Private: ## client: WFSClient, OWSClient, OGCAbstractObject, R6 ## featureTypes: list ## fetchFeatureTypes: function (xmlObj, version) ## fromComplexTypes: function (value) ## operationsMetadata: OWSOperationsMetadata, R6 ## owsVersion: 1.1 ## request: OWSGetCapabilities, OWSHttpRequest, OGCAbstractObject, R6 ## service: WFS ## serviceIdentification: OWSServiceIdentification, R6 ## serviceProvider: OWSServiceProvider, R6 ## serviceVersion: 2.0.0 ## system_fields: verbose.info verbose.debug loggerType wrap element names ... ## url: https://geo.api.vlaanderen.be/BWK/wfs ## xmlElement: Capabilities ## xmlExtraNamespaces: NULL ## xmlNamespacePrefix: WFS_1_1 ## xmlNodeToCharacter: function (x, ..., indent = \u0026quot;\u0026quot;, tagSeparator = \u0026quot;\\n\u0026quot;)  This is again an R6 class object and the $ can be used to chain together several functions, much in the same way as the pipe operator %\u0026gt;%. The following chunk illustrates its use. Try executing this incrementally (select and execute code up to first $, second $, …) to see what happens.\nbwk_client$ getCapabilities()$ findFeatureTypeByName(\u0026#34;BWK:Bwkhab\u0026#34;)$ getDescription() %\u0026gt;% map_chr(function(x){x$getName()}) ## [1] \u0026quot;UIDN\u0026quot; \u0026quot;OIDN\u0026quot; \u0026quot;TAG\u0026quot; \u0026quot;EVAL\u0026quot; \u0026quot;EENH1\u0026quot; ## [6] \u0026quot;EENH2\u0026quot; \u0026quot;EENH3\u0026quot; \u0026quot;EENH4\u0026quot; \u0026quot;EENH5\u0026quot; \u0026quot;EENH6\u0026quot; ## [11] \u0026quot;EENH7\u0026quot; \u0026quot;EENH8\u0026quot; \u0026quot;V1\u0026quot; \u0026quot;V2\u0026quot; \u0026quot;V3\u0026quot; ## [16] \u0026quot;HERK\u0026quot; \u0026quot;INFO\u0026quot; \u0026quot;BWKLABEL\u0026quot; \u0026quot;HAB1\u0026quot; \u0026quot;PHAB1\u0026quot; ## [21] \u0026quot;HAB2\u0026quot; \u0026quot;PHAB2\u0026quot; \u0026quot;HAB3\u0026quot; \u0026quot;PHAB3\u0026quot; \u0026quot;HAB4\u0026quot; ## [26] \u0026quot;PHAB4\u0026quot; \u0026quot;HAB5\u0026quot; \u0026quot;PHAB5\u0026quot; \u0026quot;HERKHAB\u0026quot; \u0026quot;HERKPHAB\u0026quot; ## [31] \u0026quot;HABLEGENDE\u0026quot; \u0026quot;SHAPE\u0026quot;  # or shorter bwk_client$ describeFeatureType(typeName = \u0026#34;BWK:Bwkhab\u0026#34;) %\u0026gt;% map_chr(function(x){x$getName()}) ## [1] \u0026quot;UIDN\u0026quot; \u0026quot;OIDN\u0026quot; \u0026quot;TAG\u0026quot; \u0026quot;EVAL\u0026quot; \u0026quot;EENH1\u0026quot; ## [6] \u0026quot;EENH2\u0026quot; \u0026quot;EENH3\u0026quot; \u0026quot;EENH4\u0026quot; \u0026quot;EENH5\u0026quot; \u0026quot;EENH6\u0026quot; ## [11] \u0026quot;EENH7\u0026quot; \u0026quot;EENH8\u0026quot; \u0026quot;V1\u0026quot; \u0026quot;V2\u0026quot; \u0026quot;V3\u0026quot; ## [16] \u0026quot;HERK\u0026quot; \u0026quot;INFO\u0026quot; \u0026quot;BWKLABEL\u0026quot; \u0026quot;HAB1\u0026quot; \u0026quot;PHAB1\u0026quot; ## [21] \u0026quot;HAB2\u0026quot; \u0026quot;PHAB2\u0026quot; \u0026quot;HAB3\u0026quot; \u0026quot;PHAB3\u0026quot; \u0026quot;HAB4\u0026quot; ## [26] \u0026quot;PHAB4\u0026quot; \u0026quot;HAB5\u0026quot; \u0026quot;PHAB5\u0026quot; \u0026quot;HERKHAB\u0026quot; \u0026quot;HERKPHAB\u0026quot; ## [31] \u0026quot;HABLEGENDE\u0026quot; \u0026quot;SHAPE\u0026quot;  This lists all available fields for the layer “BWK:Bwkhab”.\nHere is how to get a character vector naming all available operations of the WFS:\nbwk_client$ getCapabilities()$ getOperationsMetadata()$ getOperations() %\u0026gt;% map_chr(function(x){x$getName()}) ## [1] \u0026quot;GetCapabilities\u0026quot; \u0026quot;DescribeFeatureType\u0026quot; \u0026quot;GetFeature\u0026quot; ## [4] \u0026quot;GetPropertyValue\u0026quot; \u0026quot;ListStoredQueries\u0026quot; \u0026quot;DescribeStoredQueries\u0026quot; ## [7] \u0026quot;CreateStoredQuery\u0026quot; \u0026quot;DropStoredQuery\u0026quot;  The next chunk shows how we can extract the available output formats. We will see later that GetFeature is the operation needed to read or download data from the WFS. The metadata for this operation has what we want and we can extract it with a combination of purrr::map() and `purrr::pluck().\nbwk_client$ getCapabilities()$ getOperationsMetadata()$ getOperations() %\u0026gt;% map(function(x){x$getParameters()}) %\u0026gt;% pluck(3, \u0026#34;outputFormat\u0026#34;) ## [1] \u0026quot;text/xml; subtype=gml/3.2\u0026quot; ## [2] \u0026quot;gml32\u0026quot; ## [3] \u0026quot;application/gml+xml; version=3.2\u0026quot; ## [4] \u0026quot;GML2\u0026quot; ## [5] \u0026quot;KML\u0026quot; ## [6] \u0026quot;SHAPE-ZIP\u0026quot; ## [7] \u0026quot;application/json\u0026quot; ## [8] \u0026quot;application/vnd.google-earth.kml xml\u0026quot; ## [9] \u0026quot;application/vnd.google-earth.kml+xml\u0026quot; ## [10] \u0026quot;csv\u0026quot; ## [11] \u0026quot;gml3\u0026quot; ## [12] \u0026quot;json\u0026quot; ## [13] \u0026quot;text/xml; subtype=gml/2.1.2\u0026quot; ## [14] \u0026quot;text/xml; subtype=gml/3.1.1\u0026quot;  Some more examples follow. Let’s extract the bounding boxes for all layers.\nbwk_client$ getCapabilities()$ getFeatureTypes() %\u0026gt;% map(function(x){x$getBoundingBox()}) ## [[1]] ## min max ## x 2.525262 5.936009 ## y 50.673762 51.505480 ## ## [[2]] ## min max ## x 2.525262 5.936009 ## y 50.673762 51.505480 ## ## [[3]] ## min max ## x 2.525262 5.936009 ## y 50.673762 51.505480  As expected for this WFS, the bounding boxes are the same for all layers.\nThe final example shows how to get the abstract so we can read about the contents of the layers.\nbwk_client$ getCapabilities()$ getFeatureTypes() %\u0026gt;% map_chr(function(x){x$getAbstract()}) ## [1] \u0026quot;Zone waaraan een biologische waarde gegeven wordt, alsook een aanduiding van het Natura 2000-habitattype, de vegetatiekundige eenheden of ecotopen, bodembedekking en gegevens over eventueel aanwezige kleine landschapselementen (BWK-karteringseenheden).\u0026quot; ## [2] \u0026quot;Op de Biologische Waarderingskaart krijgen een aantal gebieden een specifieke arcering omwille van de aanwezigheid van bepaalde fauna-elementen. De afbakening is gebaseerd op soorten die tot de Rode lijst-categorieën 'Met uitsterven bedreigd', 'Bedreigd' en 'Kwetsbaar' behoren. Een ruimere omschrijving wordt verstrekt in het afzonderlijk document per kaartblad \\\u0026quot;toelichtingXX\\\u0026quot;, met XX het kaartbladnummer.\u0026quot; ## [3] \u0026quot;Voorkomen van het Natura 2000 habitattype 3260, de submontane- en laaglandrivieren met waterranonkel- en fonteinkruidvegetaties.\u0026quot;  Read or download vector data from WFS: GetFeature Example 1: an entire layer The map of regions of Belgium.\nwfs_regions \u0026lt;- \u0026#34;https://eservices.minfin.fgov.be/arcgis/services/R2C/Regions/MapServer/WFSServer\u0026#34; regions_client \u0026lt;- WFSClient$new(wfs_regions, serviceVersion = \u0026#34;2.0.0\u0026#34;) regions_client$getFeatureTypes(pretty = TRUE) ## name title ## 1 R2C_Regions:Regions Regions  url \u0026lt;- parse_url(wfs_regions) url$query \u0026lt;- list(service = \u0026#34;wfs\u0026#34;, #version = \u0026#34;2.0.0\u0026#34;, # optional request = \u0026#34;GetFeature\u0026#34;, typename = \u0026#34;regions\u0026#34;, srsName = \u0026#34;EPSG:4326\u0026#34; ) request \u0026lt;- build_url(url) bel_regions \u0026lt;- read_sf(request) #Lambert2008 ggplot(bel_regions) + geom_sf() Example 2: filter by attributes In this example, we only want to extract specific regions. This can be done using either standard OGC filter specification or using a Contextual Query Language (CQL) filter. The latter is currently a draft proposal for an OGC standard and is subject to change. A didactical explanation of CQL can be found here.1 CQL currently only works for WFS services that are hosted on a GeoServer!\nIn this example we also show how the previously used R code can be stitched together using the pipe (%\u0026gt;%) operator.\nStandard OGC filter\nUnfortunately, the standard OGC filter format is very verbose…\nwfs_regions %\u0026gt;% parse_url() %\u0026gt;% list_merge(query = list(service = \u0026#34;wfs\u0026#34;, #version = \u0026#34;2.0.0\u0026#34;, # optional request = \u0026#34;GetFeature\u0026#34;, typename = \u0026#34;regions\u0026#34;, srsName = \u0026#34;EPSG:4326\u0026#34;, filter = \u0026#34;\u0026lt;Filter\u0026gt;\u0026lt;PropertyIsEqualTo\u0026gt;\u0026lt;PropertyName\u0026gt;regions:NameDUT\u0026lt;/PropertyName\u0026gt;\u0026lt;Literal\u0026gt;\u0026#39;Vlaams Gewest\u0026#39;\u0026lt;/Literal\u0026gt;\u0026lt;/PropertyIsEqualTo\u0026gt;\u0026lt;/Filter\u0026gt;\u0026#34;)) %\u0026gt;% build_url() %\u0026gt;% read_sf() %\u0026gt;% ggplot() + geom_sf() CQL filter\nWe use a different WFS service for which CQL works. First we need to know the names of the fields by which we can filter.\nwfs_vrbg \u0026lt;- \u0026#34;https://geo.api.vlaanderen.be/VRBG/wfs\u0026#34; vrbg_client \u0026lt;- WFSClient$new(wfs_vrbg, serviceVersion = \u0026#34;1.1.0\u0026#34;) vrbg_client$ getCapabilities()$ findFeatureTypeByName(\u0026#34;VRBG:Refprv\u0026#34;)$ getDescription() %\u0026gt;% map_chr(function(x){x$getName()}) ## [1] \u0026quot;UIDN\u0026quot; \u0026quot;OIDN\u0026quot; \u0026quot;TERRID\u0026quot; \u0026quot;NAAM\u0026quot; \u0026quot;NISCODE\u0026quot; \u0026quot;NUTS2\u0026quot; \u0026quot;SHAPE\u0026quot;  # another way of doing this: wfs_vrbg %\u0026gt;% parse_url() %\u0026gt;% list_merge(query = list(service = \u0026#34;wfs\u0026#34;, #version = \u0026#34;1.1.0\u0026#34;, # optional request = \u0026#34;DescribeFeatureType\u0026#34;, typeName = \u0026#34;VRBG:Refprv\u0026#34;)) %\u0026gt;% build_url() %\u0026gt;% GET() ## Response [https://geo.api.vlaanderen.be/VRBG/wfs?service=wfs\u0026amp;request=DescribeFeatureType\u0026amp;typeName=VRBG%3ARefprv] ## Date: 2023-08-31 06:04 ## Status: 200 ## Content-Type: text/xml; subtype=gml/3.2 ## Size: 1.53 kB ## \u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot;?\u0026gt;\u0026lt;xsd:schema xmlns:xsd=\u0026quot;http://www.w3.or... ## \u0026lt;xsd:import namespace=\u0026quot;http://www.opengis.net/gml/3.2\u0026quot; schemaLocation=\u0026quot;http... ## \u0026lt;xsd:complexType name=\u0026quot;RefprvType\u0026quot;\u0026gt; ## \u0026lt;xsd:complexContent\u0026gt; ## \u0026lt;xsd:extension base=\u0026quot;gml:AbstractFeatureType\u0026quot;\u0026gt; ## \u0026lt;xsd:sequence\u0026gt; ## \u0026lt;xsd:element maxOccurs=\u0026quot;1\u0026quot; minOccurs=\u0026quot;1\u0026quot; name=\u0026quot;UIDN\u0026quot; nillable=\u0026quot;fals... ## \u0026lt;xsd:element maxOccurs=\u0026quot;1\u0026quot; minOccurs=\u0026quot;1\u0026quot; name=\u0026quot;OIDN\u0026quot; nillable=\u0026quot;fals... ## \u0026lt;xsd:element maxOccurs=\u0026quot;1\u0026quot; minOccurs=\u0026quot;0\u0026quot; name=\u0026quot;TERRID\u0026quot; nillable=\u0026quot;tr... ## \u0026lt;xsd:element maxOccurs=\u0026quot;1\u0026quot; minOccurs=\u0026quot;0\u0026quot; name=\u0026quot;NAAM\u0026quot; nillable=\u0026quot;true... ## ...  The CQL filter format is much more human readable and easier to code:\nsf_prov \u0026lt;- wfs_vrbg %\u0026gt;% parse_url() %\u0026gt;% list_merge(query = list(service = \u0026#34;wfs\u0026#34;, #version = \u0026#34;1.1.0\u0026#34;, # optional request = \u0026#34;GetFeature\u0026#34;, typeName = \u0026#34;VRBG:Refprv\u0026#34;, srsName = \u0026#34;EPSG:31370\u0026#34;, cql_filter=\u0026#34;NAAM=\u0026#39;West-Vlaanderen\u0026#39;\u0026#34;)) %\u0026gt;% build_url() %\u0026gt;% read_sf(crs = 31370) sf_prov ## Simple feature collection with 1 feature and 7 fields ## Geometry type: MULTISURFACE ## Dimension: XY ## Bounding box: xmin: 21991.63 ymin: 155928.6 xmax: 90410.78 ymax: 229724.6 ## Projected CRS: BD72 / Belgian Lambert 72 ## # A tibble: 1 × 8 ## gml_id UIDN OIDN TERRID NAAM NISCODE NUTS2 SHAPE ## * \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;MULTISURFACE [m]\u0026gt; ## 1 Refprv.3 20 3 351 West-Vlaa… 30000 BE25 (POLYGON ((80681.53 2276…  Also check out example 5 for a more advanced use of the CQL filter.\nNote, the rather exotic geometry type that is returned (MULTISURFACE). Some sf functions, such as st_buffer(), will not work out of the box for this type. In this specific case, we need an intermediate step st_cast(to = \u0026quot;GEOMETRYCOLLECTION\u0026quot;) to make it work.\nsf_prov %\u0026gt;% st_buffer(dist = 100) # errors ## Error in scan(text = lst[[length(lst)]], quiet = TRUE): scan() expected 'a real', got 'ParseException:' ## Error in (function (msg) : ParseException: Unknown WKB type 12  sf_prov_buffer \u0026lt;- sf_prov %\u0026gt;% st_cast(to = \u0026#34;GEOMETRYCOLLECTION\u0026#34;) %\u0026gt;% st_buffer(dist = 10000) # works sf_prov %\u0026gt;% ggplot() + geom_sf(data = sf_prov_buffer) + geom_sf() If you need to further convert to geometry type POLYGON, then use st_collection_extract() to extract the Polygon from the GeometryCollection:\nsf_prov %\u0026gt;% st_cast(to = \u0026#34;GEOMETRYCOLLECTION\u0026#34;) %\u0026gt;% st_collection_extract(type = \u0026#34;POLYGON\u0026#34;) ## Simple feature collection with 1 feature and 7 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: 21991.63 ymin: 155928.6 xmax: 90410.78 ymax: 229724.6 ## Projected CRS: BD72 / Belgian Lambert 72 ## # A tibble: 1 × 8 ## gml_id UIDN OIDN TERRID NAAM NISCODE NUTS2 SHAPE ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;POLYGON [m]\u0026gt; ## 1 Refprv.3 20 3 351 West-Vlaa… 30000 BE25 ((80681.53 227626.5, 807…  More generally however, if a row of the MULTISURFACE object corresponds to either a MULTIPOLYGON (collection of polygons) or a POLYGON with holes, with the above st_collection_extract(type = \u0026quot;POLYGON\u0026quot;) this row will be repeated for all constituting polygons. This results in too many features. It can be solved as follows:\n# to demonstrate, we first create an sf object of several MULTISURFACE features: sf_prov_all \u0026lt;- wfs_vrbg %\u0026gt;% parse_url() %\u0026gt;% list_merge(query = list(service = \u0026#34;wfs\u0026#34;, request = \u0026#34;GetFeature\u0026#34;, typeName = \u0026#34;VRBG:Refprv\u0026#34;, srsName = \u0026#34;EPSG:31370\u0026#34;)) %\u0026gt;% build_url() %\u0026gt;% read_sf(crs = 31370) # converting: sf_prov_all %\u0026gt;% # 5 MULTISURFACE features st_cast(\u0026#34;GEOMETRYCOLLECTION\u0026#34;) %\u0026gt;% mutate(id = seq_len(nrow(.))) %\u0026gt;% st_collection_extract(\u0026#34;POLYGON\u0026#34;) %\u0026gt;% # 28 POLYGON features aggregate(list(.$id), first, do_union = FALSE) %\u0026gt;% # 5 MULTIPOLYGON features select(-id, -Group.1) %\u0026gt;% as_tibble %\u0026gt;% st_as_sf ## Simple feature collection with 5 features and 7 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 21991.63 ymin: 153058.3 xmax: 258871.8 ymax: 244027.2 ## Projected CRS: BD72 / Belgian Lambert 72 ## # A tibble: 5 × 8 ## gml_id UIDN OIDN TERRID NAAM NISCODE NUTS2 geometry ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;MULTIPOLYGON [m]\u0026gt; ## 1 Refprv.1 18 2 357 Antwerpen 10000 BE21 (((178136.3 244009.2, 17… ## 2 Refprv.2 19 4 359 Vlaams Br… 20001 BE24 (((200484.9 193541, 2004… ## 3 Refprv.3 20 3 351 West-Vlaa… 30000 BE25 (((80681.53 227626.5, 80… ## 4 Refprv.4 21 1 355 Limburg 70000 BE22 (((231494.3 219142.5, 23… ## 5 Refprv.5 22 5 356 Oost-Vlaa… 40000 BE23 (((145735.2 220358.3, 14…  Example 3: restrict to a bounding box This examples illustrates how you can read or download information from a WFS for further use in R.\nLet’s start by reading in the “BWK:Bwkhab” layer for the Hallerbos area.\nThe main part is defining the input and output. We store the data in GeoJSON format which is an open standard format designed for representing simple geographical features, along with their non-spatial attributes. It is based on JSON, the JavaScript Object Notation.\nWe also add the bounding box from which we want to retrieve the data. This is very important to add. If you omit a bounding box, the service will return the entire map which can be very large.\nurl \u0026lt;- parse_url(wfs_bwk) url$query \u0026lt;- list(service = \u0026#34;WFS\u0026#34;, #version = \u0026#34;2.0.0\u0026#34;, # optional request = \u0026#34;GetFeature\u0026#34;, typename = \u0026#34;BWK:Bwkhab\u0026#34;, bbox = \u0026#34;142600,153800,146000,156900\u0026#34;) request \u0026lt;- build_url(url) Let’s check what we are about to read with sf::st_layers(). This time, the function does return useful information, but the layer name BWK:Bwkhab seems to be overwritten.\nst_layers(request) ## Driver: GML ## Available layers: ## layer_name geometry_type features fields crs_name ## 1 Bwkhab Curve Polygon 670 32 BD72 / Belgian Lambert 72  bwk_hallerbos \u0026lt;- read_sf(request) Note that the layer = ... argument of sf:read_sf() cannot be used to specify the layer when you pass a WFS GetFeature request to the function.\nLet’s make a simple plot of the object. Note that the object contains features outside of the bounding box. Those are features which have only some part within the bounding box.\nggplot(bwk_hallerbos) + geom_sf() You can use sf::st_write() to save this layer in any format that is listed by sf::st_drivers().\nExample 4: downloading data to disk Continuing from the previous example, we can download the data with httr::GET and httr::write_disk(). The only difference is that we add outputFormat to the request. In this way we can deviate from the default output format and choose the output format we like.\nurl \u0026lt;- parse_url(wfs_bwk) url$query \u0026lt;- list(service = \u0026#34;WFS\u0026#34;, #version = \u0026#34;2.0.0\u0026#34;, # optional request = \u0026#34;GetFeature\u0026#34;, typename = \u0026#34;BWK:Bwkhab\u0026#34;, bbox = \u0026#34;142600,153800,146000,156900\u0026#34;, outputFormat = \u0026#34;application/json\u0026#34;) request \u0026lt;- build_url(url) file \u0026lt;- tempfile(fileext = \u0026#34;.geojson\u0026#34;) GET(url = request, write_disk(file)) ## Response [https://geo.api.vlaanderen.be/BWK/wfs?service=WFS\u0026amp;request=GetFeature\u0026amp;typename=BWK%3ABwkhab\u0026amp;bbox=142600%2C153800%2C146000%2C156900\u0026amp;outputFormat=application%2Fjson] ## Date: 2023-08-31 06:04 ## Status: 200 ## Content-Type: application/json;charset=UTF-8 ## Size: 859 kB ## \u0026lt;ON DISK\u0026gt; C:\\Users\\HANS_V~1\\AppData\\Local\\Temp\\RtmpIVvgfZ\\filebd827562159.geojson  At this point, all features are downloaded and can be used in R as we would with any other local file. So we need to load the file with read_sf() from sf.\nbwk_hallerbos2 \u0026lt;- read_sf(file) Downloading data could be important for reproducible workflows (e.g. by using the downloaded data as the ‘real’ starting point and distribute it).\nExample 5: extract feature data at particular points In some situations, we do not need the spatial features (polygons, lines, points), but are interested in the data at a particular point (i.e. attribute table data) of the spatial feature. When working in a local GIS environment, one would use a spatial operator to extract the data (e.g. within, intersects, contains,…). As we discussed earlier, WFS supports certain spatial operators as part of the service to directly query this data and overcomes the need to download the spatial feature data first.\nConsider the following use case: You want to extract the attribute data from a soil map for a number of sampling points (point coordinates). This use case can be tackled by relying on the WFS service and the affiliated spatial operators.\nOur example data point (in Lambert 72):\nx_lam \u0026lt;- 173995.67 y_lam \u0026lt;- 212093.44 From this point we know the data, so we can verify the result (in Dutch):\n Bodemtype: s-Pgp3(v) Bodemserie: Pgp Textuurklasse: licht zandleem Drainageklasse: uiterst nat, gereduceerd  Hence, we now want to extract these soil properties from the WFS, for the coordinates defined above.\nproperties_of_interest \u0026lt;- c(\u0026#34;Drainageklasse\u0026#34;, \u0026#34;Textuurklasse\u0026#34;, \u0026#34;Bodemserie\u0026#34;, \u0026#34;Bodemtype\u0026#34;) The URL of the wfs service of the soil map of the Flemish region:\nwfs_bodemtypes \u0026lt;- \u0026#34;https://www.dov.vlaanderen.be/geoserver/bodemkaart/bodemtypes/wfs\u0026#34; The essential part is to set up the proper query! The required data for the service is defined in the metadata description. This can look a bit overwhelming at the start, but is a matter of looking for some specific elements of the (XML) document:\n service (WFS) and request (GetFeature) are mandatory fields (see below); version (1.1.0) is optional typeName: Look at the different \u0026lt;FeatureType... enlisted and pick the \u0026lt;Name\u0026gt; of the one you’re interested in. In this particular case bodemkaart:bodemtypes is the only one available. outputFormat: The supported output formats are enlisted in \u0026lt;ows:Parameter name=\u0026quot;outputFormat\u0026quot;\u0026gt;. As the service provides CSV as output, this is a straightforward option. json is another popular one. propertyname: A list of the attribute table fields (cfr. supra). A full list of the Flanders soil map is provided here. We also define the CRS, using the EPSG code. CQL_FILTER: Define the spatial operator, in this case INTERSECTS of the WFS geom and our POINT coordinate. The operators are enlisted in the \u0026lt;ogc:SpatialOperators\u0026gt; field. Note that geom is the name of the geometry field (in this example geom but other web services may use a different name such as SHAPE, geometry or the_geom).  Formatting all this information in a query and executing the request (GET) towards the service:\nquery \u0026lt;- list(service = \u0026#34;WFS\u0026#34;, request = \u0026#34;GetFeature\u0026#34;, #version = \u0026#34;1.1.0\u0026#34;, # optional typeName = \u0026#34;bodemkaart:bodemtypes\u0026#34;, outputFormat = \u0026#34;csv\u0026#34;, propertyname = as.character(paste(properties_of_interest, collapse = \u0026#34;,\u0026#34;)), CRS = \u0026#34;EPSG:31370\u0026#34;, CQL_FILTER = sprintf(\u0026#34;INTERSECTS(geom,POINT(%s %s))\u0026#34;, x_lam, y_lam)) result \u0026lt;- GET(wfs_bodemtypes, query = query) result ## Response [https://www.dov.vlaanderen.be/geoserver/bodemkaart/bodemtypes/wfs?service=WFS\u0026amp;request=GetFeature\u0026amp;typeName=bodemkaart%3Abodemtypes\u0026amp;outputFormat=csv\u0026amp;propertyname=Drainageklasse%2CTextuurklasse%2CBodemserie%2CBodemtype\u0026amp;CRS=EPSG%3A31370\u0026amp;CQL_FILTER=INTERSECTS%28geom%2CPOINT%28173995.67%20212093.44%29%29] ## Date: 2022-03-15 08:07 ## Status: 200 ## Content-Type: text/csv;charset=UTF-8 ## Size: 129 B ## FID,Bodemtype,Bodemserie,Textuurklasse,Drainageklasse ## bodemtypes.72727,s-Pgp3(v),Pgp,licht zandleem,\u0026quot;uiterst nat, gereduceerd\u0026quot;  The result is not yet formatted to be used as a dataframe. We need to use a small trick using the textConnection function to get from the result (bits) towards a readable output in a dataframe:\ndf \u0026lt;- read.csv(textConnection(content(result, \u0026#39;text\u0026#39;))) knitr::kable(df)    FID Bodemtype Bodemserie Textuurklasse Drainageklasse     bodemtypes.72727 s-Pgp3(v) Pgp licht zandleem uiterst nat, gereduceerd    Which indeed corresponds to the data of the coordinate.\nIn order to make your coding work lighter, you can easily put the WFS R code (building the WFS request and retrieving its results) inside a function. Just use the code of the inborutils::extract_soil_map_data() function (documented here) as a template for your own function.\nNow this function makes it really easy to request the data:\nlibrary(inborutils) extract_soil_map_data( x_lam = x_lam, y_lam = y_lam) %\u0026gt;% knitr::kable() ## Defaulting to Bodemserie, Unibodemtype, Bodemtype. To avoid this message provide properties of interest in the function call.     Bodemtype Unibodemtype Bodemserie     s-Pgp3(v) s-Pgp3(v) Pgp    extract_soil_map_data( x_lam = x_lam, y_lam = y_lam, properties_of_interest = c(\u0026#34;Eenduidige_legende\u0026#34;, \u0026#34;Textuurklasse\u0026#34;)) %\u0026gt;% knitr::kable()    Textuurklasse Eenduidige_legende     licht zandleem De twee bovenvermelde grondwatergronden op licht zandleem (Pgp en Pgg) zijn permanent zeer nat. Ze zijn overstroomd in de winter en hebben een zomerwaterstand op ongeveer 40 cm. Ze zijn ongeschikt voor landbouw; zelfs weiden geven geen bevredigende resultaten. Deze bodems zijn dus ongeschikt voor uitbating; eventueel kunnen populieren, mits de nodige ontwatering, aangepast worden, de resultaten zijn echter weinig gunstig.    extract_soil_map_data( x_lam = x_lam, y_lam = y_lam, properties_of_interest = properties_of_interest) %\u0026gt;% knitr::kable()    Bodemtype Bodemserie Textuurklasse Drainageklasse     s-Pgp3(v) Pgp licht zandleem uiterst nat, gereduceerd    # multiple point locations xy \u0026lt;- data.frame(id = c(\u0026#34;loc1\u0026#34;, \u0026#34;loc2\u0026#34;), x = c(173995.67, 180000), y = c(212093.4, 212000)) xy %\u0026gt;% group_by(id) %\u0026gt;% summarise(extract_soil_map_data(x, y, properties_of_interest = properties_of_interest)) %\u0026gt;% knitr::kable()    id Bodemtype Bodemserie Textuurklasse Drainageklasse     loc1 s-Pgp3(v) Pgp licht zandleem uiterst nat, gereduceerd   loc2 Sdg Sdg lemig zand matig nat, matig gleyig    Example 6: pagination When a layer of a WFS service contains many thousands of features, often the WFS service will restrict the number of features that can be obtained with one request. This is done to optimize performance (responsiveness) of the WFS service: it is more efficient to transfer small packages of data and this is even more the case when multiple user requests are transmitted concurrently to the service.\nIn this example we show how to deal with this situation. A technique called ‘pagination’ can be used to obtain all features one is interested in by sending multiple requests to the server. The service is available from version ‘2.0.0’ onwards. Servers running older versions may or may not have support for ‘pagination’.\nWe will use the ‘Watervlakken’ WFS service for this example.\n# this layer returns just 1000 features (for performance reasons) wfs \u0026lt;- \u0026#34;https://gisservices.inbo.be/arcgis/services/Watervlakken/MapServer/WFSServer?\u0026#34; url \u0026lt;- parse_url(wfs) url$query \u0026lt;- list(service = \u0026#34;wfs\u0026#34;, version = \u0026#34;2.0.0\u0026#34;, # facultative request = \u0026#34;GetFeature\u0026#34;, typename = \u0026#34;Watervlakken:Watervlakken\u0026#34;, srsName = \u0026#34;EPSG:31370\u0026#34; ) request \u0026lt;- build_url(url) gd \u0026lt;- read_sf(request) gd ## Simple feature collection with 1000 features and 12 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 136507.8 ymin: 187356 xmax: 192040.7 ymax: 237604.4 ## Projected CRS: Belge 1972 / Belgian Lambert 72 ## # A tibble: 1,000 x 13 ## gml_id OBJECTID WVLC NAAM OPPWVL OMTWVL SHAPE_Length SHAPE_Area DIEPKL ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 Watervlakk~ 1 ANTB~ Zwal~ 111. 52.9 52.9 111. \u0026lt;NA\u0026gt; ## 2 Watervlakk~ 2 ANTB~ \u0026lt;NA\u0026gt; 1679. 165. 165. 1679. \u0026lt;NA\u0026gt; ## 3 Watervlakk~ 3 ANTB~ \u0026lt;NA\u0026gt; 2634. 232. 232. 2634. \u0026lt;NA\u0026gt; ## 4 Watervlakk~ 4 ANTB~ \u0026lt;NA\u0026gt; 525. 156. 156. 525. \u0026lt;NA\u0026gt; ## 5 Watervlakk~ 5 ANTB~ \u0026lt;NA\u0026gt; 409. 80.0 80.0 409. \u0026lt;NA\u0026gt; ## 6 Watervlakk~ 6 ANTB~ \u0026lt;NA\u0026gt; 663. 121. 121. 663. \u0026lt;NA\u0026gt; ## 7 Watervlakk~ 7 ANTB~ \u0026lt;NA\u0026gt; 2402. 269. 269. 2402. \u0026lt;NA\u0026gt; ## 8 Watervlakk~ 8 ANTB~ \u0026lt;NA\u0026gt; 3940. 274. 274. 3940. \u0026lt;NA\u0026gt; ## 9 Watervlakk~ 9 ANTB~ \u0026lt;NA\u0026gt; 146. 46.2 46.2 146. \u0026lt;NA\u0026gt; ## 10 Watervlakk~ 10 ANTB~ \u0026lt;NA\u0026gt; 3402. 251. 251. 3402. \u0026lt;NA\u0026gt; ## # ... with 990 more rows, and 4 more variables: GEBIED \u0026lt;chr\u0026gt;, HYLAC \u0026lt;int\u0026gt;, ## # WTRLICHC \u0026lt;chr\u0026gt;, geometry \u0026lt;MULTIPOLYGON [m]\u0026gt;  As we can see from the output, only 1000 features are returned.\nTo know in advance how many features the WFS has in total, you can add resultType=\u0026quot;hits\u0026quot; to the query:\nurl$query \u0026lt;- list(service = \u0026#34;wfs\u0026#34;, version = \u0026#34;2.0.0\u0026#34;, # facultative request = \u0026#34;GetFeature\u0026#34;, typename = \u0026#34;Watervlakken:Watervlakken\u0026#34;, srsName = \u0026#34;EPSG:31370\u0026#34;, resultType = \u0026#34;hits\u0026#34; ) request \u0026lt;- build_url(url) result \u0026lt;- GET(request) parsed \u0026lt;- xml2::as_list(content(result, \u0026#34;parsed\u0026#34;)) n_features \u0026lt;- attr(parsed$FeatureCollection, \u0026#34;numberMatched\u0026#34;) n_features ## [1] \u0026quot;88713\u0026quot;  We see that there are 88713 features. In the next chunk we show how startIndex and count can be used together to obtain more features (3000 in the example).\n# use count and startIndex to return more features (pagination) get_watervlakken \u0026lt;- function(index = 0) { wfs \u0026lt;- \u0026#34;https://gisservices.inbo.be/arcgis/services/Watervlakken/MapServer/WFSServer?\u0026#34; request \u0026lt;- build_url(url) request # url in browser bekeken en nodige info in opgezocht url$query \u0026lt;- list(service = \u0026#34;wfs\u0026#34;, version = \u0026#34;2.0.0\u0026#34;, # facultative request = \u0026#34;GetFeature\u0026#34;, typename = \u0026#34;Watervlakken:Watervlakken\u0026#34;, srsName = \u0026#34;EPSG:31370\u0026#34;, startIndex=index, count=1000) request \u0026lt;- build_url(url) gd \u0026lt;- read_sf(request) return(gd) } gd \u0026lt;- map_dfr(.x = seq(0, 2000, 1000), .f = get_watervlakken ) gd ## Simple feature collection with 3000 features and 14 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 136507.8 ymin: 187156.5 xmax: 192040.7 ymax: 237604.4 ## Projected CRS: Belge 1972 / Belgian Lambert 72 ## # A tibble: 3,000 x 15 ## gml_id OBJECTID WVLC NAAM OPPWVL OMTWVL SHAPE_Length SHAPE_Area DIEPKL ## * \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 Watervlakk~ 1 ANTB~ Zwal~ 111. 52.9 52.9 111. \u0026lt;NA\u0026gt; ## 2 Watervlakk~ 2 ANTB~ \u0026lt;NA\u0026gt; 1679. 165. 165. 1679. \u0026lt;NA\u0026gt; ## 3 Watervlakk~ 3 ANTB~ \u0026lt;NA\u0026gt; 2634. 232. 232. 2634. \u0026lt;NA\u0026gt; ## 4 Watervlakk~ 4 ANTB~ \u0026lt;NA\u0026gt; 525. 156. 156. 525. \u0026lt;NA\u0026gt; ## 5 Watervlakk~ 5 ANTB~ \u0026lt;NA\u0026gt; 409. 80.0 80.0 409. \u0026lt;NA\u0026gt; ## 6 Watervlakk~ 6 ANTB~ \u0026lt;NA\u0026gt; 663. 121. 121. 663. \u0026lt;NA\u0026gt; ## 7 Watervlakk~ 7 ANTB~ \u0026lt;NA\u0026gt; 2402. 269. 269. 2402. \u0026lt;NA\u0026gt; ## 8 Watervlakk~ 8 ANTB~ \u0026lt;NA\u0026gt; 3940. 274. 274. 3940. \u0026lt;NA\u0026gt; ## 9 Watervlakk~ 9 ANTB~ \u0026lt;NA\u0026gt; 146. 46.2 46.2 146. \u0026lt;NA\u0026gt; ## 10 Watervlakk~ 10 ANTB~ \u0026lt;NA\u0026gt; 3402. 251. 251. 3402. \u0026lt;NA\u0026gt; ## # ... with 2,990 more rows, and 6 more variables: GEBIED \u0026lt;chr\u0026gt;, HYLAC \u0026lt;int\u0026gt;, ## # WTRLICHC \u0026lt;chr\u0026gt;, geometry \u0026lt;MULTIPOLYGON [m]\u0026gt;, CONNECT \u0026lt;chr\u0026gt;, ## # PEILBEHEER \u0026lt;chr\u0026gt;  Whether it is a good idea or not to request many thousands of features from a WFS really depends on what our further plans are with the spatial data.\nDo we really need the attribute data? If not, and the only goal is visualization, we should have used a WMS (Web Mapping Service) instead (see this tutorial).\nDo we need all data from the WFS? We need to think carefully which features and which attributes are really needed for whatever use case we might have. When we have a clear idea which subset of the data we need, we can apply what we have learned in the examples in this tutorial to restrict the request to what we need.\nIf we find ourselves in the case where we think we really need to request many, many features from a WFS service, it may also be more advised to use a download service instead of a WFS (search) service.\nReferences Lovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2020. Geocomputation with R. https://geocompr.robinlovelace.net.\n    Note that CQL was formerly called Common Query Language.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","href":"/tutorials/spatial_wfs_services/","title":"Using WFS services in R"},{"content":"","href":"/tags/webservice/","title":"webservice"},{"content":"","href":"/tags/github/","title":"github"},{"content":"Note: there is also a vignette in the checklist package for R, that covers below topics and more, such as integration with ORCID. If you are working with R and wish to control some of the below steps from R, then definitely read the vignette!\nGitHub and Zenodo: background GitHub hosts a remote Git server that you can use for distributed version control with Git. For more information about Git and GitHub, see other tutorials on this website! In short, Git helps you with version control (maintaining snapshots) of a directory (file folder) – a repository – and GitHub helps you with distributing versions and with collaboration.\nOn GitHub, one can designate specific snapshots (commits) as a release. Each release will typically have its version number. In this way, you can release a version 1.2.0, 1.3.0 etcetera of your source code – typically a software package, a report, a website, a note or whatever you wish to maintain and distribute.\nZenodo is a scientific repository funded by the European Commission and hosted at CERN. It allows researchers to deposit research papers, data sets, research software, reports, and any other research related digital artefacts (enumeration from Wikipedia). Zenodo is aimed at preserving the deposits for the long term, which is great for reproducibility purposes. Also, it keeps multiple versions of your stuff together under the same record. Moreover, it provides a stable DOI link for each version and for each record as whole!\nIntegrating GitHub and Zenodo Just having a public code repository at GitHub is not enough for long-term reproducibility. This is where Zenodo comes in! From within Zenodo, you can setup a webhook in the GitHub repository which is triggered when you make a new release at GitHub. Then, GitHub will send the entire repository snapshot to Zenodo where it will be published as a zip-file.\nAlso, Zenodo will increment the version number or take that (and other metadata) from a .zenodo.json metadata file, if you provide such a file in the repository. Do know that you can still manually edit all metatadata of an existing publication at the Zenodo website (authors, title, description, version number, etc.). See further to discover this \u0026lsquo;Edit\u0026rsquo; button! You cannot change the uploaded data themselves however, since a Zenodo publication is fixed (stable in time). Changing data is what versions are meant for.\nLet\u0026rsquo;s suppose you already have a GitHub repository and you want to set up such integration.\nLet\u0026rsquo;s do it!\nOh, wait!\nDid you know that you cannot just undo a publication at Zenodo? So make sure to get familiar first!\nIf you are just taking your first steps and you want to do a safe exercise, then replace \u0026lsquo;https://zenodo.org\u0026rsquo; by \u0026lsquo;https://sandbox.zenodo.org\u0026rsquo;! Consider the Zenodo Sandbox a playground for your exercise.\nThe Zenodo Sandbox creates the publications at https://sandbox.zenodo.org, e.g. https://sandbox.zenodo.org/record/1139470, but those are not perpetual and the displayed DOI\u0026rsquo;s will not work (since they aren\u0026rsquo;t actually created).\nFirst of all, make an account at https://zenodo.org (or the Sandbox!) and log in.\nStep 1: in Zenodo, go to \u0026lsquo;GitHub\u0026rsquo; in your account settings Go to the GitHub settings in Zenodo.\nStep 2: find your repository, and flip the toggle to \u0026lsquo;on\u0026rsquo; Your list of repositories.\nFlip the toggle!\nNow you are ready for your first release.\nReleasing at GitHub triggers a Zenodo publication Step 1: draft a new release In GitHub, go to the releases page, and click \u0026lsquo;Draft new release\u0026rsquo;. Alternatively, extend the repository URL with /releases/new.\nYou get a page where you can:\n select or create a tag. This is required: releases always refer to a tag. A Git tag is a label that is fixed to a specific snapshot (commit). It will typically contain a version number that you decide yourself (e.g. v2 or v2023). choose a title for your release. You will typically want to include the version number. add a description. This is not required, but most commonly you will enumerate the most important changes compared to a previous release.  Step 2: make the release Just push \u0026lsquo;publish release\u0026rsquo;!\nStep 3: inspect the result at Zenodo In seconds, the record (or the new version) will be created at Zenodo.\nHow do you know the URL?\nIn the GitHub settings at Zenodo, you can see a DOI badge for each enabled repository. Click on it to get the URL!1\nNavigate to the URL:\nTune your repository metadata for Zenodo As referred above, you can include a .zenodo.json file in your repository to provide specific metadata for Zenodo.\nYou can find more explanation and an example at the developer\u0026rsquo;s site of Zenodo.\nA simple example of the content of .zenodo.json is:\n{ \u0026#34;title\u0026#34;: \u0026#34;test title\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;test description\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;GPL-3.0\u0026#34;, \u0026#34;upload_type\u0026#34;: \u0026#34;software\u0026#34;, \u0026#34;access_right\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.14.0\u0026#34;, \u0026#34;creators\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Vanderhaeghe, Floris\u0026#34;, \u0026#34;affiliation\u0026#34;: \u0026#34;Research Institute for Nature and Forest\u0026#34;, \u0026#34;orcid\u0026#34;: \u0026#34;0000-0002-6378-6229\u0026#34; } ], \u0026#34;keywords\u0026#34;: [ \u0026#34;T\u0026#34;, \u0026#34;my strange keyword\u0026#34;, \u0026#34;another\u0026#34; ] } But you can do more with .zenodo.json. As an example, see this one in the protocolhelper repository!\nR users can also use functionality of the checklist package to generate .zenodo.json from a README.md file.\nFor an existing publication, you can always change metadata manually if needed. This can be done directly in Zenodo, as is demonstrated next with relation to communities (= part of the metadata).\nZenodo communities A Zenodo community can be regarded as a collection of Zenodo records. A community has a name and a description, and one Zenodo record can belong to more than one community.\nAn example is the INBO community at Zenodo: all Zenodo content uploaded by INBO.\nIn the above referred example of .zenodo.json, you can also find how to add your record to a community via .zenodo.json, e.g. the INBO community in case of INBO staff.\nAlternatively, after a first version of your record has been created at Zenodo, you can do it manually in Zenodo:\nStep 1: push the \u0026lsquo;Edit\u0026rsquo; button You need to be logged in of course, and you must have administrator rights for the record at hand.\nStep 2: search for the community and select it For example, add the INBO community if you are INBO staff:\nStep 3: click \u0026lsquo;Publish\u0026rsquo; at the top or bottom of the page That\u0026rsquo;s all!\nHave fun with GitHub and Zenodo!\n  Alternatively, go to your list of uploads in Zenodo (click on \u0026lsquo;Upload\u0026rsquo; at the top or navigate to https://zenodo.org/deposit).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","href":"/tutorials/git_zenodo/","title":"Set up Zenodo - GitHub integration"},{"content":"","href":"/tags/zenodo/","title":"zenodo"},{"content":"","href":"/tags/api/","title":"api"},{"content":"","href":"/tags/biodiversity/","title":"biodiversity"},{"content":"","href":"/authors/damianooldoni/","title":"damianooldoni"},{"content":"","href":"/tags/gbif/","title":"gbif"},{"content":"Introduction The R package rgbif is a R wrapper to the GBIF API which allows you to get data from GBIF directly in your R session. The package is very well documented. Please, look at the articles on how to:\n setup your GBIF username and password get occurrence data work with taxonomy related functions match and download a long species list  So, why write another tutorial about rgbif? Because I think you can benefit from these short instructions to get a (species) checklist dataset from GBIF without having to read through all the documentation.\nlibrary(tidyverse) # To do datascience library(rgbif) # To lookup names in the GBIF backbone taxonomy library(knitr) Retrieve a GBIF checklist dataset in R You want to load the Validated red lists of Flanders, Belgium checklist in your R session.\nThe URL of this checklist contains important information: the datasetKey, which is the alphanumeric string following the prefix https://www.gbif.org/dataset/. In our case:\ndatasetKey \u0026lt;- \u0026#34;fc18b0b1-8777-4c8a-8cb8-f9f15870d6a9\u0026#34; You need to run the name_usage() rgbif function using the argument datasetKey\nred_list \u0026lt;- name_usage(datasetKey = datasetKey) You get a list of two:\nnames(red_list) ## [1] \u0026quot;meta\u0026quot; \u0026quot;data\u0026quot;  The slot meta contains metadata:\nred_list$meta ## # A tibble: 1 × 3 ## offset limit endOfRecords ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;lgl\u0026gt; ## 1 0 100 FALSE  which are sometimes very important! In our case, for example, we can see that we didn’t download all the data as endOfRecords is FALSE. The reason is that name_usage has an argument called limit which is the number of records to return. The default value is 100.\nTo get all the taxa we need to specify a sufficiently high number, e.g. 10000. The hard limit of the API is ten times bigger, 100k, which is an extremely high number for such kind of datasets:\nred_list \u0026lt;- name_usage(datasetKey = datasetKey, limit = 10000) Now we should have all taxa:\nred_list$meta$endOfRecords ## [1] TRUE  Let’s check the taxa in slot data:\nkable(red_list$data[1:10,])    key nubKey nameKey taxonID kingdom phylum order family genus species kingdomKey phylumKey classKey orderKey familyKey genusKey speciesKey datasetKey parentKey parent scientificName canonicalName authorship nameType rank origin taxonomicStatus nomenclaturalStatus numDescendants lastCrawled lastInterpreted issues synonym class     152624576 2020551 8883304 INBO_RL_WBU_2013_VAL_45 Animalia Arthropoda Hemiptera Pleidae Plea Plea minutissima 196630267 196630270 196630271 196631222 196631224 196631225 152624576 fc18b0b1-8777-4c8a-8cb8-f9f15870d6a9 196631225 Plea Plea minutissima Plea minutissima  SCIENTIFIC SPECIES SOURCE ACCEPTED NA 0 2022-11-24T03:04:58.029+00:00 2022-07-09T09:43:12.315+00:00  FALSE Insecta   152624577 4926451 5755267 INBO_RL_PLA_2006_VAL_537 Plantae Tracheophyta Isoetales Isoetaceae Isoetes Isoetes echinospora 196630540 196631273 196632770 196632772 196632774 196632775 152624577 fc18b0b1-8777-4c8a-8cb8-f9f15870d6a9 196632775 Isoetes Isoetes echinospora Durieu Isoetes echinospora Durieu SCIENTIFIC SPECIES SOURCE ACCEPTED NA 0 2022-11-24T03:04:58.029+00:00 2022-07-09T09:43:17.826+00:00  FALSE Lycopsida   152624582 5152464 9652230 INBO_RL_WBU_2013_VAL_46 Animalia Arthropoda Hemiptera Nepidae Ranatra Ranatra linearis 196630267 196630270 196630271 196631222 196631276 196631281 152624582 fc18b0b1-8777-4c8a-8cb8-f9f15870d6a9 196631281 Ranatra Ranatra linearis Ranatra linearis  SCIENTIFIC SPECIES SOURCE ACCEPTED NA 0 2022-11-24T03:04:58.029+00:00 2022-07-09T09:43:12.567+00:00  FALSE Insecta   152624585 2688345 6532778 INBO_RL_PLA_2006_VAL_628 Plantae Tracheophyta Lycopodiales Lycopodiaceae Lycopodiella Lycopodiella inundata 196630540 196631273 196632770 196632777 196632778 196632782 152624585 fc18b0b1-8777-4c8a-8cb8-f9f15870d6a9 196632782 Lycopodiella Lycopodiella inundata (L.) Holub Lycopodiella inundata (L.) Holub SCIENTIFIC SPECIES SOURCE ACCEPTED NA 0 2022-11-24T03:04:58.029+00:00 2022-07-09T09:43:17.840+00:00  FALSE Lycopsida   152624586 5152456 7481479 INBO_RL_WBU_2013_VAL_37 Animalia Arthropoda Hemiptera Nepidae Nepa Nepa cinerea 196630267 196630270 196630271 196631222 196631276 196631278 152624586 fc18b0b1-8777-4c8a-8cb8-f9f15870d6a9 196631278 Nepa Nepa cinerea Nepa cinerea  SCIENTIFIC SPECIES SOURCE ACCEPTED NA 0 2022-11-24T03:04:58.029+00:00 2022-07-09T09:43:12.561+00:00  FALSE Insecta   152624589 2688393 6533650 INBO_RL_PLA_2006_VAL_630 Plantae Tracheophyta Lycopodiales Lycopodiaceae Lycopodium Lycopodium clavatum 196630540 196631273 196632770 196632777 196632778 196632787 152624589 fc18b0b1-8777-4c8a-8cb8-f9f15870d6a9 196632787 Lycopodium Lycopodium clavatum L. Lycopodium clavatum L. SCIENTIFIC SPECIES SOURCE ACCEPTED NA 0 2022-11-24T03:04:58.029+00:00 2022-07-09T09:43:17.851+00:00  FALSE Lycopsida   152624591 2688369 6532520 INBO_RL_PLA_2006_VAL_629 Plantae Tracheophyta Lycopodiales Lycopodiaceae Lycopodium Lycopodium annotinum 196630540 196631273 196632770 196632777 196632778 196632787 152624591 fc18b0b1-8777-4c8a-8cb8-f9f15870d6a9 196632787 Lycopodium Lycopodium annotinum L. Lycopodium annotinum L. SCIENTIFIC SPECIES SOURCE ACCEPTED NA 0 2022-11-24T03:04:58.029+00:00 2022-07-09T09:43:17.849+00:00  FALSE Lycopsida   152624592 4485490 7623379 INBO_RL_WBU_2013_VAL_43 Animalia Arthropoda Hemiptera Notonectidae Notonecta Notonecta viridis 196630267 196630270 196630271 196631222 196631298 196631299 152624592 fc18b0b1-8777-4c8a-8cb8-f9f15870d6a9 196631299 Notonecta Notonecta viridis Notonecta viridis  SCIENTIFIC SPECIES SOURCE ACCEPTED NA 0 2022-11-24T03:04:58.029+00:00 2022-07-09T09:43:12.626+00:00  FALSE Insecta   152624594 2020518 7623100 INBO_RL_WBU_2013_VAL_40 Animalia Arthropoda Hemiptera Notonectidae Notonecta Notonecta maculata 196630267 196630270 196630271 196631222 196631298 196631299 152624594 fc18b0b1-8777-4c8a-8cb8-f9f15870d6a9 196631299 Notonecta Notonecta maculata Notonecta maculata  SCIENTIFIC SPECIES SOURCE ACCEPTED NA 0 2022-11-24T03:04:58.029+00:00 2022-07-09T09:43:12.615+00:00  FALSE Insecta   152624595 2688495 19956112 INBO_RL_PLA_2006_VAL_515 Plantae Tracheophyta Lycopodiales Lycopodiaceae Huperzia Huperzia selago 196630540 196631273 196632770 196632777 196632778 196632785 152624595 fc18b0b1-8777-4c8a-8cb8-f9f15870d6a9 196632785 Huperzia Huperzia selago (L.) Bernh. ex Schrank et C.F.P. Mart. Huperzia selago (L.) Bernh. ex Schrank \u0026amp; C.F.P.Mart. SCIENTIFIC SPECIES SOURCE ACCEPTED NA 0 2022-11-24T03:04:58.029+00:00 2022-07-09T09:43:17.844+00:00  FALSE Lycopsida    But what do those keys stay for?\n key (sometimes called taxonKey) is the taxon identifier assigned by GBIF. The same taxon in different checklists will have a different key. taxonID is the taxon identifier as provided by the authors. The authors should keep it stable between publications. nubKey is the taxon identifier within the GBIF Backbone Taxonomy dataset, or simply backbone. The same taxon in different checklists will have the same nubKey. The link between the taxon in the checklist and the taxon in the backbone is automatically created by GBIF behind the scenes when publishing a checklist. You can control the match before publication of course, by using the gbif match utility in rgbif (see the vignette match and download a long species list) or the GBIF data validator web tool. kingdomKey, phylumKey, classKey, orderKey, familyKey, genusKey, speciesKey, parentKey: keys referring to the taxon classification as proposed in the checklist dataset. Notice that if the taxon is a species, key and speciesKey are the same. Keep in mind that the classication can be different than the classification in the backbone. Not all checklists are published with such detailed taxonomic classification. For example, the LIFE RIPARIAS target species list contains only the kingdom the taxa belong to. So, only kingdomKey and speciesKey are present.  Notice also that all these taxonomic related keys point to specific web pages on the GBIF website. You need only to add the prefix https://www.gbif.org/species/. For example, the first taxon has key 152624576 and the corresponding webpage is https://www.gbif.org/species/152624576, while its nubkey, 2020551, points to the webpage of the taxon as mentioned in the backbone: https://www.gbif.org/species/2020551.\nNow, let’s check how many taxa you got:\nnrow(red_list$data) ## [1] 4771  However, the webpage on GBIF says 3036 records\nWho lies?!? Nobody, actually. The reason is the taxonomic classification added to each taxon. How to get the 3036 records of the red list? And which taxa are actually part of the added classification? The column origin is what we need!\nred_list$data %\u0026gt;% count(origin) ## # A tibble: 2 × 2 ## origin n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 DENORMED_CLASSIFICATION 1708 ## 2 SOURCE 3063  You need then to filter on origin with value \u0026quot;SOURCE\u0026quot;:\nred_list_taxa \u0026lt;- red_list$data %\u0026gt;% filter(origin == \u0026#34;SOURCE\u0026#34;) nrow(red_list_taxa) ## [1] 3063  Retrieve non taxonomic information from a GBIF checklist in R A checklist dataset is not always only a list of taxa: it can be enriched by extensions containing information about e.g. the distribution, the species profile or other less strict defined information described in the description extension.\nHow to get this information in R? Unfortunately, this information is linked to the taxon, not to the checklist dataset as a whole. In other words, you get an error if you try this:\ndistribution \u0026lt;- name_usage(datasetKey = datasetKey, data = \u0026#34;distribution\u0026#34;) Error in exists(z, private$crul_h_pool) : invalid first argument You need a loop over all the taxa (key):\ndistribution \u0026lt;- red_list_taxa %\u0026gt;% pull(key) %\u0026gt;% map_dfr(., function(x) { distribution_details \u0026lt;-name_usage(x, data = \u0026#34;distribution\u0026#34;) return(distribution_details$data) } ) Overview:\nkable(distribution[1:10,])    taxonKey locationId locality country temporal threatStatus establishmentMeans source remarks     152624576 ISO_3166:BE-VLG Flanders BE 2013 LEAST_CONCERN NATIVE Lock et al. (2013) Momenteel niet in gevaar   152624577 ISO_3166:BE-VLG Flanders BE 2006 CRITICALLY_ENDANGERED NATIVE Van Landuyt et al. (2006) Met verdwijning bedreigd   152624582 ISO_3166:BE-VLG Flanders BE 2013 LEAST_CONCERN NATIVE Lock et al. (2013) Momenteel niet in gevaar   152624585 ISO_3166:BE-VLG Flanders BE 2006 LEAST_CONCERN NATIVE Van Landuyt et al. (2006) Momenteel niet bedreigd   152624586 ISO_3166:BE-VLG Flanders BE 2013 LEAST_CONCERN NATIVE Lock et al. (2013) Momenteel niet in gevaar   152624589 ISO_3166:BE-VLG Flanders BE 2006 ENDANGERED NATIVE Van Landuyt et al. (2006) Bedreigd   152624591 ISO_3166:BE-VLG Flanders BE 2006 REGIONALLY_EXTINCT NATIVE Van Landuyt et al. (2006) Verdwenen uit Vlaanderen en het Brussels Gewest   152624592 ISO_3166:BE-VLG Flanders BE 2013 LEAST_CONCERN NATIVE Lock et al. (2013) Momenteel niet in gevaar   152624594 ISO_3166:BE-VLG Flanders BE 2013 LEAST_CONCERN NATIVE Lock et al. (2013) Momenteel niet in gevaar   152624595 ISO_3166:BE-VLG Flanders BE 2006 CRITICALLY_ENDANGERED NATIVE Van Landuyt et al. (2006) Met verdwijning bedreigd    This step is slower than retrieving the taxonomic information as we need 3036 calls to the API, one per taxon, instead of one.\nCan you find this information on the GBIF website? Of course! All information about the extensions is shown on the taxon page. Below a screenshot showing the section related to the distribution extension for Plea minutissima (key: 152624576):\nCite the checklist dataset If the (checklist) dataset you used is worth to be cited, you can retrieve the citation by running gbif_citation() function:\ngbif_citation(x = datasetKey) ## \u0026lt;\u0026lt;rgbif citation\u0026gt;\u0026gt; ## Citation: Maes D, Brosens D, T’jollyn F, Desmet P, Piesschaert F, Van Hoey ## S, Adriaens T, Dekoninck W, Devos K, Lock K, Onkelinx T, Packet J, ## Speybroeck J, Thomaes A, Van Den Berge K, Van Landuyt W, Verreycken H ## (2020). Validated red lists of Flanders, Belgium. Version 1.7. Research ## Institute for Nature and Forest (INBO). Checklist dataset ## https://doi.org/10.15468/8tk3tk accessed via GBIF.org on 2022-12-15.. ## Accessed from R via rgbif (https://github.com/ropensci/rgbif) on ## 2022-12-15 ## Rights: http://creativecommons.org/publicdomain/zero/1.0/legalcode  INBO datasets on GBIF INBO is an official GBIF publisher. Do you want to know which datasets have been published so far by INBO? You can have a look via this link:\nhttps://www.gbif.org/dataset/search?publishing_org=1cd669d0-80ea-11de-a9d0-f1765f95f18b\nYou can finetune the search by selecting the type of the datasets. For example, for checklist datasets:\nhttps://www.gbif.org/dataset/search?type=CHECKLIST\u0026amp;publishing_org=1cd669d0-80ea-11de-a9d0-f1765f95f18b\nCan you do this in R? Of course. Use the rgbif function dataset_search() specifying INBO’s ID as publishing organization. Again, you get a list with two slots, meta and data. Guess where the datasets are :-)\nINBO_datasets \u0026lt;- dataset_search( publishingOrg = \u0026#34;1cd669d0-80ea-11de-a9d0-f1765f95f18b\u0026#34; ) INBO_datasets$data ## # A tibble: 92 × 8 ## datasetTitle datas…¹ type hosti…² hosti…³ publi…⁴ publi…⁵ publi…⁶ ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Florabank1 - A grid-ba… 271c44… OCCU… Resear… 1cd669… Resear… 1cd669… BE ## 2 LBBG_ZEEBRUGGE - Lesse… 355b8f… OCCU… Resear… 1cd669… Resear… 1cd669… BE ## 3 DAISIE - Inventory of … 39f36f… CHEC… Resear… 1cd669… Resear… 1cd669… BE ## 4 HG_OOSTENDE - Herring … 6c860e… OCCU… Resear… 1cd669… Resear… 1cd669… BE ## 5 Watervogels - Winterin… 7f9eb6… SAMP… Resear… 1cd669… Resear… 1cd669… BE ## 6 RINSE - Registry of no… 3f5e93… CHEC… Resear… 1cd669… Resear… 1cd669… BE ## 7 O_VLIELAND - Eurasian … cd1590… OCCU… Resear… 1cd669… Resear… 1cd669… BE ## 8 Vlinderdatabank - Butt… 7888f6… OCCU… Resear… 1cd669… Resear… 1cd669… BE ## 9 VIS - Fishes in inland… 823dc5… OCCU… Resear… 1cd669… Resear… 1cd669… BE ## 10 Broedvogels - Atlas of… 81c5a0… OCCU… Resear… 1cd669… Resear… 1cd669… BE ## # … with 82 more rows, and abbreviated variable names ¹​datasetKey, ## # ²​hostingOrganization, ³​hostingOrganizationKey, ⁴​publishingOrganization, ## # ⁵​publishingOrganizationKey, ⁶​publishingCountry  To finetune the search as we did online, use type argument:\nINBO_checklists \u0026lt;- dataset_search( publishingOrg = \u0026#34;1cd669d0-80ea-11de-a9d0-f1765f95f18b\u0026#34;, type = \u0026#34;checklist\u0026#34; ) INBO_checklists$data ## # A tibble: 15 × 8 ## datasetTitle datas…¹ type hosti…² hosti…³ publi…⁴ publi…⁵ publi…⁶ ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 DAISIE - Inventory of … 39f36f… CHEC… Resear… 1cd669… Resear… 1cd669… BE ## 2 RINSE - Registry of no… 3f5e93… CHEC… Resear… 1cd669… Resear… 1cd669… BE ## 3 Validated red lists of… fc18b0… CHEC… Resear… 1cd669… Resear… 1cd669… BE ## 4 Non-validated red list… 2fc239… CHEC… Resear… 1cd669… Resear… 1cd669… BE ## 5 Ad hoc checklist of al… 1f3505… CHEC… Resear… 1cd669… Resear… 1cd669… BE ## 6 National checklists an… f9af6f… CHEC… Resear… 1cd669… Resear… 1cd669… BE ## 7 Checklist of alien bir… e1c3be… CHEC… Resear… 1cd669… Resear… 1cd669… BE ## 8 RINSE - Pathways and v… 1738f2… CHEC… Resear… 1cd669… Resear… 1cd669… BE ## 9 Registry of introduced… e082b1… CHEC… Resear… 1cd669… Resear… 1cd669… BE ## 10 Checklist of alien her… a12e2b… CHEC… Resear… 1cd669… Resear… 1cd669… BE ## 11 Checklist of alien spe… 222119… CHEC… Resear… 1cd669… Resear… 1cd669… BE ## 12 Red list of dragonflie… 72aa79… CHEC… Resear… 1cd669… Resear… 1cd669… BE ## 13 RIPARIAS target specie… fd004d… CHEC… Resear… 1cd669… Resear… 1cd669… BE ## 14 Checklist of non-nativ… 98940a… CHEC… Resear… 1cd669… Resear… 1cd669… BE ## 15 Checklist of alien par… 2c0fb6… CHEC… Resear… 1cd669… Resear… 1cd669… BE ## # … with abbreviated variable names ¹​datasetKey, ²​hostingOrganization, ## # ³​hostingOrganizationKey, ⁴​publishingOrganization, ## # ⁵​publishingOrganizationKey, ⁶​publishingCountry  INBO as hosting organization INBO is not only a GBIF publisher. It also hosts GBIF data for many other organisations. How to get all hosted datasets? By selecting INBO in the field “Host”:\nOnline: https://www.gbif.org/dataset/search?hosting_org=1cd669d0-80ea-11de-a9d0-f1765f95f18b\nIn R, by using the argument hostingOrg within function dataset_search():\ndatasets_hosted_by_inbo \u0026lt;- dataset_search( hostingOrg = \u0026#34;1cd669d0-80ea-11de-a9d0-f1765f95f18b\u0026#34;, limit = 1000) datasets_hosted_by_inbo$data ## # A tibble: 117 × 8 ## datasetTitle datas…¹ type hosti…² hosti…³ publi…⁴ publi…⁵ publi…⁶ ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Waarnemingen.be - Bird… e7cbb0… OCCU… Resear… 1cd669… Natuur… 4d3cee… BE ## 2 Florabank1 - A grid-ba… 271c44… OCCU… Resear… 1cd669… Resear… 1cd669… BE ## 3 Waarnemingen.be - Plan… bfc6fe… OCCU… Resear… 1cd669… Natuur… 4d3cee… BE ## 4 LBBG_ZEEBRUGGE - Lesse… 355b8f… OCCU… Resear… 1cd669… Resear… 1cd669… BE ## 5 DAISIE - Inventory of … 39f36f… CHEC… Resear… 1cd669… Resear… 1cd669… BE ## 6 Waarnemingen.be - Butt… 1f968e… OCCU… Resear… 1cd669… Natuur… 4d3cee… BE ## 7 Waarnemingen.be / obse… 2c38cf… CHEC… Resear… 1cd669… Natuur… 4d3cee… BE ## 8 HG_OOSTENDE - Herring … 6c860e… OCCU… Resear… 1cd669… Resear… 1cd669… BE ## 9 Waarnemingen.be - Non-… 9a0b66… OCCU… Resear… 1cd669… Natuur… 4d3cee… BE ## 10 Watervogels - Winterin… 7f9eb6… SAMP… Resear… 1cd669… Resear… 1cd669… BE ## # … with 107 more rows, and abbreviated variable names ¹​datasetKey, ## # ²​hostingOrganization, ³​hostingOrganizationKey, ⁴​publishingOrganization, ## # ⁵​publishingOrganizationKey, ⁶​publishingCountry  Notice again the limit argument with a sufficiently high number as value. Exercise: check the meta slot to check if we got all records.\nWhich publishing organizations host GBIF data at INBO?\ndatasets_hosted_by_inbo$data %\u0026gt;% distinct(publishingOrganization) ## # A tibble: 8 × 1 ## publishingOrganization ## \u0026lt;chr\u0026gt; ## 1 Natuurpunt ## 2 Research Institute for Nature and Forest (INBO) ## 3 Invasive Species Specialist Group ISSG ## 4 Flanders Environment Agency (VMM) ## 5 Province East Flanders ## 6 Ghent University ## 7 RATO vzw ## 8 Agency for Nature and Forests (Flemish Government)  A nerdy touch: the GBIF API output We said at the beginning of this tutorial that the information you get via rgbif is harvested by using the GBIF API. So, the question is: can you see the API output on the web as well? Answer is YES! You need some prefixes and again the values in the field key. Below we use the taxon Plea minutissima (key: 152624576) as example:\n taxonomic information: https://api.gbif.org/v1/species/152624576 distribution information: https://api.gbif.org/v1/species/152624576/distributions description information: https://api.gbif.org/v1/species/152624576/descriptions  And the checklist dataset? Here you go: https://api.gbif.org/v1/dataset/fc18b0b1-8777-4c8a-8cb8-f9f15870d6a9. Notice the word dataset instead of species in the prefix.\nNotice that all information exposed by the GBIF API is written using the JSON format. To parse it in a humanly readable way, you can use the Google Chrome extension JSON Formatter or another similar extension. The difference is impressive, isn’t?\nRaw: Parsed: ","href":"/tutorials/r_gbif_checklist/","title":"Get species checklist data from GBIF with rgbif"},{"content":"","href":"/authors/jhnwllr/","title":"jhnwllr"},{"content":"","href":"/tags/rgbif/","title":"rgbif"},{"content":"See the vignette/tutorial at https://docs.ropensci.org/rgbif/articles/multiple_values.html\n","href":"/tutorials/vignette_rgbif_multiple_values/","title":"rgbif: Multiple values"},{"content":"Before we start This tutorial assumes you have a GitHub account, and have Git, R and RStudio installed on your system. The purpose of this tutorial is to make clear recommendations about authentication for employees at INBO who use Git version control in their workflows. The guidelines here are meant for people who use Git and GitHub in RStudio, but the general principles can be applied to other software too (e.g. GitHub Desktop).\nIf you do not have a GitHub account, you can sign up for a new GitHub account. The installation of Git, R and RStudio at INBO, is done by IT administrators who follow these installation instructions for admins. In addition, the user is asked to do some extra steps after installation of Git, R and RStudio.\nTo authenticate means passing information (= credentials) that proves that you are exactly who you declare to be. Authentication is about securely accessing your GitHub account’s resources either directly by yourself or on behalf of you by a third party. For instance, you can grant an R package the right to do stuff on your behalf (e.g. reading your e-mail address). You determine what can and what cannot be done and this can be different for various third parties.\nMost of the material in this tutorial is explained in length in https://happygitwithr.com/connect-intro.html. We will mainly follow the advice given in the usethis package.\nAuthentication is a quite confusing and technical topic. In the TL;DR section (Too Long; Didn’t read) we just provide a summary of the recipe to follow without any explanation. Using this recipe will hopefully just work. If you want to get some intuition about what you are doing, you will need to read the remainder of the tutorial too (and also check out the weblinks included in this tutorial).\nTL;DR First, enable two-factor authentication (2FA) for your GitHub account: follow these steps. You need a time-based one-time password app for 2FA to work. We recommend a mobile app which you can download on your smartphone such as Aegis (open source, Android) or Google Authenticator (closed source, Android or iOS).\nNext, in R, install the usethis package (this will also install packages gert and gitcreds which we use below) and the checklist package:\ninstall.packages(\u0026#34;usethis\u0026#34;) install.packages(\u0026#34;checklist\u0026#34;, repos = c( inbo = \u0026#34;https://inbo.r-universe.dev\u0026#34;, CRAN = \u0026#34;https://cloud.r-project.org/\u0026#34;) ) Use the following commands to configure some global git options (the scope, your user name and email, and the default branch name to initialize a repo). You need to adapt “Your Name” and “your.name@inbo.be” in the code. It’s also possible you already configured this. You can check if this is the case with the command View(gert::git_config_global()) which lists all your current global git configuration settings.\nif (checklist::yesno( paste0(\u0026#34;Are you sure you want to execute this code?\\n\u0026#34;, \u0026#34;This action will overwrite Git global configuration settings.\\n\u0026#34;, \u0026#34;Make sure you changed \u0026#39;your name\u0026#39; and \u0026#39;your@email.com\u0026#39; in the code,\\n\u0026#34;, \u0026#34;if you didn\u0026#39;t select negative answer.\u0026#34;))) { usethis::use_git_config( scope = \u0026#34;user\u0026#34;, user.name = \u0026#34;Your Name\u0026#34;, user.email = \u0026#34;your.name@inbo.be\u0026#34;, init.defaultbranch = \u0026#34;main\u0026#34; ) } else { message(\u0026#34;Action aborted\u0026#34;) } # you can ignore this warning # Warning message: # In orig[nm] \u0026lt;- git_cfg_get(nm, \u0026#34;global\u0026#34;) %||% list(NULL) : # number of items to replace is not a multiple of replacement length If you plan to use functions that create something on GitHub (a repo, an issue, a pull request, a branch, …) and for all remote operations from the command line (git pull, git push, git clone, git fetch), you can use the following commands to create a Personal Access Token (PAT) and add it to the Git Credential Manager:\n?usethis::create_github_token # read the help file usethis::create_github_token() #browser opens, follow instructions Add the PAT to the Git Credential Manager:\n?gitcreds::gitcreds_set # read the help file gitcreds::gitcreds_set() #paste PAT After you have added the PAT to the Git Credential Manager, there is no need for you to store it elsewhere. WARNING: handle your Personal Access Token (PAT) as a secret. Anyone who has your token, has access to your GitHub account.\nCheck if everything is OK:\nusethis::git_sitrep() The default Git protocol should be ‘https’ (recommended on Windows).\nThe usethis package promotes the use of a global .gitignore file which prevents that some file types that could contain sensitive information (from your account / credentials) are tracked by the Git version control system. However, we prefer a project-specific .gitignore file for this purpose. This is one of the many things that the checklist package will take care of for you. It is therefore good practice to use the checklist package to set up your RStudio projects for either R packages or regular R code projects. We refer to the checklist package documentation for further information.\nModes of authentication The way in which you need to authenticate depends on how you access your resources in GitHub. For instance, to manage your repositories online you can sign-in to GitHub using a username and password followed by two-factor authentication. As another example, a function like checklist::new_branch() will create a new branch on GitHub and locally. In order to create a new branch, you will need a Personal Access Token with appropriate scopes if you use the HTTPS protocol. A token with no assigned scopes can only access public information.\nWe will follow the recommendations given in the usethis package for safe Git and GitHub authentication on Windows, which cover multiple facets:\n Turn on two-factor authentication for your GitHub account. Adopt HTTPS as your Git transport protocol. Use a personal access token (PAT) for all Git remote operations from the command line or from R. Allow tools to store and retrieve your credentials from the Git Credential Manager.  This deserves some further explanation. The first recommendation essentially is an extra layer of security compared to a simple username-and-password authentication on GitHub. It is unrelated to git operations. To enable two-factor authentication for your GitHub account, follow these steps. You need a time-based one-time password app for 2FA to work. We recommend a mobile app which you can download on your smartphone such as Aegis (open source, Android) or Google Authenticator (closed source, Android or iOS). The closed tools don’t require users ‘to read on’ and make everything simple, while open tools will require some minimal responsibility e.g. to take care of personal backups. Some open source desktop applications are available as well and are listed in https://github.com/andOTP/andOTP/wiki/Open-Source-2FA-Apps and https://en.wikipedia.org/wiki/Comparison_of_OTP_applications. Mostly these open source tools are geared towards offline storage and give users maximum control over their credentials. 2FA is not that intrusive. It only kicks in when you login from a new device or when your last login on a device was a long time ago. If someone steals your username and password, they still can’t login using that combination from their device due to 2FA.\nThe second recommendation is to use the HTTPS protocol to transport information (data) from your local Git repositories to their remote (online) counterparts. A good alternative is the SSH protocol. We have also included in this tutorial how to use the SSH protocol, but in our experience this is not necessary in most circumstances and more involved to implement in Windows. Users of the Linux operating system, on the other hand, may prefer the SSH protocol. It is always possible to switch between HTTPS and SSH protocols and we explain in the SSH section how to do that.\nThe third recommendation is mandatory when you use the HTTPS protocol. GitHub no longer allows a simple password for Git remote operations. The remote is the place where you store your code on a Git Server, for instance https://github.com/inbo/tutorials. Git remote operations include, among others, git clone, git fetch, git pull and git push.\nThe fourth recommendation is the easiest, because it just works out of the box with recent versions of Git (2.29 or higher). The installation of Git on Windows comes with a Git Credential Manager which allows that web apps like hackmd or R packages like gert and gh can ask for your credentials and get them from the credential store. In earlier days, it was sometimes needed to store your PAT in a .Renviron text file which you store only locally (the file will in that case contain a line GITHUB_PAT=…). This is no longer needed and should be avoided because of the risk to expose the PAT when that file is accidentally put online. You can check if you have previously set a GITHUB_PAT in your .Renviron with the R command usethis::edit_r_environ().\nCreating Personal Access Tokens (PAT) Creating a new PAT We move the steps about creating a PAT upfront, because this includes guidelines to store the PAT with the Git Credential Manager so they can be discovered automatically.\nPersonal access tokens can only be used for HTTPS Git operations. When you create a PAT, you will be asked which scopes should be allowed (i.e. what rights do you give). We suggest that you stick with the scopes recommended by the usethis package. Executing the following command takes your browser to a pre-filled form with the recommended scopes (repo, user, workflow) where you can create your PAT.\nusethis::create_github_token() After you have done this, you can store this PAT in the Git Credential Manager using this code:\ngitcreds::gitcreds_set() which will open a prompt where you can paste your PAT (or replace an old one).\nIf you want to know more about PATs read this section from happy git with R and this section from the usethis package. In case your PAT is compromised, you should deactivate it ASAP on GitHub.\nAfter you’ve done this, you can check your authentication settings. The usethis package has a function to get a situation report on your current Git/GitHub status, including information about authentication:\nusethis::git_sitrep() The situation report should normally report that the default Git protocol is https, which we will discuss next.\nRegenerating an expired PAT The PAT generated in the previous section has an expiration date associated to it. This is an extra fail safe security layer. The default is 30 days and you can specify a maximum expiration date of 1 year since creation. This means that from time to time, you will need to regenerate your PAT.\nTo do this, head over to the Github settings tokens page and click on the expired PAT. Next, click regenerate token. After that is done, run the command explained in the previous section again to add the newly generated token to the Git Credential Manager:\ngitcreds::gitcreds_set() Using the HTTPS protocol The HTTPS protocol is the GitHub default URL transport protocol that uses the Transport Layer Security (TLP) encryption protocol to encrypt communications. To learn more about what https is, read this explanation.\nThe usual way to choose the protocol is when you clone a repository to your computer. You can do this in various ways:\n Point-and-click approach:  Go to https://github.com/inbo/tutorials, press the clone button and copy the URL (default is HTTPS: https://github.com/inbo/tutorials.git) Open RStudio: File \u0026gt; New project \u0026gt; Version control \u0026gt; Git \u0026gt; paste https://github.com/inbo/tutorials.git \u0026gt; create project   Using git commands  Open a terminal (Go to the folder where you want to clone the repository using cd command or open the terminal directly at that location) type git clone https://github.com/inbo/tutorials.git   Using R packages: choose one:  usethis::create_from_github(repo_spec = \u0026quot;https://github.com/inbo/tutorials.git\u0026quot;, destdir = \u0026quot;path/to/gitrepofolder\u0026quot;) gert::git_clone(url = \u0026quot;https://github.com/inbo/tutorials.git\u0026quot;, path = \u0026quot;path/to/gitrepofolder\u0026quot;)    See also in SSH protocol the paragraph about the use of the git remote command.\nWhen you use these git commands or R functions, your PAT will be discovered from the Git Credential Manager and automatically authorize access (if you followed the steps in the previous section correctly) 1.\nCreating public-private SSH-key pairs Below we describe the steps you need to run in order to make SSH-keys (Secure SHell). The SSH protocol is, next to the HTTPS-protocol, a way to safely interact with GitHub. A general and easy to follow explanation about SSH can be read here. Both HTTPS and SSH are secure ways to communicate with a server (pass information between your computer and a server). For a discussion of technical differences and similarities on how both protocols handle security, this resource about SSH and TLS is useful.\nA private key, that is specific to your computer, and a matching public key that is stored on your GitHub account are needed. The provided instructions are for Windows and are taken from https://happygitwithr.com/ssh-keys.html. The same source may be consulted for other operating systems.\n  Open RStudio: Tools \u0026gt; Global Options…\u0026gt; Git/SVN \u0026gt; Create RSA Key… You can optionally use a passphrase for extra protection of the key. Without password, anyone who has a copy of your private key, can impersonate you when authenticating on GitHub. When the private key is password protected they need your password too. Click create and apply.\n  In RStudio, open a Rproject with Git version control and navigate to the Git pane. Open the Git Shell (More \u0026gt; Shell). First we check if the SSH agent works with the following command (The first $ is the prompt and you should not copy it. To paste in the shell click the right mouse button.):\n$ eval $(ssh-agent -s) # which should give something like this: Agent pid 59566  If that works, we need to add the SSH key (if you provided a passphrase, you will need it here.):\n$ ssh-add ~/.ssh/id_rsa    Restart RStudio. Next Tools \u0026gt; Global Options…\u0026gt; Git/SVN and click View public key in the SSH-RSA section. Copy to the clipboard.\n  Open your personal GitHub account https://github.com/\u0026lt;githubusername\u0026gt; (you may need to login). Click your profile-icon in the upperright corner and go to Settings \u0026gt; SSH and GPG keys. Click New SSH key. Paste the public key in the field and provide an informative title (e.g. \u0026lt;year\u0026gt;-\u0026lt;computername\u0026gt;). Click Add SSH Key.\n  To use the SSH protocol (or HTTPS for that matter), you need to set the remote of your repository correctly. The following command is useful to find out which protocol your repository uses (Git pane \u0026gt; More \u0026gt; Shell) (open the Shell from within the RStudio project with Git version control which you want to switch to SSH):\n$ git remote -v  You will either see https://github.com/\u0026lt;OWNER\u0026gt;/\u0026lt;REPO\u0026gt;.git or git@github.com:\u0026lt;OWNER\u0026gt;/\u0026lt;REPO\u0026gt;.git. The first one is the HTTPS-protocol, the latter one the SSH-protocol. Switching between protocols is possible at any time. To switch from HTTPS-protocol to SSH-protocol, type:\n$ git remote set-url origin git@github.com:USERNAME/REPOSITORY.git  Check if the remote is set correctly with git remote -v.\n    In case you did not follow the guidelines correctly, Git will prompt you to log in using a browser window and asks for your username and password. In the password section you will need to provide your PAT (not your GitHub password, which is indeed confusing). If you enabled two-factor authentication (like you should have), you’ll need to complete the 2FA challenge. Once you’ve authenticated successfully, your credentials are stored in the Git Credential Manager and will be used every time you clone an HTTPS URL. Git will not ask you for your credentials again unless you change your credentials.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","href":"/tutorials/git_authentication/","title":"Git and GitHub Authentication on Windows"},{"content":"","href":"/tags/rstudio/","title":"rstudio"},{"content":"WMS stands for Web Map Service. The service provides prerendered tiles at different scales. This makes it useful to include them as background images in maps.\nWe will use the leaflet package to make interactive maps.\nlibrary(leaflet) First, we define some WMS URLs for Flanders and Belgium to play with:\n# Flanders: wms_grb \u0026lt;- \u0026#34;https://geo.api.vlaanderen.be/GRB-basiskaart/wms\u0026#34; wms_ortho \u0026lt;- \u0026#34;https://geo.api.vlaanderen.be/OMWRGBMRVL/wms\u0026#34; wms_inbo \u0026lt;- \u0026#34;https://geo.api.vlaanderen.be/INBO/wms\u0026#34; wms_hunting \u0026lt;- \u0026#34;https://geo.api.vlaanderen.be/Jacht/wms\u0026#34; # Belgium: wms_cartoweb_be \u0026lt;- \u0026#34;https://cartoweb.wms.ngi.be/service\u0026#34; wms_ortho_be \u0026lt;- \u0026#34;https://wms.ngi.be/inspire/ortho/service\u0026#34;   wms_grb links to the WMS of the GRB-basiskaart, the Flemish cadastral map. It depicts land parcels, buildings, watercourses, roads and railroads.\n  wms_ortho contains a mosaic of recent orthophotos made during the winter. The layer Ortho contains the images, the layer Vliegdagcontour detail on the time when the pictures were taken.\n  wms_inbo is a WMS providing several layers.\n  wms_hunting displays hunting grounds in Flanders.\n  wms_cartoweb_be provides several cartographic layers from the National Geographic Institute (NGI).\n  wms_ortho_be provides recent orthophotos for the whole Belgian territory, compiled by NGI.\n  In the WFS tutorial, we present a handy overview of websites with WMS (and WFS) services.\nSimple maps WMS layers can be added to a leaflet map using the addWMSTiles() function.\nIt is required to define the map view with setView(), by providing a map center (as longitude and latitude coordinates) and a zoom level.\nleaflet() %\u0026gt;% setView(lng = 4.287638, lat = 50.703039, zoom = 15) %\u0026gt;% addWMSTiles( wms_grb, layers = \u0026#34;GRB_BSK\u0026#34;, options = WMSTileOptions(format = \u0026#34;image/png\u0026#34;, transparent = TRUE) )  Leaflet map with the GRB-basiskaart as background  Note: run the code to see this and the following interactive maps.\nleaflet() %\u0026gt;% setView(lng = 4.287638, lat = 50.703039, zoom = 15) %\u0026gt;% addWMSTiles( wms_ortho, layers = \u0026#34;Ortho\u0026#34;, options = WMSTileOptions(format = \u0026#34;image/png\u0026#34;, transparent = TRUE) )  Leaflet map with the orthophoto mosaic as background  leaflet() %\u0026gt;% setView(lng = 4.287638, lat = 50.703039, zoom = 15) %\u0026gt;% addWMSTiles( wms_inbo, layers = \u0026#34;PNVeg\u0026#34;, options = WMSTileOptions(format = \u0026#34;image/png\u0026#34;, transparent = TRUE) )  Leaflet map with the potential natural vegetation  leaflet() %\u0026gt;% setView(lng = 4.287638, lat = 50.703039, zoom = 15) %\u0026gt;% addWMSTiles( wms_cartoweb_be, layers = \u0026#34;topo\u0026#34;, options = WMSTileOptions(format = \u0026#34;image/png\u0026#34;) )  Leaflet map of the topographic CartoWeb layer. At zoom level 15.  Setting another zoom level of the CartoWeb service triggers the display of another topographic map. Of course you can do that interactively with the above map (not supported on this web site though).\nleaflet() %\u0026gt;% setView(lng = 4.287638, lat = 50.703039, zoom = 12) %\u0026gt;% addWMSTiles( wms_cartoweb_be, layers = \u0026#34;topo\u0026#34;, options = WMSTileOptions(format = \u0026#34;image/png\u0026#34;) )  Leaflet map of the topographic CartoWeb layer. At zoom level 12.  Combining multiple layers Below, we also add OpenStreetMap in the background.\nleaflet() %\u0026gt;% setView(lng = 4.287638, lat = 50.703039, zoom = 14) %\u0026gt;% addTiles(group = \u0026#34;OSM\u0026#34;) %\u0026gt;% addWMSTiles( wms_hunting, layers = \u0026#34;Jachtterr\u0026#34;, options = WMSTileOptions(format = \u0026#34;image/png\u0026#34;, transparent = TRUE) )  Leaflet map of hunting grounds with the OpenStreetMap in the background  By adding addWMSTiles() multiple times, several WMSs can be displayed on top of each other.\nleaflet() %\u0026gt;% setView(lng = 4.287638, lat = 50.703039, zoom = 14) %\u0026gt;% addTiles(group = \u0026#34;OSM\u0026#34;) %\u0026gt;% addWMSTiles( wms_grb, layers = \u0026#34;GRB_BSK\u0026#34;, options = WMSTileOptions(format = \u0026#34;image/png\u0026#34;, transparent = TRUE), group = \u0026#34;GRB\u0026#34; ) %\u0026gt;% addWMSTiles( wms_hunting, layers = \u0026#34;Jachtterr\u0026#34;, options = WMSTileOptions(format = \u0026#34;image/png\u0026#34;, transparent = TRUE), group = \u0026#34;hunting\u0026lt;br\u0026gt;grounds\u0026#34; ) %\u0026gt;% addLayersControl( baseGroups = \u0026#34;OSM\u0026#34;, overlayGroups = c(\u0026#34;GRB\u0026#34;, \u0026#34;hunting\u0026lt;br\u0026gt;grounds\u0026#34;), options = layersControlOptions(collapsed = FALSE) )  Leaflet map with the GRB-basiskaart, hunting ground and the OpenStreetMap (OSM) as background  The overlay layer of the NGI CartoWeb service is aimed at higher zoom levels and is useful to put on top of a map. Here we take the NGI orthophoto service as a background map. We must set the overlay layer transparent in order to see the layers below – the default option being transparent = FALSE.\nleaflet() %\u0026gt;% setView(lng = 4.287638, lat = 50.703039, zoom = 16) %\u0026gt;% addWMSTiles( wms_ortho_be, layers = \u0026#34;orthoimage_coverage\u0026#34;, group = \u0026#34;Orthophoto BE\u0026#34;) %\u0026gt;% addWMSTiles( wms_cartoweb_be, layers = \u0026#34;overlay\u0026#34;, options = WMSTileOptions(format = \u0026#34;image/png\u0026#34;, transparent = TRUE), group = \u0026#34;Topo BE\u0026#34; ) %\u0026gt;% addLayersControl( baseGroups = \u0026#34;Orthophoto BE\u0026#34;, overlayGroups = \u0026#34;Topo BE\u0026#34; )  Leaflet map with the Belgian orthophoto mosaic as background and a topographic overlay on top  ","href":"/tutorials/spatial_wms_services/","title":"Using WMS services in R"},{"content":"","href":"/tags/checklist/","title":"checklist"},{"content":"See the vignette/tutorial at https://inbo.github.io/checklist/articles/zenodo.html\n","href":"/tutorials/vignette_checklist_zenodo/","title":"checklist: Setting up the integration between GitHub, Zenodo and ORCID"},{"content":"","href":"/tags/frictionless/","title":"frictionless"},{"content":"See the vignette/tutorial at https://frictionlessdata.github.io/frictionless-r/articles/frictionless.html\n","href":"/tutorials/vignette_frictionless_frictionless/","title":"frictionless: Introduction to frictionless"},{"content":"","href":"/authors/peterdesmet/","title":"peterdesmet"},{"content":"See the vignette/tutorial at https://docs.ropensci.org/rgbif/articles/gbif_citations.html\n","href":"/tutorials/vignette_rgbif_cite_gbif_mediated_data/","title":"rgbif: Citing GBIF mediated data"},{"content":"See the vignette/tutorial at https://docs.ropensci.org/rgbif/articles/getting_occurrence_data.html\n","href":"/tutorials/vignette_rgbif_occurrence_downloads/","title":"rgbif: Downloading occurrence data from GBIF"},{"content":"See the vignette/tutorial at https://docs.ropensci.org/rgbif/articles/downloading_a_long_species_list.html\n","href":"/tutorials/vignette_rgbif_species_downloads/","title":"rgbif: Downloading species lists from GBIF"},{"content":"See the vignette/tutorial at https://docs.ropensci.org/rgbif/articles/gbif_credentials.html\n","href":"/tutorials/vignette_rgbif_credentials_setup/","title":"rgbif: Set up your GBIF username and password"},{"content":"See the vignette/tutorial at https://docs.ropensci.org/rgbif/articles/taxonomic_names.html\n","href":"/tutorials/vignette_rgbif_taxonomic_names/","title":"rgbif: Working with taxonomic names"},{"content":"","href":"/authors/stijnvanhoey/","title":"stijnvanhoey"},{"content":"","href":"/tags/waterinfo/","title":"wateRinfo"},{"content":"See the vignette/tutorial at https://docs.ropensci.org/wateRinfo/articles/wateRinfo.html\n","href":"/tutorials/vignette_waterinfo_waterinfo/","title":"wateRinfo: Introduction to downloading time series data from waterinfo.be"},{"content":"","href":"/tags/etn/","title":"etn"},{"content":"See the vignette/tutorial at https://inbo.github.io/etn/articles/acoustic_telemetry.html\n","href":"/tutorials/vignette_etn_acoustic_telemetry/","title":"etn: Explore acoustic telemetry data"},{"content":"","href":"/authors/ambermertens/","title":"ambermertens"},{"content":"Introduction and setup This tutorial originated while preparing an R-demonstration during a GIS club at INBO on 16 September 2021. (Further material of the GIS club is available here.)\nA straightforward approach for transforming spatial data is available in another tutorial. The current tutorial tackles a few extra aspects.\nThe tutorial assumes you have some pre-existing knowledge:\n Basic knowledge about coordinate reference systems (CRSs) and geodetic datums; see another tutorial and the references and links therein. Knowing how to read geospatial files with sf. There is another tutorial demonstrating some aspects.  In this tutorial we will apply conversions (without datum shift) and transformations (with datum shift) to sf and sfc objects. In the background, these operations use the PROJ library.\nLet’s load the needed R packages and prepare a more appropriate theme for mapping with ggplot2:\nlibrary(dplyr) library(sf) library(ggplot2) theme_set(theme_bw()) theme_update(panel.grid = element_line(colour = \u0026#34;grey80\u0026#34;)) library(mapview) Set up input data if needed:\nif (!file.exists(file.path(gisclubdata_path, \u0026#34;gemeenten_belgie.shp\u0026#34;))) { googledrive::drive_download(googledrive::as_id(\u0026#34;1-epL-fyKB8eS-WwuZhjsl8uyZYplAqFA\u0026#34;), file.path(tempdir(), \u0026#34;data_gisclub.zip\u0026#34;)) unzip(file.path(tempdir(), \u0026#34;data_gisclub.zip\u0026#34;), exdir = tempdir()) list.files(file.path(tempdir(), \u0026#34;data_gisclub\u0026#34;), full.names = TRUE) %\u0026gt;% file.copy(gisclubdata_path, recursive = TRUE) } The code assumes that you have a gisclubdata_path object defined (directory path as a string).\nJoining and mapping polygon layers For a given municipalities and watercourses dataset, we would like to append the municipality name to the watercourses by making a spatial join.\nReading data and trying to join Let’s read the data file of Belgian municipalities:\npath_municipalities \u0026lt;- file.path(gisclubdata_path, \u0026#34;gemeenten_belgie.shp\u0026#34;) municipalities \u0026lt;- read_sf(path_municipalities) It looks like this:\nmunicipalities Simple feature collection with 581 features and 3 fields Geometry type: MULTIPOLYGON Dimension: XYZ Bounding box: xmin: 2.541331 ymin: 49.49695 xmax: 6.408098 ymax: 51.50511 z_range: zmin: 0 zmax: 0 Geodetic CRS: WGS 84 # A tibble: 581 × 4 T_MUN_NL Shape_Leng Shape_Area geometry \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;MULTIPOLYGON [°]\u0026gt; 1 ’s Gravenbrakel 0.759 0.0108 Z (((4.095037 50.66599 0, 4.095101 50.… 2 Aalst 0.676 0.0101 Z (((4.054666 50.99444 0, 4.054666 50.… 3 Aalter 0.735 0.0154 Z (((3.41099 51.15987 0, 3.41275 51.15… 4 Aarlen 0.951 0.0148 Z (((5.860749 49.72667 0, 5.860889 49.… 5 Aarschot 0.640 0.00807 Z (((4.927684 51.03717 0, 4.92797 51.0… 6 Aartselaar 0.265 0.00141 Z (((4.401293 51.14814 0, 4.400461 51.… 7 Aat 0.977 0.0163 Z (((3.756864 50.69743 0, 3.757621 50.… 8 Affligem 0.261 0.00229 Z (((4.114181 50.92559 0, 4.115272 50.… 9 Aiseau-Presles 0.393 0.00284 Z (((4.584122 50.43827 0, 4.584458 50.… 10 Alken 0.336 0.00361 Z (((5.275147 50.9102 0, 5.275645 50.9… # … with 571 more rows  It appears that the points have three coordinate dimensions (XYZ). However the features have only two useful dimensions:\nst_dimension(municipalities)  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [38] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [112] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [149] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [186] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [223] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [260] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [297] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [334] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [371] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [408] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [445] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [482] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [519] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [556] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2  Some tools – e.g. spherical geoprocessing – expect XY data. So we will drop the Z dimension with st_zm(). Meanwhile we select the only necessary attribute and rename it, using select().\nmunicipalities \u0026lt;- municipalities %\u0026gt;% select(municipality = T_MUN_NL) %\u0026gt;% st_zm() We perform similar operations on the watercourses dataset. Here, data are already defined as two dimensions.\npath_watercourses \u0026lt;- file.path(gisclubdata_path, \u0026#34;watergangen_antw.shp\u0026#34;) watercourses \u0026lt;- read_sf(path_watercourses) watercourses Simple feature collection with 7258 features and 14 fields Geometry type: POLYGON Dimension: XY Bounding box: xmin: 138050.5 ymin: 201644.5 xmax: 162761.3 ymax: 230509 CRS: NA # A tibble: 7,258 × 15 OIDN UIDN VERSIE BEGINDATUM VERSDATUM VHAG NAAM OPNDATUM BGNINV \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;date\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;int\u0026gt; 1 205462 492073 3 2017-07-05 2021-05-06 3320 nvt 2017-06-20 13 2 179828 488809 2 2015-01-20 2021-05-06 -9 nvt 2014-12-02 4 3 223031 499842 2 2018-03-28 2021-05-06 54710 Kallebeek… 2018-02-27 4 4 267383 462960 2 2020-02-22 2021-05-06 -9 nvt 2020-01-08 4 5 72361 481073 3 2010-11-08 2021-05-06 65101 nvt 2020-06-22 6 6 223073 499879 3 2018-03-28 2021-05-06 -9 nvt 2018-02-27 13 7 72079 480970 3 2010-11-08 2021-05-06 53460 nvt 2016-06-20 3 8 145811 567202 3 2012-11-20 2021-07-07 -8 ng 2012-10-19 11 9 205453 492064 3 2017-07-05 2021-05-06 -9 nvt 2017-06-20 10 10 269842 463401 2 2020-04-22 2021-05-06 -9 nvt 2020-04-03 4 # … with 7,248 more rows, and 6 more variables: LBLBGNINV \u0026lt;chr\u0026gt;, LENGTE \u0026lt;dbl\u0026gt;, # OPPERVL \u0026lt;dbl\u0026gt;, Shape_Leng \u0026lt;dbl\u0026gt;, Shape_Area \u0026lt;dbl\u0026gt;, geometry \u0026lt;POLYGON\u0026gt;  Note that not all columns are printed. Let’s preview all columns with glimpse():\nglimpse(watercourses) Rows: 7,258 Columns: 15 $ OIDN \u0026lt;dbl\u0026gt; 205462, 179828, 223031, 267383, 72361, 223073, 72079, 14581… $ UIDN \u0026lt;dbl\u0026gt; 492073, 488809, 499842, 462960, 481073, 499879, 480970, 567… $ VERSIE \u0026lt;int\u0026gt; 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 1, 2, 2, 1, 2, 1, 2, 3, 2, 3,… $ BEGINDATUM \u0026lt;date\u0026gt; 2017-07-05, 2015-01-20, 2018-03-28, 2020-02-22, 2010-11-08… $ VERSDATUM \u0026lt;date\u0026gt; 2021-05-06, 2021-05-06, 2021-05-06, 2021-05-06, 2021-05-06… $ VHAG \u0026lt;dbl\u0026gt; 3320, -9, 54710, -9, 65101, -9, 53460, -8, -9, -9, -9, -9, … $ NAAM \u0026lt;chr\u0026gt; \u0026quot;nvt\u0026quot;, \u0026quot;nvt\u0026quot;, \u0026quot;Kallebeekgeul\u0026quot;, \u0026quot;nvt\u0026quot;, \u0026quot;nvt\u0026quot;, \u0026quot;nvt\u0026quot;, \u0026quot;nvt\u0026quot;, … $ OPNDATUM \u0026lt;date\u0026gt; 2017-06-20, 2014-12-02, 2018-02-27, 2020-01-08, 2020-06-22… $ BGNINV \u0026lt;int\u0026gt; 13, 4, 4, 4, 6, 13, 3, 11, 10, 4, 1, 4, 4, 1, 4, 4, 1, 13, … $ LBLBGNINV \u0026lt;chr\u0026gt; \u0026quot;aanmaak uniek percelenplan\\r\\n\u0026quot;, \u0026quot;bijhouding binnengebiede… $ LENGTE \u0026lt;dbl\u0026gt; 570.19, 185.33, 1491.01, 307.60, 286.06, 270.32, 48.11, 17.… $ OPPERVL \u0026lt;dbl\u0026gt; 2492.23, 494.77, 9350.36, 978.46, 432.97, 405.56, 118.29, 1… $ Shape_Leng \u0026lt;dbl\u0026gt; 570.18844, 185.32549, 1491.01243, 307.59691, 286.06075, 270… $ Shape_Area \u0026lt;dbl\u0026gt; 2492.22527, 494.77446, 9350.36080, 978.46010, 432.97004, 40… $ geometry \u0026lt;POLYGON\u0026gt; POLYGON ((141976.5 222453.1..., POLYGON ((142200.6 2050…  We only select some:\nwatercourses \u0026lt;- watercourses %\u0026gt;% select(polygon_id = OIDN, vhag_code = VHAG, watercourse = NAAM) watercourses Simple feature collection with 7258 features and 3 fields Geometry type: POLYGON Dimension: XY Bounding box: xmin: 138050.5 ymin: 201644.5 xmax: 162761.3 ymax: 230509 CRS: NA # A tibble: 7,258 × 4 polygon_id vhag_code watercourse geometry \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;POLYGON\u0026gt; 1 205462 3320 nvt ((141976.5 222453.1, 141973.1 222449.6, 1… 2 179828 -9 nvt ((142200.6 205085.9, 142197.3 205085, 142… 3 223031 54710 Kallebeekgeul ((146874.6 203777.1, 146879.3 203775.9, 1… 4 267383 -9 nvt ((148997 209136.1, 148996.7 209136.1, 148… 5 72361 65101 nvt ((141780.9 213697.8, 141780.7 213695.3, 1… 6 223073 -9 nvt ((144377.8 205161.2, 144408.7 205152.8, 1… 7 72079 53460 nvt ((140094.1 210962.7, 140093.8 210962.5, 1… 8 145811 -8 ng ((149004.6 209659.9, 149002.5 209666.4, 1… 9 205453 -9 nvt ((139682 225241.4, 139697.5 225222, 13971… 10 269842 -9 nvt ((155762.1 207273.7, 155766.5 207273, 155… # … with 7,248 more rows  In the above prints of the municipalities and watercourses objects, you could already see their coordinate reference system (CRS). With st_crs() you can extract the crs object; it is printed as:\nst_crs(municipalities) Coordinate Reference System: User input: WGS 84 wkt: GEOGCRS[\u0026quot;WGS 84\u0026quot;, DATUM[\u0026quot;World Geodetic System 1984\u0026quot;, ELLIPSOID[\u0026quot;WGS 84\u0026quot;,6378137,298.257223563, LENGTHUNIT[\u0026quot;metre\u0026quot;,1]]], PRIMEM[\u0026quot;Greenwich\u0026quot;,0, ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433]], CS[ellipsoidal,2], AXIS[\u0026quot;latitude\u0026quot;,north, ORDER[1], ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433]], AXIS[\u0026quot;longitude\u0026quot;,east, ORDER[2], ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433]], ID[\u0026quot;EPSG\u0026quot;,4326]]  The CRS name ‘WGS 84’ can also be returned by st_crs(municipalities)$Name. As seen from the WKT string, this is a geographical CRS.\nHowever for watercourses the CRS is missing:\nst_crs(watercourses) Coordinate Reference System: NA  Can we make the spatial join already?\nst_join(watercourses, municipalities) %\u0026gt;% try Error in st_geos_binop(\u0026quot;intersects\u0026quot;, x, y, sparse = sparse, prepared = prepared, : st_crs(x) == st_crs(y) is not TRUE  Fortunately, this doesn’t work: st_join() requires its inputs to belong to the same CRS.\nWell, we know that watercourses is in the projected CRS EPSG:31370 (BD72 / Belgian Lambert 72), so we can just update it:\nst_crs(watercourses) \u0026lt;- \u0026#34;EPSG:31370\u0026#34; Since the CRSs are still different, this should not be sufficient for the join.\nst_join(watercourses, municipalities) %\u0026gt;% try Error in st_geos_binop(\u0026quot;intersects\u0026quot;, x, y, sparse = sparse, prepared = prepared, : st_crs(x) == st_crs(y) is not TRUE  Indeed!\nLet’s transform municipalities to EPSG:31370:\nmunicipalities_31370 \u0026lt;- st_transform(municipalities, \u0026#34;EPSG:31370\u0026#34;) And now…\n(watercourses_comm \u0026lt;- st_join(watercourses, municipalities_31370)) Simple feature collection with 7502 features and 4 fields Geometry type: POLYGON Dimension: XY Bounding box: xmin: 138050.5 ymin: 201644.5 xmax: 162761.3 ymax: 230509 Projected CRS: BD72 / Belgian Lambert 72 # A tibble: 7,502 × 5 polygon_id vhag_code watercourse geometry municipality * \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;POLYGON [m]\u0026gt; \u0026lt;chr\u0026gt; 1 205462 3320 nvt ((141976.5 222453.1, 141973.… Beveren 2 179828 -9 nvt ((142200.6 205085.9, 142197.… Kruibeke 3 223031 54710 Kallebeekgeul ((146874.6 203777.1, 146879.… Kruibeke 4 267383 -9 nvt ((148997 209136.1, 148996.7 … Antwerpen 5 72361 65101 nvt ((141780.9 213697.8, 141780.… Beveren 6 223073 -9 nvt ((144377.8 205161.2, 144408.… Kruibeke 7 72079 53460 nvt ((140094.1 210962.7, 140093.… Beveren 8 145811 -8 ng ((149004.6 209659.9, 149002.… Antwerpen 9 205453 -9 nvt ((139682 225241.4, 139697.5 … Beveren 10 269842 -9 nvt ((155762.1 207273.7, 155766.… Mortsel # … with 7,492 more rows  Succeeded!\nTwo things to note here:\n st_join() has an argument ‘join’ that defines the type of topological relationship that is used to either join or not join a geometry from the first layer with a geometry from the second. Various so-called ‘binary predicates’ can be used to define this relationship (see ?st_join), but the default one is st_intersects(), i.e. an intersection as defined by the ‘DE-9IM’ model of binary geometry relations. Since several watercourses intersect more than one municipality, several of them are repeated in watercourses_comm, each with another values for municipality.  We chose to make the spatial join in the EPSG:31370 CRS. However, we can equally do it on a spherical model of the Earth, when using the geographical coordinates. We expect the resulting joined attributes to be equivalent. Let’s check:\nst_join(st_transform(watercourses, \u0026#34;EPSG:4326\u0026#34;), municipalities) %\u0026gt;% st_drop_geometry %\u0026gt;% all.equal(watercourses_comm %\u0026gt;% st_drop_geometry) [1] TRUE  Mapping While it has only one attribute and 581 rows, the object size of municipalities_31370 is relatively large because its polygons have numerous vertices:\nobject.size(municipalities_31370) %\u0026gt;% format(units = \u0026#34;MB\u0026#34;) [1] \u0026quot;22.1 Mb\u0026quot;  This makes a simple map take long to render in R (not shown here; just try yourself):\nggplot(data = municipalities_31370) + geom_sf() For the sake of this exercise, we make a simplified derived object comm_simpl_31370 with fewer vertices:\ncomm_simpl_31370 \u0026lt;- st_simplify(municipalities_31370, dTolerance = 10) Its size is much smaller:\nobject.size(comm_simpl_31370) %\u0026gt;% format(units = \u0026#34;MB\u0026#34;) [1] \u0026quot;3 Mb\u0026quot;  So it renders faster.\nggplot(data = comm_simpl_31370) + geom_sf() However, mind that the simplification may be OK for mapping at an appropriate scale, but beware that information has been lost, so shapes have changed and gaps + overlaps will appear between polygons. Hence don’t use this simplified version if you process it further, or for detailed mapping.\nNote the latitude-longitude graticule: the map follows the CRS of the plotted object, but the graticule is latitude-longitude by default.\nWe can plot the graticule of another CRS with coord_sf():\nggplot(data = comm_simpl_31370) + geom_sf() + coord_sf(datum = \u0026#34;EPSG:31370\u0026#34;) The above is the most obvious choice here, since it’s the CRS of the object, but we could as well plot the graticule of e.g. EPSG:3035 (ETRS89-extended / LAEA Europe):\nggplot(data = comm_simpl_31370) + geom_sf() + coord_sf(datum = \u0026#34;EPSG:3035\u0026#34;) Next to the ggplot2 package, which we used above, interactive maps can be made with the mapview package.\nHere too, time to render differs between:\nmapview(municipalities_31370, legend = FALSE) and:\nmapview(comm_simpl_31370, legend = FALSE) Note: run the code to see this and the following interactive maps.\nIn this case, mapview is using the leaflet package with some nice defaults, but in the above examples it makes a color map by default because there’s only one attribute. We don’t want this and we make some further adjustments:\nmapview(comm_simpl_31370, col.regions = \u0026#34;pink\u0026#34;, alpha.regions = 0.3, legend = FALSE, map.types = \u0026#34;OpenStreetMap\u0026#34;) Note that a separate tutorial about leaflet is available on this website!\nBeware that mapview and leaflet produce maps in the ‘WGS 84 / Pseudo-Mercator’ CRS (EPSG:3857). For that to happen, mapview transforms spatial objects on-the-fly. Pseudo Mercator or Web Mercator is a much used projection in web mapping because the coordinate projection is computed much faster. However it should not be used in official mapping or as a projected CRS for geoprocessing, since it has more distortions (including shape) than e.g. the (truly) conformal Mercator projection (e.g. in CRS EPSG:3395 – WGS 84 / World Mercator – which is best used in the equatorial region).\nLet’s show two layers with mapview:\nmapview(list(comm_simpl_31370, watercourses), col.regions = c(\u0026#34;pink\u0026#34;, \u0026#34;blue\u0026#34;), alpha.regions = c(0.2, 0.5), color = c(\u0026#34;grey\u0026#34;, NA), legend = FALSE, map.types = \u0026#34;OpenStreetMap\u0026#34;) Different transformation pipelines We consider a layer of points that correspond to locations where characteristics were determined of Flemish surfacewater bodies:\npath_points \u0026lt;- file.path(gisclubdata_path, \u0026#34;meetplaatsen_oppwater.shp\u0026#34;) points \u0026lt;- read_sf(path_points) Check the CRS:\nst_crs(points) Coordinate Reference System: User input: Amersfoort / RD New wkt: PROJCRS[\u0026quot;Amersfoort / RD New\u0026quot;, BASEGEOGCRS[\u0026quot;Amersfoort\u0026quot;, DATUM[\u0026quot;Amersfoort\u0026quot;, ELLIPSOID[\u0026quot;Bessel 1841\u0026quot;,6377397.155,299.1528128, LENGTHUNIT[\u0026quot;metre\u0026quot;,1]]], PRIMEM[\u0026quot;Greenwich\u0026quot;,0, ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433]], ID[\u0026quot;EPSG\u0026quot;,4289]], CONVERSION[\u0026quot;RD New\u0026quot;, METHOD[\u0026quot;Oblique Stereographic\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,9809]], PARAMETER[\u0026quot;Latitude of natural origin\u0026quot;,52.1561605555556, ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433], ID[\u0026quot;EPSG\u0026quot;,8801]], PARAMETER[\u0026quot;Longitude of natural origin\u0026quot;,5.38763888888889, ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433], ID[\u0026quot;EPSG\u0026quot;,8802]], PARAMETER[\u0026quot;Scale factor at natural origin\u0026quot;,0.9999079, SCALEUNIT[\u0026quot;unity\u0026quot;,1], ID[\u0026quot;EPSG\u0026quot;,8805]], PARAMETER[\u0026quot;False easting\u0026quot;,155000, LENGTHUNIT[\u0026quot;metre\u0026quot;,1], ID[\u0026quot;EPSG\u0026quot;,8806]], PARAMETER[\u0026quot;False northing\u0026quot;,463000, LENGTHUNIT[\u0026quot;metre\u0026quot;,1], ID[\u0026quot;EPSG\u0026quot;,8807]]], CS[Cartesian,2], AXIS[\u0026quot;easting (X)\u0026quot;,east, ORDER[1], LENGTHUNIT[\u0026quot;metre\u0026quot;,1]], AXIS[\u0026quot;northing (Y)\u0026quot;,north, ORDER[2], LENGTHUNIT[\u0026quot;metre\u0026quot;,1]], USAGE[ SCOPE[\u0026quot;Engineering survey, topographic mapping.\u0026quot;], AREA[\u0026quot;Netherlands - onshore, including Waddenzee, Dutch Wadden Islands and 12-mile offshore coastal zone.\u0026quot;], BBOX[50.75,3.2,53.7,7.22]], ID[\u0026quot;EPSG\u0026quot;,28992]]  The points object has the projected ‘Amersfoort / RD New’ CRS (EPSG:28992), which was created for The Netherlands – onshore. See e.g. Wikipedia for more background about this CRS.\nEssentially we are dealing with a different projected CRS than EPSG:31370. EPSG:28992 uses the ‘Amersfoort’ geodetic datum, defined using the ‘Bessel 1841’ ellipsoid, while EPSG:31370 has the ‘Reseau National Belge 1972’ datum, defined using the ‘International 1924’ ellipsoid.1\nWe will transform points (back) to EPSG:31370 since the data are outside the intended area of usage. Note that this dataset has been made for mere demonstrative purposes: since at least a part of the points fall outside the area of usage (geographic bounding box) of the ‘Amersfoort / RD New’ CRS, the dataset is not appropriate to start from. But let’s suppose it is all one had available to work with!\nStandard approach By default, the appropriate transformation pipeline (concatenated coordinate operations) can differ between different areas that are represented in a spatial dataset. This is taken care of automatically, based on the bounding boxes of each CRS, but it will also depend on the availability of specific transformation grids.\npoints_31370 \u0026lt;- st_transform(points, \u0026#34;EPSG:31370\u0026#34;) Notice the completely different coordinate ranges between both CRSs:\nst_geometry(points) #\u0026gt; Geometry set for 7320 features  #\u0026gt; Geometry type: POINT #\u0026gt; Dimension: XY #\u0026gt; Bounding box: xmin: -62470.03 ymin: 300542 xmax: 193543.8 ymax: 427302.7 #\u0026gt; Projected CRS: Amersfoort / RD New #\u0026gt; First 5 geometries: #\u0026gt; POINT (-30849.68 354341.7) #\u0026gt; POINT (-31237.91 354683.3) #\u0026gt; POINT (-28757.74 356391.3) #\u0026gt; POINT (709.4341 373861.2) #\u0026gt; POINT (116680.8 340639.1) st_geometry(points_31370) #\u0026gt; Geometry set for 7320 features  #\u0026gt; Geometry type: POINT #\u0026gt; Dimension: XY #\u0026gt; Bounding box: xmin: 3174.827 ymin: 152992.4 xmax: 260391 ymax: 278347.1 #\u0026gt; Projected CRS: BD72 / Belgian Lambert 72 #\u0026gt; First 5 geometries: #\u0026gt; POINT (35483.62 205530.5) #\u0026gt; POINT (35090.55 205866.5) #\u0026gt; POINT (37545.94 207609.8) #\u0026gt; POINT (66666.83 225461.1) #\u0026gt; POINT (183089.2 193870.8) A specific area shown with ggplot2 in EPSG:31370:\nggplot() + geom_sf(data = watercourses, fill = \u0026#34;lightblue\u0026#34;, colour = \u0026#34;lightblue\u0026#34;) + geom_sf(data = points_31370, fill = \u0026#34;red\u0026#34;, shape = 21) + lims(x = c(154e3, 155e3), y = c(218e3, 219e3)) + coord_sf(datum = \u0026#34;EPSG:31370\u0026#34;) Enforcing a specific pipeline We can request available transformation pipelines with sf_proj_pipelines():\npipelines \u0026lt;- sf_proj_pipelines(\u0026#34;EPSG:28992\u0026#34;, \u0026#34;EPSG:31370\u0026#34;) One can further limit this by specifying the area of interest (aoi argument), which we didn’t do here.\nIt is a dataframe with following structure:\nglimpse(pipelines) Rows: 13 Columns: 9 $ id \u0026lt;chr\u0026gt; \u0026quot;pipeline\u0026quot;, \u0026quot;pipeline\u0026quot;, \u0026quot;pipeline\u0026quot;, \u0026quot;pipeline\u0026quot;, \u0026quot;pipeline… $ description \u0026lt;chr\u0026gt; \u0026quot;Inverse of RD New + Amersfoort to ETRS89 (9) + Inverse o… $ definition \u0026lt;chr\u0026gt; \u0026quot;+proj=pipeline +step +inv +proj=sterea +lat_0=52.1561605… $ has_inverse \u0026lt;lgl\u0026gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRU… $ accuracy \u0026lt;dbl\u0026gt; 0.011, 0.201, 0.260, 0.450, 1.001, 1.250, 2.000, 2.000, 2… $ axis_order \u0026lt;lgl\u0026gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F… $ grid_count \u0026lt;int\u0026gt; 2, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0 $ instantiable \u0026lt;lgl\u0026gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRU… $ containment \u0026lt;lgl\u0026gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…  Some metadata about the pipelines:\npipelines %\u0026gt;% as_tibble %\u0026gt;% select(definition, description, grid_count, accuracy, instantiable) # A tibble: 13 × 5 definition description grid_count accuracy instantiable \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; 1 +proj=pipeline +step … Inverse of RD New + … 2 0.011 TRUE 2 +proj=pipeline +step … Inverse of RD New + … 1 0.201 TRUE 3 +proj=pipeline +step … Inverse of RD New + … 1 0.26 TRUE 4 +proj=pipeline +step … Inverse of RD New + … 0 0.45 TRUE 5 +proj=pipeline +step … Inverse of RD New + … 1 1.00 TRUE 6 +proj=pipeline +step … Inverse of RD New + … 0 1.25 TRUE 7 +proj=pipeline +step … Inverse of RD New + … 0 2 TRUE 8 +proj=pipeline +step … Inverse of RD New + … 0 2 TRUE 9 +proj=pipeline +step … Inverse of RD New + … 0 2 TRUE 10 +proj=pipeline +step … Inverse of RD New + … 0 2 TRUE 11 +proj=pipeline +step … Inverse of RD New + … 0 6 TRUE 12 +proj=pipeline +step … Inverse of RD New + … 0 6 TRUE 13 +proj=pipeline +step … Inverse of RD New + … 0 NA TRUE  We can see that the first one has best accuracy. Several pipelines are based on one or two grids for a better accuracy.\nWhen printing the pipelines object, its first row is printed in a more readable manner:\npipelines Candidate coordinate operations found: 13 Strict containment: FALSE Axis order auth compl: FALSE Source: EPSG:28992 Target: EPSG:31370 Best instantiable operation has accuracy: 0.011 m Description: Inverse of RD New + Amersfoort to ETRS89 (9) + Inverse of BD72 to ETRS89 (3) + Belgian Lambert 72 Definition: +proj=pipeline +step +inv +proj=sterea +lat_0=52.1561605555556 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=463000 +ellps=bessel +step +proj=hgridshift +grids=nl_nsgi_rdtrans2018.tif +step +inv +proj=hgridshift +grids=be_ign_bd72lb72_etrs89lb08.tif +step +proj=lcc +lat_0=90 +lon_0=4.36748666666667 +lat_1=51.1666672333333 +lat_2=49.8333339 +x_0=150000.013 +y_0=5400088.438 +ellps=intl  Since the rows of pipelines are sorted by instantiable and accuracy, it makes sense to designate the printed pipeline as Best instantiable operation.\nIt follows that the same information appears when manually selecting and then printing, except for the number of ‘candidate operations found’:\npipelines[1,] Candidate coordinate operations found: 1 Strict containment: FALSE Axis order auth compl: FALSE Source: EPSG:28992 Target: EPSG:31370 Best instantiable operation has accuracy: 0.011 m Description: Inverse of RD New + Amersfoort to ETRS89 (9) + Inverse of BD72 to ETRS89 (3) + Belgian Lambert 72 Definition: +proj=pipeline +step +inv +proj=sterea +lat_0=52.1561605555556 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=463000 +ellps=bessel +step +proj=hgridshift +grids=nl_nsgi_rdtrans2018.tif +step +inv +proj=hgridshift +grids=be_ign_bd72lb72_etrs89lb08.tif +step +proj=lcc +lat_0=90 +lon_0=4.36748666666667 +lat_1=51.1666672333333 +lat_2=49.8333339 +x_0=150000.013 +y_0=5400088.438 +ellps=intl  If you’d like to get even more metadata on the available pipelines, including the WKT definitions, you can also run PROJ’s projinfo command line program from within R if it’s available in your system PATH.\nsystem(\u0026#34;projinfo -s EPSG:28992 -t EPSG:31370 --spatial-test intersects -o WKT2:2019\u0026#34;) The default operation returned by PROJ’s projinfo, based on the union of the source and target CRS’s bounding box, appears to be just one pipeline with so-called ballpark accuracy. It’s obtained by dropping the --spatial-test intersects option (or replacing intersects by contains, which is default)2, and so it matches the pipeline where accuracy equals NA in our pipelines dataframe:\npipelines[13,] Candidate coordinate operations found: 1 Strict containment: FALSE Axis order auth compl: FALSE Source: EPSG:28992 Target: EPSG:31370 Best instantiable operation has only ballpark accuracy Description: Inverse of RD New + Ballpark geographic offset from Amersfoort to BD72 + Belgian Lambert 72 Definition: +proj=pipeline +step +inv +proj=sterea +lat_0=52.1561605555556 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=463000 +ellps=bessel +step +proj=lcc +lat_0=90 +lon_0=4.36748666666667 +lat_1=51.1666672333333 +lat_2=49.8333339 +x_0=150000.013 +y_0=5400088.438 +ellps=intl  The low accuracy of a ballpark transformation is because datum differences between source and target CRSs are neglected:\n It does not attempt any datum shift, hence the “ballpark” qualifier in its name. Its accuracy is unknown, and could lead in some cases to errors of a few hundreds of metres.\n (Source: https://proj.org/glossary.html#term-Ballpark-transformation)\nWith st_transform() we can enforce a specific pipeline. Note that it is only available for sfc objects, not sf objects.\nLet’s apply the most accurate pipeline to all points:\n# We select the pipeline with lowest accuracy (\u0026lt; 0.02), by filtering on accuracy. # If the grids are installed, this should match the first line of the pipelines object. chosen_pipeline_definition \u0026lt;- pipelines %\u0026gt;% filter(accuracy \u0026lt; 0.02) %\u0026gt;% pull(definition) points_31370_strict \u0026lt;- st_transform(st_geometry(points), \u0026#34;EPSG:31370\u0026#34;, pipeline = chosen_pipeline_definition) %\u0026gt;% st_sf(st_drop_geometry(points), geometry = .) %\u0026gt;% as_tibble %\u0026gt;% st_as_sf And compare:\nst_geometry(points_31370) #\u0026gt; Geometry set for 7320 features  #\u0026gt; Geometry type: POINT #\u0026gt; Dimension: XY #\u0026gt; Bounding box: xmin: 3174.827 ymin: 152992.4 xmax: 260391 ymax: 278347.1 #\u0026gt; Projected CRS: BD72 / Belgian Lambert 72 #\u0026gt; First 5 geometries: #\u0026gt; POINT (35483.62 205530.5) #\u0026gt; POINT (35090.55 205866.5) #\u0026gt; POINT (37545.94 207609.8) #\u0026gt; POINT (66666.83 225461.1) #\u0026gt; POINT (183089.2 193870.8) st_geometry(points_31370_strict) #\u0026gt; Geometry set for 7320 features (with 9 geometries empty) #\u0026gt; Geometry type: POINT #\u0026gt; Dimension: XY #\u0026gt; Bounding box: xmin: 18976.03 ymin: 152963.3 xmax: 260391 ymax: 244024 #\u0026gt; Projected CRS: BD72 / Belgian Lambert 72 #\u0026gt; First 5 geometries: #\u0026gt; POINT (35393.39 205495.3) #\u0026gt; POINT (35000.39 205831.3) #\u0026gt; POINT (37455.41 207574.3) #\u0026gt; POINT (66666.83 225461.1) #\u0026gt; POINT (183089.2 193870.8) Notice the 9 empty geometries in case of points_31370_strict!\nWhat is happening?\np \u0026lt;- ggplot() + geom_sf(data = points_31370, colour = \u0026#34;red\u0026#34;) + geom_sf(data = points_31370_strict, colour = \u0026#34;green\u0026#34;) p It appears that points too far at sea are considered invalid when applying this pipeline, and their geometry was cleared (although the corresponding row was not dropped).\nIf we make a zoomable plot, there’s more to discover.\nplotly::ggplotly(p) Most points on the map are on the exact same location between both approaches. We can see that points west and south of the geographic bounding box for usage of the Dutch CRS have a different position depending on the applied approach.\nIt can be further investigated with mapview (note that this does a further reprojection to EPSG:3857):\nmapview(list(points_31370, points_31370_strict), col.regions = c(\u0026#34;red\u0026#34;, \u0026#34;green\u0026#34;)) Here it appears that the western and southern points of points_31370 were transformed with the ballpark-accuracy pipeline: they are way off the surfacewater bodies!\nIn this case, one might prefer enforcing the specific pipeline outside the geographic bounding box of EPSG:28992, as we did for points_31370_strict. However best explore the other pipelines as well, e.g. the most accurate one that doesn’t depend on a grid, and consider using that outside the geographic bounding box.\nBottomline is that you should be suspicious about coordinates outside the CRS’s geographic bounding box! So keep an eye on them when applying a default transformation procedure.\nFurther reading and more packages Much more information can be found in the online books by Pebesma \u0026amp; Bivand (2021) and Lovelace et al. (2019). The sf package has a nice website at https://r-spatial.github.io/sf; version 0.6-1 was introduced in Pebesma (2018).\nThere are several more R packages to make beautiful maps. We specifically mention tmap (Tennekes, 2018) and mapsf for making production-ready thematic maps. tmap is used a lot in Lovelace et al. (2019).\nFurther, there is the plotKML package, which renders spatial objects in Google Earth (Hengl et al., 2015)!\nBibliography Hengl T., Roudier P., Beaudette D. \u0026amp; Pebesma E. (2015). plotKML: Scientific Visualization of Spatio-Temporal Data. Journal of Statistical Software 63. http://www.geostat-course.org/system/files/jss1079.pdf.\n Lovelace R., Nowosad J. \u0026amp; Muenchow J. (2019). Geocomputation with R. https://geocompr.robinlovelace.net/.\n Pebesma E. (2018). Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal 10 (1): 439–446. https://journal.r-project.org/archive/2018/RJ-2018-009/index.html.\n Pebesma E. \u0026amp; Bivand R. (2021). Spatial Data Science. https://r-spatial.org/book.\n Tennekes M. (2018). tmap: Thematic Maps in R. Journal of Statistical Software 84 (1): 1–39. https://doi.org/10.18637/jss.v084.i06.\n    Note that several European countries, including Belgium, now provide national projected CRSs that use the common ETRS89 datum (European Terrestrial Reference System 1989). An example is the Belgian CRS EPSG:3812 (ETRS89 / Belgian Lambert 2008).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n In the documentation of projinfo, we can read about the --spatial-test option: ‘Specify how the area of use of coordinate operations found in the database are compared to the area of use specified explicitly with --area or --bbox, or derived implicitly from the area of use of the source and target CRS. By default, projinfo will only keep coordinate operations whose area of use is strictly within the area of interest (contains strategy). If using the intersects strategy, the spatial test is relaxed, and any coordinate operation whose area of use at least partly intersects the area of interest is listed.’\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","href":"/tutorials/spatial_transform_crs_2/","title":"Coordinate operations and maps in R"},{"content":"","href":"/tags/bibliography/","title":"bibliography"},{"content":"","href":"/tags/bookdown/","title":"bookdown"},{"content":"Enter VME mode With the advent of RStudio version 1.4+, working with bibliographies and citing documents in R Markdown has much improved. In this tutorial we focus on best practices to further ease the citation experience when working with R Markdown documents. We also give some guidelines specific to the Research Institute for Nature and Forest.\nMore general tutorials can be found at the following web pages:\n https://pandoc.org/MANUAL.html#citations https://rstudio.github.io/visual-markdown-editing/citations.html  The latter webpage is part of the documentation about the Visual Markdown Editing (VME) mode that is available since RStudio 1.4+ as an alternative to writing R Markdown documents in source mode. VME is a what-you-see-is-what-you-get1 interface for interacting with R Markdown. This interface is similar to writing documents in word processing software (google docs, MS Word, \u0026hellip;). Moreover, the VME mode has more functionalities for citations than the source mode. To switch between source mode and VME, press the button :\nIn this tutorial, we assume you are familiar with the basics of R Markdown or Bookdown.\nPreparing your R Markdown document to work with citations There are various places where you can specify things that impact working with citations. The most important are specifications you can provide in the YAML header of your R Markdown document. Next to that, the RStudio global and project options provide some additional settings.\nDocument level specifications (YAML header specifications) Specifying a bibliography You can specify one or more bibliography files with the bibliography argument in a number of formats, including the most commonly used formats at the Research Institute for Nature and Forest: CSL-JSON, CSL-YAML, BibTex and EndNote.\nWe strongly recommend however to use CSL-JSON or CSL-YAML, since CSL is the native format for Pandoc.2 All other bibliographic formats are internally converted to CSL-JSON, but with the exception of CSL-YAML this can in general not be perfect. Compared to CSL-JSON, Pandoc\u0026rsquo;s CSL-YAML has identical contents while having the most human-friendly syntax. CSL-JSON is the formal standard supported by all CSL processors3 .\nSome example specifications:\n--- author: Research Institute for Nature and Forest date: '`r format(Sys.Date())`' bibliography: references.yaml ---  Note that it is not even necessary at this stage that the bibliography file exists. It is possible to just give it a name and start adding citations in VME mode. When the first citation is inserted, for instance from a linked Zotero database, the file will be automatically created and the reference will be added to the file.\nFor users of Endnote, we recommend to export selected references to BibTeX format (as far as we know, it is not possible to export to CSL-JSON format). To do this:\n Edit -\u0026gt; Output Styles -\u0026gt; Open style manager and check the BibTex Export entry make sure that BibTex Export is checked in the Edit -\u0026gt; Output styles menu as well File -\u0026gt; Export, select Text file from the file type dropdown menu and save the file as \u0026lt;filename\u0026gt;.bibtex  We recommend to only include references in the bibliography file that are cited in the document.\n--- author: Research Institute for Nature and Forest date: '`r format(Sys.Date())`' bibliography: - path/to/my/ref.json - other_references_from_endnote.bibtex ---  Specifying a citation style The Research Institute for Nature and Forest has its own citation style. There is a separate tutorial that shows how to use this style in various reference manager software.\nSpecifying a citation style is done in the YAML header using the csl argument. The csl argument should be a path or URL to a CSL style file. To specify the style file for the Research Institute for Nature and Forest it is best to use the URL of the most up to date version:\n--- author: Research Institute for Nature and Forest date: '`r format(Sys.Date())`' bibliography: ref.json csl: https://raw.githubusercontent.com/citation-style-language/styles/master/research-institute-for-nature-and-forest.csl ---  Other YAML citation arguments The ymlthis field guide, Pandoc \u0026amp; associated citeproc documentation and RStudio VME citations section mention the following useful arguments:\n citation-abbreviations  Path to a CSL abbreviations JSON file, which can be used to translate from full journal titles to abbreviated journal titles. Examples of such files can be found here.\n link-citations  Logical. Add citations hyperlinks to the corresponding bibliography entries? Default is false.\n nocite  Citation IDs (@item1) to include in the bibliography even if they are not cited in the document. Including the wildcard pattern @* will include all citations in the bibliography regardless of whether they are cited in the document.\n suppress-bibliography  Logical. Suppress bibliography? Default is false.\n lang  Locale to use in formatting citations. If this is not set, the locale is taken from the default-locale attribute of the CSL file. en-US is used if a locale is not specified in either the metadata or the CSL file. The CSL style file of the Research Institute for Nature and Forest has been specifically checked and tailored for three different languages: lang: en, lang: nl and lang: fr.\n reference-section-title  If this has a value, a section header with this title will be added before the bibliography. If reference-section-title is not specified and the document ends with a section header, this final header will be treated as the bibliography header.\n zotero  Specify one or more Zotero libraries. Or false to turn of Zotero library entirely.\n  Here is an example of a valid YAML with all citation arguments:\n--- author: Research Institute for Nature and Forest date: '2022-02-02' bibliography: ref.json csl: https://raw.githubusercontent.com/citation-style-language/styles/master/research-institute-for-nature-and-forest.csl citation-abbreviations: abbreviations.json link-citations: true nocite: - '@item1' - '@item2' suppress-bibliography: false lang: en-US zotero: true ---  Rstudio project and global options The project options allow you to select a Zotero reference manager library or libraries for the RStudio project you are working in. The default will detect your local Zotero library. It is also possible to point to other libraries, which can include a group library that is shared among co-workers.\nThe global options are more or less the same as the project options but will apply to all your RStudio projects. The defaults are OK.\nCiting documents Syntax The following Markdown syntax is used to refer to documents. It is based on citation keys and the basic building block is @citationkey.\n   syntax result (depending on chosen csl style)     @adams1975 concludes that \u0026hellip; Adams (1975) concludes that \u0026hellip;   @adams1975[p.33] concludes that \u0026hellip; Adams (1975, p. 33) concludes that \u0026hellip;   \u0026hellip; end of sentence [@adams1975]. \u0026hellip; end of sentence (Adams, 1975).   \u0026hellip; end of sentence [see @adams1975,p.33]. \u0026hellip; end of sentence (see Adams, 1975, p. 33).   delineate multiple authors with colon: [@adams1975; @aberdeen1958] delineate multiple authors with colon: (Aberdeen, 1958; Adams, 1975)    Insert a citation You can either type the syntax in source mode, or use the insert citation button in VME mode. Using the VME mode has multiple benefits:\n  autocompletion: typing @ will automatically show a list of available references\n  searching and inserting references using one of the following methods (from the menu: Insert -\u0026gt; Citation; or shortcut CTRL+SHIFT+F8)\n From the bibliography or bibliographies referred to in the YAML header\u0026rsquo;s bibliography field From a Zotero library From DOI Crossref DataCite PubMed    This means that the Zotero reference manager is highly recommended when you want to work smoothly with citations in R Markdown. This reference manager also integrates nicely with other word processing software such as MS Word and Googledocs.\nCiting R and R packages The knitr::write_bib() function comes in handy if you need to cite R packages. The first argument accepts a character vector of R package names and the second argument can be used to specify a packages.bib file to which the BibTex entries will be written.\nFor instance:\n# to cite specific packages knitr::write_bib(c(\u0026#34;base\u0026#34;, \u0026#34;bookdown\u0026#34;, \u0026#34;rmarkdown\u0026#34;), \u0026#34;packages.bib\u0026#34;) \u0026hellip; allows us to cite @R-base, @R-bookdown and @R-rmarkdown if we add packages.bib to the bibliography YAML field.\nBibliography placement By default, the bibliography will be placed at the end of the document. So, you will want a final header titled # References or # Bibliography at the end your document. See also the reference-section-title field that we discussed in Other YAML citation arguments.\nIf you want to place the bibliography somewhere else, for instance before the appendices, you can insert a div html tag in source mode:\n# References \u0026lt;div id=\u0026quot;refs\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; # Appendix  If you use the bookdown::gitbook output format, further tweaks are possible. This format allows you to write a html report or book that consists of multiple html pages. You can for instance choose to split each chapter of a report into separate html pages. In that case, by default, the bibliography is split and all citation items that are cited on a given html page are put at the end of that page, so that readers do not have to navigate to a different bibliography page to see the details of citations. This feature can be disabled by setting the split_bib YAML field to FALSE, in which case all citations cited in the entire report or book are put on a separate bibliography page. To do this, you can add specific keys in the YAML header:\n--- author: Research Institute for Nature and Forest date: '`r format(Sys.Date())`' site: bookdown::bookdown_site output: bookdown::gitbook: split_by: chapter split_bib: false ---  References Aberdeen J.E.C. (1958). The effect of quadrat size, plant size, and plant distribution on frequency estimates in plant ecology. Australian Journal of Botany 6 (1): 47–58. http://www.publish.csiro.au/paper/BT9580047.\n Adams S.N. (1975). Sheep and cattle grazing in forests: a review. Journal of Applied Ecology 12: 143\u0026ndash;152.\n    Commonly abbreviated as WYSIWYG, in contrast to WYSIWYM, of which plain Markdown (in RStudio source mode) is an example.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Pandoc is the workhorse used by R Markdown for converting markdown to a desired output (such as html).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n A CSL processor is a piece of software to parse CSL styles, locale files, item metadata and citing details, and generate citations and bibliographies in the correct format. Multiple CSL processors are available for use in software. The reference implementation is the citeproc-js processor, which is implemented e.g. in Zotero and Mendeley; the citeproc-js project has also formulated the CSL-JSON standard. Pandoc, the document conversion software used by R Markdown, uses its own citeproc CSL processor.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","href":"/tutorials/r_citations_markdown/","title":"Citations in R Markdown"},{"content":"","href":"/tags/csl/","title":"csl"},{"content":"","href":"/tags/gitbook/","title":"gitbook"},{"content":"Coordinate reference systems: minimal background What? A coordinate reference system (CRS) – also called spatial reference system (SRS) – is what you need if you want to interpret numeric coordinates as actual point locations with reference to the Earth. Two types of coordinate reference system are much used in spatial science: geodetic and projected CRSs. The former serve only to locate coordinates relative to a 3D model of the Earth surface, while the latter add a projection to generate coordinates on a 2D map. Coordinate operations convert or transform coordinates from one CRS to another, and you often need them because the CRS may differ between dataset 1, dataset 2 or a specific mapping technology (such as leaflet).\nAs you can expect, a CRS is defined by several elements. Essentially, a CRS exists of:\n a coordinate system, a ‘datum’ (s.l.): it localizes the geodetic coordinate system relative to the Earth and needs a geometric definition of the ellipsoid, only for projected CRSs: coordinate conversion parameters that determine the conversion from the geodetic to the projected coordinates.  We will not go deeper into these components, because we want to focus on implementation. However it is highly recommended to read further about this, in order to better understand what a CRS means. A good contemporary resource in an R context is the section ‘Coordinate reference systems: background’ in Bivand (2019) (there is also an accompanying video of that lesson).\nThere are a few coordinated lists of CRSs around the globe, the most famous one being the EPSG dataset, where each CRS has a unique EPSG code. You can consult these CRSs interactively at https://epsg.org (official source) and through third-party websites such as https://jjimenezshaw.github.io/crs-explorer and http://epsg.io. For example, the ‘World Geodetic System 1984’ (WGS 84) is a geodetic CRS with EPSG code 4326, and ‘BD72 / Belgian Lambert 72’ is a projected CRS with EPSG code 31370.\nIn R, you can also search for CRSs and EPSG codes since these are included in the PROJ database, used by R packages like sf. An example for Belgian CRSs:\nproj_db \u0026lt;- system.file(\u0026#34;proj/proj.db\u0026#34;, package = \u0026#34;sf\u0026#34;) # For dynamically linked PROJ, provide the path to proj.db yourself: if (proj_db == \u0026#34;\u0026#34;) proj_db \u0026lt;- proj_db_path crs_table \u0026lt;- sf::read_sf(proj_db, \u0026#34;crs_view\u0026#34;) # extracts the \u0026#34;crs_view\u0026#34; table subset(crs_table, grepl(\u0026#34;Belg|Ostend\u0026#34;, name) \u0026amp; auth_name == \u0026#34;EPSG\u0026#34;)[2:5] # A tibble: 8 × 4 auth_name code name type \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; 1 EPSG 3447 ETRS89 / Belgian Lambert 2005 projected 2 EPSG 3812 ETRS89 / Belgian Lambert 2008 projected 3 EPSG 21500 BD50 (Brussels) / Belge Lambert 50 projected 4 EPSG 31300 BD72 / Belge Lambert 72 projected 5 EPSG 31370 BD72 / Belgian Lambert 72 projected 6 EPSG 5710 Ostend height vertical 7 EPSG 6190 BD72 / Belgian Lambert 72 + Ostend height compound 8 EPSG 8370 ETRS89 / Belgian Lambert 2008 + Ostend height compound  How did we represent a CRS in R? Evolutions in PROJ and GDAL. It is good to know this, but you can skip this section if you like.\nThe reason for writing this tutorial are the recent (and ongoing) changes in several important geospatial libraries, especially GDAL and PROJ. They are used by most geospatial tools, including the key geospatial R packages rgdal, sp, sf, stars, terra and raster.\nSince long, coordinate reference systems in R (and many other tools) have been represented by so called ‘PROJ.4 strings’ (or ‘proj4strings’), referring to the long-standing version 4 of the PROJ library. But, we will not use them here! It is discouraged to use ‘PROJ strings’1 any longer to represent a CRS; several string elements for CRSs are now deprecated or unsupported. Currently, PROJ (https://proj.org) regards PROJ strings only as a means of specifying a coordinate operation (conversions or transformations between CRSs). Performing coordinate operations is the main aim of PROJ.\nLet’s just have one last nostalgic peek (and then, no more!!) to the proj4string for EPSG:31370, the Belgian Lambert 72 CRS:2\n+proj=lcc +lat_1=51.16666723333333 +lat_2=49.8333339 +lat_0=90 +lon_0=4.367486666666666 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +towgs84=-106.8686,52.2978,-103.7239,0.3366,-0.457,1.8422,-1.2747 +units=m +no_defs  Several reasons have led to recent changes in GDAL and PROJ, such as the former use of the WGS 84 CRS as an intermediate ‘hub’ in coordinate transformation or in defining a CRS’s datum (introducing unneeded errors), higher accuracy requirements in transformations and the availability of better CRS specification standards than PROJ strings. The changes are included in GDAL 3 and PROJ ≥ 6, which many R packages now support and promote.\nIn consequence, support for PROJ strings to represent a CRS is reduced and discouraged. It can still be done, preferrably adding the +type=crs element to distinguish such a string from modern PROJ strings. The latter represent a coordinate operation, not a CRS. Currently, support for most geodetic datums is already lacking in PROJ strings3 (unless one defines it indirectly, but likely less accurately, with the now deprecated +towgs84 key). The WGS 84 ensemble datum4 (datum:EPSG::6326) is now by default assumed for a CRS declared with a PROJ string.\nIf you want to read more about the changes, here are some recommended resources:\n https://www.r-spatial.org (at the time of writing, especially Pebesma \u0026amp; Bivand (2020)) Bivand (2020a) (rgdal vignette) Bivand (2020b) (video recording; see slides 45-66 in Bivand (2020c)) https://gdalbarn.com Nowosad \u0026amp; Lovelace (2020) (webinar and slides, also including other developments in spatial R)  What is the new way to represent a CRS in R? Answer: The current approach is the WKT2 string, a recent and much better standard, maintained by the Open Geospatial Consortium. WKT stands for ‘Well-known text.’ ‘WKT2’ is simply the recent version of WKT, approved in 2019, so you can also refer to it as WKT.5\nFor example, this is the WKT2 string for WGS 84:\nGEOGCRS[\u0026quot;WGS 84 (with axis order normalized for visualization)\u0026quot;, ENSEMBLE[\u0026quot;World Geodetic System 1984 ensemble\u0026quot;, MEMBER[\u0026quot;World Geodetic System 1984 (Transit)\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,1166]], MEMBER[\u0026quot;World Geodetic System 1984 (G730)\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,1152]], MEMBER[\u0026quot;World Geodetic System 1984 (G873)\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,1153]], MEMBER[\u0026quot;World Geodetic System 1984 (G1150)\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,1154]], MEMBER[\u0026quot;World Geodetic System 1984 (G1674)\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,1155]], MEMBER[\u0026quot;World Geodetic System 1984 (G1762)\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,1156]], MEMBER[\u0026quot;World Geodetic System 1984 (G2139)\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,1309]], ELLIPSOID[\u0026quot;WGS 84\u0026quot;,6378137,298.257223563, LENGTHUNIT[\u0026quot;metre\u0026quot;,1], ID[\u0026quot;EPSG\u0026quot;,7030]], ENSEMBLEACCURACY[2.0], ID[\u0026quot;EPSG\u0026quot;,6326]], PRIMEM[\u0026quot;Greenwich\u0026quot;,0, ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433], ID[\u0026quot;EPSG\u0026quot;,8901]], CS[ellipsoidal,2], AXIS[\u0026quot;geodetic longitude (Lon)\u0026quot;,east, ORDER[1], ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433, ID[\u0026quot;EPSG\u0026quot;,9122]]], AXIS[\u0026quot;geodetic latitude (Lat)\u0026quot;,north, ORDER[2], ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433, ID[\u0026quot;EPSG\u0026quot;,9122]]], USAGE[ SCOPE[\u0026quot;Horizontal component of 3D system.\u0026quot;], AREA[\u0026quot;World.\u0026quot;], BBOX[-90,-180,90,180]], REMARK[\u0026quot;Axis order reversed compared to EPSG:4326\u0026quot;]]  An alternative representation of the WKT2 string - not yet official at the time of writing - is PROJJSON. It is a more convenient format to use in coding.\nHow to specify a CRS in R? library(sp) library(raster) library(sf) Great news!\nThe R packages further down, and many that depend on them, now provide a means of CRS specification irrespective of the GDAL/PROJ version, hence compliant with newer GDAL/PROJ.\n  DO:\nThe general principle that we recommend is: specify the CRS by using the EPSG code, but do so without using a PROJ string.\nNote: in case you wish to define a custom CRS yourself, ideally use WKT2 or its more convenient PROJJSON counterpart.\n  DON’T:\nIt’s no longer advised to use PROJ strings to specify a CRS, such as +init=epsg:????, +proj=longlat, … (even though that might still work, their usage is discouraged).\n  Below it is demonstrated how to specify a CRS that is defined in the EPSG database (hence, having an EPSG code), for several important geospatial R packages: sf, sp and raster. Other geospatial R packages should normally inherit their approach.\nFirst, a practical note:\n Some packages internally still derive a PROJ string as well. This happens even while you did not enter a PROJ string. Note that the derived PROJ string will not be used further if you’re on GDAL 3 / PROJ ≥ 6, and a WKT2 string will be generated as well for actual use. In the presence of GDAL 3 / PROJ ≥ 6 and when using sp or raster, you will (at the time of writing) get a warning from rgdal about dropped keys in the generated PROJ strings.6 You can safely ignore this warning on condition that you didn’t define the CRS with a PROJ string. Also, you can suppress the warning with options(rgdal_show_exportToProj4_warnings = \u0026quot;none\u0026quot;) in the beginning of your script (before loading rgdal or dependent packages).  As a demo data set for vector data, we use a dataset of city centers (points) included in the rgdal package.\ncities \u0026lt;- st_read(system.file(\u0026#34;vectors/cities.shp\u0026#34;, package = \u0026#34;rgdal\u0026#34;)) This is how it looks like:\ncities Simple feature collection with 606 features and 4 fields Geometry type: POINT Dimension: XY Bounding box: xmin: -165.27 ymin: -53.15 xmax: 177.1302 ymax: 78.2 Geodetic CRS: WGS 84 First 10 features: NAME COUNTRY POPULATION CAPITAL geometry 1 Murmansk Russia 468000 N POINT (33.08604 68.96355) 2 Arkhangelsk Russia 416000 N POINT (40.64616 64.52067) 3 Saint Petersburg Russia 5825000 N POINT (30.45333 59.95189) 4 Magadan Russia 152000 N POINT (150.78 59.571) 5 Perm' Russia 1160000 N POINT (56.23246 58.00024) 6 Yekaterinburg Russia 1620000 N POINT (60.61013 56.84654) 7 Nizhniy Novgorod Russia 2025000 N POINT (43.94067 56.28968) 8 Glasgow UK 1800000 N POINT (-4.269948 55.86281) 9 Kazan' Russia 1140000 N POINT (49.14547 55.73301) 10 Chelyabinsk Russia 1325000 N POINT (61.39261 55.145)  We now convert it to a plain dataframe (non-spatial) with the XY coordinates as columns, for the sake of the exercise. We want to add the CRS ourselves!\ncities \u0026lt;- cbind(st_drop_geometry(cities), st_coordinates(cities)) head(cities, 10) # top 10 rows  NAME COUNTRY POPULATION CAPITAL X Y 1 Murmansk Russia 468000 N 33.086040 68.96355 2 Arkhangelsk Russia 416000 N 40.646160 64.52067 3 Saint Petersburg Russia 5825000 N 30.453327 59.95189 4 Magadan Russia 152000 N 150.780014 59.57100 5 Perm' Russia 1160000 N 56.232464 58.00024 6 Yekaterinburg Russia 1620000 N 60.610130 56.84654 7 Nizhniy Novgorod Russia 2025000 N 43.940670 56.28968 8 Glasgow UK 1800000 N -4.269948 55.86281 9 Kazan' Russia 1140000 N 49.145466 55.73301 10 Chelyabinsk Russia 1325000 N 61.392612 55.14500  sf package Note that also the stars package, useful to represent vector and raster data cubes, uses the below approach.\nLet’s check whether sf uses (for Windows: comes with) the minimal PROJ/GDAL versions that we want!\nsf::sf_extSoftVersion()  GEOS GDAL proj.4 GDAL_with_GEOS USE_PROJ_H \u0026quot;3.10.1\u0026quot; \u0026quot;3.4.0\u0026quot; \u0026quot;8.2.0\u0026quot; \u0026quot;true\u0026quot; \u0026quot;true\u0026quot; PROJ \u0026quot;8.2.0\u0026quot;  Good to go!\nDefining a CRS with sf You can simply provide the EPSG code using st_crs():\ncrs_wgs84 \u0026lt;- st_crs(4326) # WGS 84 has EPSG code 4326 It is a so-called crs object:\nclass(crs_wgs84) [1] \u0026quot;crs\u0026quot;  Printing the WKT2 string with sf You can directly acces the wkt element of the crs object:\ncat(crs_wgs84$wkt) GEOGCRS[\u0026quot;WGS 84\u0026quot;, ENSEMBLE[\u0026quot;World Geodetic System 1984 ensemble\u0026quot;, MEMBER[\u0026quot;World Geodetic System 1984 (Transit)\u0026quot;], MEMBER[\u0026quot;World Geodetic System 1984 (G730)\u0026quot;], MEMBER[\u0026quot;World Geodetic System 1984 (G873)\u0026quot;], MEMBER[\u0026quot;World Geodetic System 1984 (G1150)\u0026quot;], MEMBER[\u0026quot;World Geodetic System 1984 (G1674)\u0026quot;], MEMBER[\u0026quot;World Geodetic System 1984 (G1762)\u0026quot;], MEMBER[\u0026quot;World Geodetic System 1984 (G2139)\u0026quot;], ELLIPSOID[\u0026quot;WGS 84\u0026quot;,6378137,298.257223563, LENGTHUNIT[\u0026quot;metre\u0026quot;,1]], ENSEMBLEACCURACY[2.0]], PRIMEM[\u0026quot;Greenwich\u0026quot;,0, ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433]], CS[ellipsoidal,2], AXIS[\u0026quot;geodetic latitude (Lat)\u0026quot;,north, ORDER[1], ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433]], AXIS[\u0026quot;geodetic longitude (Lon)\u0026quot;,east, ORDER[2], ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433]], USAGE[ SCOPE[\u0026quot;Horizontal component of 3D system.\u0026quot;], AREA[\u0026quot;World.\u0026quot;], BBOX[-90,-180,90,180]], ID[\u0026quot;EPSG\u0026quot;,4326]]  There are a few extras to note:\n printing the crs object shows us both the EPSG code (more generally: the user’s CRS specification) and the WKT2 string:  crs_wgs84 Coordinate Reference System: User input: EPSG:4326 wkt: GEOGCRS[\u0026quot;WGS 84\u0026quot;, ENSEMBLE[\u0026quot;World Geodetic System 1984 ensemble\u0026quot;, MEMBER[\u0026quot;World Geodetic System 1984 (Transit)\u0026quot;], MEMBER[\u0026quot;World Geodetic System 1984 (G730)\u0026quot;], MEMBER[\u0026quot;World Geodetic System 1984 (G873)\u0026quot;], MEMBER[\u0026quot;World Geodetic System 1984 (G1150)\u0026quot;], MEMBER[\u0026quot;World Geodetic System 1984 (G1674)\u0026quot;], MEMBER[\u0026quot;World Geodetic System 1984 (G1762)\u0026quot;], MEMBER[\u0026quot;World Geodetic System 1984 (G2139)\u0026quot;], ELLIPSOID[\u0026quot;WGS 84\u0026quot;,6378137,298.257223563, LENGTHUNIT[\u0026quot;metre\u0026quot;,1]], ENSEMBLEACCURACY[2.0]], PRIMEM[\u0026quot;Greenwich\u0026quot;,0, ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433]], CS[ellipsoidal,2], AXIS[\u0026quot;geodetic latitude (Lat)\u0026quot;,north, ORDER[1], ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433]], AXIS[\u0026quot;geodetic longitude (Lon)\u0026quot;,east, ORDER[2], ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433]], USAGE[ SCOPE[\u0026quot;Horizontal component of 3D system.\u0026quot;], AREA[\u0026quot;World.\u0026quot;], BBOX[-90,-180,90,180]], ID[\u0026quot;EPSG\u0026quot;,4326]]   if the user inputted the CRS with an EPSG code (which we did!), the latter can be returned as a number:  crs_wgs84$epsg [1] 4326  You can (but should you?) export a PROJ string as well, with crs_wgs84$proj4string.\nSet the CRS of an sf object First we prepare an sf object from cities but still without a CRS:\ncities2 \u0026lt;- st_as_sf(cities, coords = c(\u0026#34;X\u0026#34;, \u0026#34;Y\u0026#34;)) cities2 Simple feature collection with 606 features and 4 fields Geometry type: POINT Dimension: XY Bounding box: xmin: -165.27 ymin: -53.15 xmax: 177.1302 ymax: 78.2 CRS: NA First 10 features: NAME COUNTRY POPULATION CAPITAL geometry 1 Murmansk Russia 468000 N POINT (33.08604 68.96355) 2 Arkhangelsk Russia 416000 N POINT (40.64616 64.52067) 3 Saint Petersburg Russia 5825000 N POINT (30.45333 59.95189) 4 Magadan Russia 152000 N POINT (150.78 59.571) 5 Perm' Russia 1160000 N POINT (56.23246 58.00024) 6 Yekaterinburg Russia 1620000 N POINT (60.61013 56.84654) 7 Nizhniy Novgorod Russia 2025000 N POINT (43.94067 56.28968) 8 Glasgow UK 1800000 N POINT (-4.269948 55.86281) 9 Kazan' Russia 1140000 N POINT (49.14547 55.73301) 10 Chelyabinsk Russia 1325000 N POINT (61.39261 55.145)  Note the missing CRS!\nLet’s add the CRS by using the EPSG code (we could also assign crs_wgs84 instead):\nst_crs(cities2) \u0026lt;- 4326 Done!\nGet the CRS of an sf object Really, all you need is st_crs(), once more!\nst_crs(cities2) Coordinate Reference System: User input: EPSG:4326 wkt: GEOGCRS[\u0026quot;WGS 84\u0026quot;, ENSEMBLE[\u0026quot;World Geodetic System 1984 ensemble\u0026quot;, MEMBER[\u0026quot;World Geodetic System 1984 (Transit)\u0026quot;], MEMBER[\u0026quot;World Geodetic System 1984 (G730)\u0026quot;], MEMBER[\u0026quot;World Geodetic System 1984 (G873)\u0026quot;], MEMBER[\u0026quot;World Geodetic System 1984 (G1150)\u0026quot;], MEMBER[\u0026quot;World Geodetic System 1984 (G1674)\u0026quot;], MEMBER[\u0026quot;World Geodetic System 1984 (G1762)\u0026quot;], MEMBER[\u0026quot;World Geodetic System 1984 (G2139)\u0026quot;], ELLIPSOID[\u0026quot;WGS 84\u0026quot;,6378137,298.257223563, LENGTHUNIT[\u0026quot;metre\u0026quot;,1]], ENSEMBLEACCURACY[2.0]], PRIMEM[\u0026quot;Greenwich\u0026quot;,0, ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433]], CS[ellipsoidal,2], AXIS[\u0026quot;geodetic latitude (Lat)\u0026quot;,north, ORDER[1], ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433]], AXIS[\u0026quot;geodetic longitude (Lon)\u0026quot;,east, ORDER[2], ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433]], USAGE[ SCOPE[\u0026quot;Horizontal component of 3D system.\u0026quot;], AREA[\u0026quot;World.\u0026quot;], BBOX[-90,-180,90,180]], ID[\u0026quot;EPSG\u0026quot;,4326]]  Great!\nAs this returns the crs object, you can also use st_crs(cities2)$wkt to specifically return the WKT2 string!\nsp package Note that the actively developed (and matured) sf package is now recommended over the sp package (a view also shared by sf and sp developers). The sp package, which has very long been the go-to package before sf matured, is maintained in order to support existing code, but it is not further developed as much.\nThe sp package relies on the rgdal R package to communicate with GDAL and PROJ, so let’s check whether rgdal uses (for Windows: comes with) the minimal PROJ/GDAL versions that we want.\nrgdal::rgdal_extSoftVersion()  GDAL GDAL_with_GEOS PROJ sp EPSG \u0026quot;3.4.0\u0026quot; \u0026quot;TRUE\u0026quot; \u0026quot;8.2.0\u0026quot; \u0026quot;1.4-6\u0026quot; \u0026quot;v10.038\u0026quot;  Okido.\nDefining a CRS with sp crs_wgs84 \u0026lt;- CRS(SRS_string = \u0026#34;EPSG:4326\u0026#34;) # WGS 84 has EPSG code 4326 It is a so-called CRS object:\nclass(crs_wgs84) [1] \u0026quot;CRS\u0026quot; attr(,\u0026quot;package\u0026quot;) [1] \u0026quot;sp\u0026quot;  Printing the WKT2 string with sp wkt_wgs84 \u0026lt;- wkt(crs_wgs84) cat(wkt_wgs84) GEOGCRS[\u0026quot;WGS 84 (with axis order normalized for visualization)\u0026quot;, ENSEMBLE[\u0026quot;World Geodetic System 1984 ensemble\u0026quot;, MEMBER[\u0026quot;World Geodetic System 1984 (Transit)\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,1166]], MEMBER[\u0026quot;World Geodetic System 1984 (G730)\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,1152]], MEMBER[\u0026quot;World Geodetic System 1984 (G873)\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,1153]], MEMBER[\u0026quot;World Geodetic System 1984 (G1150)\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,1154]], MEMBER[\u0026quot;World Geodetic System 1984 (G1674)\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,1155]], MEMBER[\u0026quot;World Geodetic System 1984 (G1762)\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,1156]], MEMBER[\u0026quot;World Geodetic System 1984 (G2139)\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,1309]], ELLIPSOID[\u0026quot;WGS 84\u0026quot;,6378137,298.257223563, LENGTHUNIT[\u0026quot;metre\u0026quot;,1], ID[\u0026quot;EPSG\u0026quot;,7030]], ENSEMBLEACCURACY[2.0], ID[\u0026quot;EPSG\u0026quot;,6326]], PRIMEM[\u0026quot;Greenwich\u0026quot;,0, ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433], ID[\u0026quot;EPSG\u0026quot;,8901]], CS[ellipsoidal,2], AXIS[\u0026quot;geodetic longitude (Lon)\u0026quot;,east, ORDER[1], ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433, ID[\u0026quot;EPSG\u0026quot;,9122]]], AXIS[\u0026quot;geodetic latitude (Lat)\u0026quot;,north, ORDER[2], ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433, ID[\u0026quot;EPSG\u0026quot;,9122]]], USAGE[ SCOPE[\u0026quot;Horizontal component of 3D system.\u0026quot;], AREA[\u0026quot;World.\u0026quot;], BBOX[-90,-180,90,180]], REMARK[\u0026quot;Axis order reversed compared to EPSG:4326\u0026quot;]]  Also note that, when printing a CRS object of sp (e.g. by running crs_wgs84), you still just get a PROJ string! We won’t print it here! However do know that the WKT2 string is currently also contained in the CRS object, just call wkt() to see it.\nSet the CRS of a Spatial* object in sp First we prepare a SpatialPointsDataFrame from cities but still without a CRS:\ncities2 \u0026lt;- cities coordinates(cities2) \u0026lt;- ~ X + Y Now, we can add a CRS:\nslot(cities2, \u0026#34;proj4string\u0026#34;) \u0026lt;- crs_wgs84 Note the name of the proj4string slot in cities2: it still reminds of old days, but the result is GDAL3/PROJ≥6 compliant!\nGet the CRS of a Spatial* object in sp Again, use wkt(): it works on both CRS and Spatial* objects!\ncat(wkt(cities2)) GEOGCRS[\u0026quot;WGS 84 (with axis order normalized for visualization)\u0026quot;, ENSEMBLE[\u0026quot;World Geodetic System 1984 ensemble\u0026quot;, MEMBER[\u0026quot;World Geodetic System 1984 (Transit)\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,1166]], MEMBER[\u0026quot;World Geodetic System 1984 (G730)\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,1152]], MEMBER[\u0026quot;World Geodetic System 1984 (G873)\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,1153]], MEMBER[\u0026quot;World Geodetic System 1984 (G1150)\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,1154]], MEMBER[\u0026quot;World Geodetic System 1984 (G1674)\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,1155]], MEMBER[\u0026quot;World Geodetic System 1984 (G1762)\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,1156]], MEMBER[\u0026quot;World Geodetic System 1984 (G2139)\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,1309]], ELLIPSOID[\u0026quot;WGS 84\u0026quot;,6378137,298.257223563, LENGTHUNIT[\u0026quot;metre\u0026quot;,1], ID[\u0026quot;EPSG\u0026quot;,7030]], ENSEMBLEACCURACY[2.0], ID[\u0026quot;EPSG\u0026quot;,6326]], PRIMEM[\u0026quot;Greenwich\u0026quot;,0, ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433], ID[\u0026quot;EPSG\u0026quot;,8901]], CS[ellipsoidal,2], AXIS[\u0026quot;geodetic longitude (Lon)\u0026quot;,east, ORDER[1], ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433, ID[\u0026quot;EPSG\u0026quot;,9122]]], AXIS[\u0026quot;geodetic latitude (Lat)\u0026quot;,north, ORDER[2], ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433, ID[\u0026quot;EPSG\u0026quot;,9122]]], USAGE[ SCOPE[\u0026quot;Horizontal component of 3D system.\u0026quot;], AREA[\u0026quot;World.\u0026quot;], BBOX[-90,-180,90,180]], REMARK[\u0026quot;Axis order reversed compared to EPSG:4326\u0026quot;]]  Et voilà!\nraster package Be aware that a terra package has been recently created as a successor to raster. It is aimed at faster processing and it is only compatible with GDAL3/PROJ≥6.\nAs you will see, raster more or less aligns with sp, although it has a few extras. For example: raster provides a crs\u0026lt;- replacement function to set the CRS.\nLet’s make a dummy raster first, without CRS:\nwithin_belgium \u0026lt;- raster(extent(188500, 190350, 227550, 229550), res = 50) values(within_belgium) \u0026lt;- 1:ncell(within_belgium) within_belgium class : RasterLayer dimensions : 40, 37, 1480 (nrow, ncol, ncell) resolution : 50, 50 (x, y) extent : 188500, 190350, 227550, 229550 (xmin, xmax, ymin, ymax) crs : NA source : memory names : layer values : 1, 1480 (min, max)  This raster is intended for use in the Belgian Lambert 72 CRS (EPSG 31370).\nDefining and printing a CRS with raster This is not applicable. Just use the facilities of the sp package if you want to make a separate CRS object for usage in a Raster* object (see below).\nSet the CRS of a Raster* object in raster We prepare a few copies of the data:\nwithin_belgium1 \u0026lt;- within_belgium within_belgium2 \u0026lt;- within_belgium within_belgium3 \u0026lt;- within_belgium within_belgium4 \u0026lt;- within_belgium Setting the CRS is done with the crs\u0026lt;- replacement function. It can take a multitude of formats; these are all equivalent:\ncrs(within_belgium1) \u0026lt;- 31370 crs(within_belgium2) \u0026lt;- \u0026#34;EPSG:31370\u0026#34; crs(within_belgium3) \u0026lt;- st_crs(31370)$wkt # a WKT string crs(within_belgium4) \u0026lt;- CRS(SRS_string = \u0026#34;EPSG:31370\u0026#34;) # an sp CRS object Note that we could also have provided the crs argument in raster(), when creating the RasterLayer object. It can take any of the above formats.\nGet the CRS of a Raster* object in raster It goes the same as in sp:\ncat(wkt(within_belgium1)) PROJCRS[\u0026quot;BD72 / Belgian Lambert 72\u0026quot;, BASEGEOGCRS[\u0026quot;BD72\u0026quot;, DATUM[\u0026quot;Reseau National Belge 1972\u0026quot;, ELLIPSOID[\u0026quot;International 1924\u0026quot;,6378388,297, LENGTHUNIT[\u0026quot;metre\u0026quot;,1]]], PRIMEM[\u0026quot;Greenwich\u0026quot;,0, ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433]], ID[\u0026quot;EPSG\u0026quot;,4313]], CONVERSION[\u0026quot;Belgian Lambert 72\u0026quot;, METHOD[\u0026quot;Lambert Conic Conformal (2SP)\u0026quot;, ID[\u0026quot;EPSG\u0026quot;,9802]], PARAMETER[\u0026quot;Latitude of false origin\u0026quot;,90, ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433], ID[\u0026quot;EPSG\u0026quot;,8821]], PARAMETER[\u0026quot;Longitude of false origin\u0026quot;,4.36748666666667, ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433], ID[\u0026quot;EPSG\u0026quot;,8822]], PARAMETER[\u0026quot;Latitude of 1st standard parallel\u0026quot;,51.1666672333333, ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433], ID[\u0026quot;EPSG\u0026quot;,8823]], PARAMETER[\u0026quot;Latitude of 2nd standard parallel\u0026quot;,49.8333339, ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433], ID[\u0026quot;EPSG\u0026quot;,8824]], PARAMETER[\u0026quot;Easting at false origin\u0026quot;,150000.013, LENGTHUNIT[\u0026quot;metre\u0026quot;,1], ID[\u0026quot;EPSG\u0026quot;,8826]], PARAMETER[\u0026quot;Northing at false origin\u0026quot;,5400088.438, LENGTHUNIT[\u0026quot;metre\u0026quot;,1], ID[\u0026quot;EPSG\u0026quot;,8827]]], CS[Cartesian,2], AXIS[\u0026quot;easting (X)\u0026quot;,east, ORDER[1], LENGTHUNIT[\u0026quot;metre\u0026quot;,1]], AXIS[\u0026quot;northing (Y)\u0026quot;,north, ORDER[2], LENGTHUNIT[\u0026quot;metre\u0026quot;,1]], USAGE[ SCOPE[\u0026quot;Engineering survey, topographic mapping.\u0026quot;], AREA[\u0026quot;Belgium - onshore.\u0026quot;], BBOX[49.5,2.5,51.51,6.4]], ID[\u0026quot;EPSG\u0026quot;,31370]]  Let’s verify whether all objects do indeed have the same CRS:\nall.equal(wkt(within_belgium1), wkt(within_belgium2)) [1] TRUE  all.equal(wkt(within_belgium1), wkt(within_belgium3)) [1] TRUE  all.equal(wkt(within_belgium1), wkt(within_belgium4)) [1] TRUE  YES!\nLiterature Bivand R. (2019). ECS530: (III) Coordinate reference systems. Tuesday 3 December 2019, 09:15-11.00, aud. C. https://rsbivand.github.io/ECS530_h19/ECS530_III.html#coordinate_reference_systems:_background (accessed September 16, 2020).\n Bivand R. (2020a). Migration to PROJ6/GDAL3. http://rgdal.r-forge.r-project.org/articles/PROJ6_GDAL3.html (accessed September 21, 2020).\n Bivand R. (2020c). How R Helped Provide Tools for Spatial Data Analysis. https://github.com/rsbivand/celebRation20_files/raw/master/bivand_200229.pdf.\n Bivand R. (2020b). Upstream software dependencies of the R-spatial ecosystem (video recording). In: How R Helped Provide Tools for Spatial Data Analysis @ CelebRation 2020. https://youtu.be/D4-roPsMz48?t=2166 (accessed September 21, 2020).\n Nowosad J. \u0026amp; Lovelace R. (2020). Recent changes in R spatial and how to be ready for them. https://geocompr.github.io/post/2020/whyr_webinar004/ (accessed September 21, 2020).\n Pebesma E. \u0026amp; Bivand R. (2020). R spatial follows GDAL and PROJ development. https://www.r-spatial.org/r/2020/03/17/wkt.html (accessed September 21, 2020).\n    ‘PROJ string’ is the term used in current PROJ documentation. Here, we only use ‘PROJ.4 string’ (or ‘proj4string’) when referring specifically to the PROJ string appearance in version 4 of the PROJ software.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Note that the currently returned PROJ string for EPSG:31370, if requested from PROJ ≥ 6 or GDAL 3 (not shown), lacks the datum reference which, in PROJ.4, was defined indirectly by +towgs84 with the 7-parameter (Helmert) transformation to the WGS 84 datum. Hence the current PROJ string is a deficient representation of EPSG:31370.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Formerly, more geodetic datums could be specified in a PROJ string with the +datum key. Currently only WGS84, NAD27 and NAD83 are still supported this way. Further, if an ellipsoid is specified with +ellps (and that includes the WGS 84 ellipsoid), the datum of the resulting CRS is considered as ‘unknown.’ The usage of PROJ strings to define CRSs, including the +datum and +towgs84 elements, will remain supported for mere backward compatibility (to support existing data sets), but is regarded deprecated and is discouraged by PROJ.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n An ensemble datum is a collection of different but closely related datum realizations without making a distinction between them. By not specifying the applicable datum realization for a coordinate data set, some degree of inaccuracy is allowed when using an ensemble datum as part of a CRS.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n In order to emphasize the fact that the improvements in version 2 were instructive to the new versions of GDAL and PROJ, you will often see explicit mention of ‘WKT2.’\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The packages give a warning especially to make developers of other, dependent packages aware that they should also make sure their functions do not require PROJ strings or use hardcoded PROJ strings. Instead, they should defer the handling of CRS representation to the basic geospatial R packages (i.e. dependent on the version of PROJ/GDAL). So, the appearance of these warnings marks a period of transition in order to let other packages become GDAL 3 and PROJ ≥ 6 compliant. And the good news is that most popular geospatial packages have become GDAL 3 and PROJ ≥ 6 compliant!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","href":"/tutorials/spatial_crs_coding/","title":"Goodbye PROJ.4! How to specify a coordinate reference system in R?"},{"content":"","href":"/tags/inbomd/","title":"INBOmd"},{"content":"","href":"/categories/literature/","title":"literature"},{"content":"","href":"/tags/markdown/","title":"markdown"},{"content":"","href":"/tags/pandoc/","title":"pandoc"},{"content":"","href":"/tags/reports/","title":"reports"},{"content":"","href":"/tags/rmarkdown/","title":"rmarkdown"},{"content":"","href":"/tags/visual-markdown-editing/","title":"visual markdown editing"},{"content":"","href":"/tags/zotero/","title":"zotero"},{"content":"","href":"/tags/renv/","title":"renv"},{"content":"What is renv? renv is an R package that provides tools to save the version of R and all packages used in a particular directory with R code (e.g. an RStudio project directory), and to reproduce (recreate) this environment on another (or your future) system. Because software versions can have important effects on the results of your work, it is important to be able to reproduce versions.\nStart using renv For general advice on renv setup and usage, see https://rstudio.github.io/renv. The functions you mostly need, are renv::init(), renv::snapshot(), renv::status(), renv::restore() (always put renv::restore() in the beginning of your R code).\nWhen using git version control, you should commit all new untracked files created by renv::init(). The most important file is renv.lock, which defines R and package versions of the project.\nAim of this tutorial This tutorial addresses the specific aspect of how to update R and packages in an renv environment and store that information in renv.lock. It is typically needed while elaborating a project, because you wish to use the latest available packages and R version.\nProcedure Our recommended approach is:\n Update all installed packages in the default user R library of your system, i.e. outside an renv environment. See this tutorial. In this way these package versions will need to be downloaded and installed only once. You may also want to update packages from other sources, such as GitHub: use remotes::install_github(). Update packages, and possibly R, in a project renv environment, and store the new state in renv.lock.  To apply step 2, choose the applicable situation below. We assume you already applied step 1!\nOnly updating packages (keeping same R version) Within the project\u0026rsquo;s R session (with renv activated):\nrenv::upgrade() # upgrades renv, if new version is available renv::update() # updates packages from CRAN and GitHub, within the project renv::snapshot() # inspect the message before confirming to overwrite renv.lock Note:\n To update packages in the renv environment, renv will take package copies of your user R library if the required versions are available there. Note that the copies are stored in a global renv package cache, i.e. a central directory where renv stores packages versions for all projects where it is used, for each R x.y version, on an as-needed basis. The project environment itself is based on symlinks to the particular package versions in the cache.  If you use version control, then don\u0026rsquo;t forget to effectively commit the changed renv.lock file.\nUpdating R and packages This procedure is relevant to the x.y upgrades, not to the patches. That is, when x or y change in R version x.y.z. When only z has changed (i.e. a \u0026lsquo;patch\u0026rsquo;), previous procedure still applies.\nBelow procedure assumes:\n you already installed the new R version on your system; you already updated your R packages in the default user R library of your system; you have now started the project\u0026rsquo;s R session with the new R version (with renv activated).  renv::upgrade() # upgrades renv, if new version is available renv::hydrate(update = \u0026#34;all\u0026#34;) # populates the renv cache with copies of up to  # date package versions, needed by the project # renv::update() should have no effect now, but running it as well won\u0026#39;t harm  # to check that all packages are indeed up to date renv::snapshot() # inspect the message before confirming to overwrite renv.lock If you use version control, then don\u0026rsquo;t forget to effectively commit the changed renv.lock file.\n","href":"/tutorials/r_renv_update/","title":"Updating the renv environment of an R project"},{"content":"","href":"/authors/cecileherr/","title":"cecileherr"},{"content":"","href":"/authors/janwouters/","title":"janwouters"},{"content":"","href":"/tags/n2khab/","title":"n2khab"},{"content":"See the vignette/tutorial at https://inbo.github.io/n2khab/articles/v025_geospatial_hab.html\n","href":"/tutorials/vignette_n2khab_v025_geospatial_hab/","title":"n2khab: Working with geospatial data sources of habitat (sub)types and RIBs"},{"content":"See the vignette/tutorial at https://inbo.github.io/checklist/articles/philosophy.html\n","href":"/tutorials/vignette_checklist_philosophy/","title":"checklist: Philosophy of the checklist package"},{"content":"See the vignette/tutorial at https://inbo.github.io/checklist/articles/getting_started.html\n","href":"/tutorials/vignette_checklist_getting_started/","title":"checklist: Setting up checklist"},{"content":"In this article of the inborutils package it is explained how to use the functions download_knmi_data_hour, read_knmi_data, read_mow_data and read_kmi_data.\n","href":"/tutorials/inborutils_download_data/","title":"(Down)load KNMI, KMI or MOW-HIC data using the inborutils package"},{"content":"","href":"/tags/inborutils/","title":"inborutils"},{"content":"","href":"/tags/analysis-of-variance/","title":"analysis of variance"},{"content":"","href":"/tags/explorative-data-analysis/","title":"explorative data analysis"},{"content":"","href":"/tags/inla/","title":"INLA"},{"content":"","href":"/tags/mixed-model/","title":"mixed model"},{"content":"","href":"/tags/multivariate-statistics/","title":"multivariate statistics"},{"content":"During the course of the years, we have taught many statistics courses at the Research Institute for Nature and Forest (INBO), both for scientists and technicians. Most of these courses have been developed and taught in-house. Some (mostly advanced) courses were taught in-house by external teachers.\nWhile we do plan to periodically repeat these courses for new employees and as a refresher, the course material can always be consulted and used for self-teaching or as a reference.\nWe chose to maintain the links to googledrive folders with the course material on our intranet and therefore limit ourselves here to just point you to that link. You can find the overview here. This course material is only accessible for INBO employees.\nDo note, however, that more recently developed in-house statistics courses will most likely end up being part of the public tutorials website you are visiting. An example is the follow-up INLA workshop. Moreover, we have several short tutorials on very specific statistical topics. The best way to find these is to visit the statistics category.\n","href":"/articles/inbo_stats_courses/","title":"Statistics courses @ INBO!"},{"content":"","href":"/tags/uncertainty/","title":"uncertainty"},{"content":"","href":"/tags/watina/","title":"watina"},{"content":"See the vignette/tutorial at https://inbo.github.io/watina/articles/v010_getstarted.html\n","href":"/tutorials/vignette_watina_v010_getstarted/","title":"watina: Getting started with watina: connecting and selecting locations"},{"content":"","href":"/authors/davorj/","title":"davorj"},{"content":"","href":"/tags/gwlogger/","title":"gwloggeR"},{"content":"See the vignette/tutorial at https://dov-vlaanderen.github.io/groundwater-logger-validation/gwloggeR/docs/articles/Airpressure.html\n","href":"/tutorials/vignette_gwlogger_airpressure/","title":"gwloggeR: Air pressure"},{"content":"See the vignette/tutorial at https://dov-vlaanderen.github.io/groundwater-logger-validation/gwloggeR/docs/articles/Airpressure-Drift.html\n","href":"/tutorials/vignette_gwlogger_airpressure_drift/","title":"gwloggeR: Air pressure: drift"},{"content":"See the vignette/tutorial at https://dov-vlaanderen.github.io/groundwater-logger-validation/gwloggeR/docs/articles/Hydropressure.html\n","href":"/tutorials/vignette_gwlogger_hydropressure/","title":"gwloggeR: Hydrostatic pressure"},{"content":"See the vignette/tutorial at https://dov-vlaanderen.github.io/groundwater-logger-validation/gwloggeR/docs/articles/gwloggeR.html\n","href":"/tutorials/vignette_gwlogger_gwlogger/","title":"gwloggeR: Introduction to gwloggeR"},{"content":"","href":"/tags/endnote/","title":"endnote"},{"content":"","href":"/authors/herwigborremans/","title":"herwigborremans"},{"content":"","href":"/tags/mendeley/","title":"mendeley"},{"content":"","href":"/categories/styleguide/","title":"styleguide"},{"content":"The INBO citation style Most journals, publishers and companies use a specified format for citations and bibliographies. The Research Institute for Nature and Forest (INBO) is no exception.\nAll bibliography managers provide ways to automate the formatting of citations and bibliographies which greatly facilitates scientific publishing. Depending on the bibliographic manager software, the style formatting rules are stored in specific files. In this tutorial you will learn how to use the INBO citation style in EndNote, Mendeley and Zotero bibliography managers.\nEndNote uses a *.ens file to store the style formatting rules, whereas Mendeley and Zotero use the open standard *.csl (Citation Style Language). EndNote, Zotero and Mendeley have recommended ways of finding and installing these files and will always point to the latest version of the style files when properly installed. This is explained in Use in bibliography managers.\nIn case you want to manually download the latest official CSL file of the INBO citation style (not the recommended way; better use your bibliography manager): it is available at the CSL style repository. In case older versions are needed, they can be obtained from successively numbered releases from our forked styles repo. To use older versions, you will need to manually download and add the file to your bibliography manager.\nThe EndNote *.ens file can be downloaded here (only accessible for INBO employees).\nCurious how the formatted INBO citation style and bibliography look like for various document types? Take a look at these links:\n PDF with English bibliography based on CSL HTML with English bibliography based on CSL PDF with Dutch bibliography based on CSL HTML with Dutch bibliography based on CSL  Use in bibliography managers Zotero   Open Zotero\n  Go to Edit \u0026gt; Preferences \u0026gt; Cite and click on “Get additional styles…”\n  In the search field in the popup window that opens type “Research Institute for Nature and Forest” and click on the corresponding link\n  There are good online tutorials that explain how to use Zotero in combination with google docs, MS Word and LibreOffice.\nMendeley   Open Mendeley\n  Go to View \u0026gt; Citation styles \u0026gt; More styles \u0026gt; Get more styles\n  In the search field type “Research Institute for Nature and Forest” and click “use this style”\n  Guidelines on how to use Mendeley in combination with MS Word or LibreOffice can be found here.\nEndNote   Download the INBO style file\n  If you do not already have a folder where style files are stored, create one (make sure you have read and write rights and choose a good location that will not be accidentally deleted (not on your desktop)\n  In EndNote go to Edit \u0026gt; Preferences \u0026gt; Folder Locations \u0026gt; Style Folder and choose the folder containing the style file(s)\n  Go to Edit \u0026gt; Output Styles \u0026gt; Open Style Manager and activate the INBO style and other styles you want to use (check the box)\n  The styles carry a name and version number, so they can be used at will.\n  The INBO style can now be used in Windows Word through the EndNote add-on.\n  The University of Melbourne provides good guidelines on how to use EndNote in combination with MS Word.\n","href":"/tutorials/style_inbo_bibliography/","title":"Use of INBO citation style with bibliography managers"},{"content":"","href":"/tags/coding-club/","title":"coding club"},{"content":"","href":"/tags/data/","title":"data"},{"content":"","href":"/authors/emmacartuyvels/","title":"emmacartuyvels"},{"content":"Before each coding club you will usually have to download some datasets and some scripts. This can be done manually from the corresponding GitHub webpages but it is much easier to use the setup_codingclub_session() function of the inborutils package.\nTo begin, make sure that you have the latest version of inborutils installed by running the following code:\nif (!\u0026#34;remotes\u0026#34; %in% rownames(installed.packages())) { install.packages(\u0026#34;remotes\u0026#34;) } remotes::install_github(\u0026#34;inbo/inborutils\u0026#34;) Next load the package:\nlibrary(inborutils) The function takes four arguments with the following default settings:\n session_date which is set to the present day root_dir which is set to the current/project directory \u0026quot;.\u0026quot; src_rel_path which is set to the subdirectory src data_rel_path which is set to the subdirectory data  So when we just run the function like this:\nsetup_codingclub_session() … we will get the coding club materials for the date of today (if and only if today there is a coding club, otherwise there are no materials and nothing will be downloaded). These materials will be saved in the src and data folders of the current working directory or project directory if you work within a RStudio project as recommended in the coding club’s getting started. R scripts are downloaded in src while datasets are downloaded in data using the date of today in the “YYYYMMDD” format as subfolders. Your coding club folder will look like this:\n├── src ├── YYYYMMDD └── YYYYMMDD_challenges.r └── data ├── YYYYMMDD ├── YYYYYMMDD_filename1.txt ├── YYYYYMMDD_filename2.txt # if more than one dataset is used └── ...  If you want to get the coding club materials for a past date, use the “YYYYMMDD” format, e.g. for material of August 25, 2020:\nsetup_codingclub_session(\u0026#34;20200225\u0026#34;) The subfolders ./src/20200225 and ./data/20200225 are automatically created and files downloaded if not present:\n├── src ├── 20200225 ├── 20200225_create_messy_project.r ├── 20200225_challenges.r └── 20200225_challenges_solutions.r └── data ├── 20200225 └── 20200225_urban_gaia_policy.txt  This function is also interactive: before downloading starts a message appears on screen showing the URL of the files to download and the absolute path where R scripts and data will be downloaded, followed by the question: Do you want to continue? (Y/n) as shown below:\nIf everything is ok, enter Y (y works as well, but it’s a kind of a secret…)\nIf your folders are not named src or data (although we recommend to name them as such) but for example scripts and data_codingclub you can specify them as such:\nsetup_codingclub_session(\u0026#34;20200225\u0026#34;, src_rel_path = \u0026#34;scripts\u0026#34;, data_rel_path = \u0026#34;data_codingclub\u0026#34;) ├── scripts ├── 20200225 ├── 20200225_create_messy_project.r ├── 20200225_challenges.r └── 20200225_challenges_solutions.r └── data_codingclub ├── 20200225 └── 20200225_urban_gaia_policy.txt  To make the function even more flexible, you can modify the root directory although you shouldn’t need to while working within an RStudio project. For example, if you want to move the root to the parent directory:\nsetup_codingclub_session(root_dir = \u0026#34;../\u0026#34;) Last but not least, you can always check examples and documentation online or via R console:\n?setup_codingclub_session Doesn’t setup_codingclub_session() work as expected? Do you think there is a bug? Or do you have some ideas to improve the function? Raise your hand! Write an issue or contact one of the INBO coding club organizers. We will be happy to help you.\n","href":"/tutorials/r_setup_codingclub_session/","title":"Tutorial setup_codingclub_session()"},{"content":"","href":"/tags/google/","title":"google"},{"content":"googlesheets4 The R package googlesheets4 provides the functionality to read and write data from a Google sheet.\nThis package is very well documented, so we just have to provide you with the right links:\n read sheets write sheets  There are also two specific in-depth articles: one about reading and one about writing sheets.\nA complete list of vignettes can be found in the Articles page.\ngooglesheets4 vs googlesheets The package googlesheets4 is the follower of googlesheets which doesn\u0026rsquo;t work anymore as it is built on the outdated Sheets v3 API. If you still have code using this package, you will get this error:\n Sign in with Google temporarily disabled for this app\n Please, consider to update your code by using the new package, googlesheets4. Good luck!\n","href":"/tutorials/r_google_sheets/","title":"Read data from Google sheets"},{"content":"","href":"/tags/spreadsheet/","title":"spreadsheet"},{"content":"This page lists selected literature and online resources. Some are related to existing tutorial pages, while others are not. They are supposed to be of high interest to this site’s users.\nSeveral of the resources were added based on an inspiring talk by Julia Lowndes at the SAFRED conference, Brussels, 27 Feb 2018.\nThese resources focus on the learning and teaching aspect, while they often also give an overview of scientific computing workflows.\nOpen data science in general  Lowndes et al. (2017): learning open data science tools Hampton et al. (2017): training approaches and needed skills in data science Stevens et al. (2018): local community of practice for scientific programming: why, how (including scheme), challenges National Center for Ecological Analysis \u0026amp; Synthesis (2020): resources for learning, collaborating and inspiring others  In relation to statistics teaching  Kaplan (2017): ten organizing blocks for introductory statistics teaching in the present data science context Cetinkaya-Rundel \u0026amp; Rundel (2017): computational infrastructure and toolkit choices to allow for the necessary pedagogical innovations in statistics education  Bibliography Cetinkaya-Rundel M. \u0026amp; Rundel C.W. (2017). Infrastructure and tools for teaching computing throughout the statistical curriculum. PeerJ Preprints 5: e3181v1. https://doi.org/10.7287/peerj.preprints.3181v1.\nHampton S.E., Jones M.B., Wasser L.A., Schildhauer M.P., Supp S.R., Brun J., Hernandez R.R., Boettiger C., Collins S.L., Gross L.J., Fernández D.S., Budden A., White E.P., Teal T.K., Labou S.G. \u0026amp; Aukema J.E. (2017). Skills and Knowledge for Data-Intensive Environmental Research. BioScience 67 (6): 546–557. https://doi.org/10.1093/biosci/bix025.\nKaplan D.T. (2017). Teaching stats for data science. PeerJ Preprints 5: e3205v1. https://doi.org/10.7287/peerj.preprints.3205v1.\nLowndes J.S.S., Best B.D., Scarborough C., Afflerbach J.C., Frazier M.R., O’Hara C.C., Jiang N. \u0026amp; Halpern B.S. (2017). Our path to better science in less time using open data science tools. Nature Ecology \u0026amp; Evolution 1 (6): s41559-017-0160-017. https://doi.org/10.1038/s41559-017-0160.\nNational Center for Ecological Analysis \u0026amp; Synthesis (2020). Openscapes Resources [WWW document]. https://www.openscapes.org/resources/ (accessed September 22, 2020).\nStevens S.L.R., Kuzak M., Martinez C., Moser A., Bleeker P.M. \u0026amp; Galland M. (2018). Building a local community of practice in scientific programming for Life Scientists. bioRxiv 265421. https://doi.org/10.1101/265421.\n","href":"/articles/skills/","title":"Acquiring the skills"},{"content":"","href":"/tags/literature/","title":"literature"},{"content":"This page lists selected literature and online resources. Some are related to existing tutorial pages, while others are not. They are supposed to be of high interest to this site’s users.\nSeveral of the resources were added based on an inspiring talk by Julia Lowndes at the SAFRED conference, Brussels, 27 Feb 2018.\nOverviews  Wilson et al. (2017): set of good computing practices that every researcher can adopt British Ecological Society (2014): planning the data life cycle; creating, processing, documenting, preserving, sharing \u0026amp; reusing data Goudeseune et al. (2019): open data management, data management plan, repositories, standards and licenses Cooper \u0026amp; Hsing (2017): file organisation, workflow documentation, code reproducibility and readability, writing reproducible reports, version control and code archiving Marwick et al. (2018): the research compendium as a solution to share research in a reproducible way Ibanez et al. (2014): vision on reproducible science, routine practices, collaboration, literate computing  See also some resources related to learning and education and the discipline of open and reproducible science.\nSpecific tools  Ross et al. (2017): about tidyverse workflow and tools https://www.tidyverse.org: website of the tidyverse packages  Focus on version control workflows  Bryan (2017): rationale, workflows and tools regarding version control for project organization Bryan et al. (2019): getting started with git and github workflows in RStudio  Bibliography British Ecological Society (ed.) (2014). A guide to data management in ecology and evolution. BES Guides to Better Science. British Ecological Society, London.\n Bryan J. (2017). Excuse me, do you have a moment to talk about version control? PeerJ Preprints 5: e3159v2. https://doi.org/10.7287/peerj.preprints.3159v2.\n Bryan J., the STAT 545 TAs \u0026amp; Hester J. (2019). Happy Git and GitHub for the useR. https://happygitwithr.com/.\n Cooper N. \u0026amp; Hsing P.-Y. (eds.) (2017). A guide to reproducible code in ecology and evolution. BES Guides to Better Science. British Ecological Society, London.\n Goudeseune L., Le Roux X., Eggermont H., Bishop B., Bléry C., Brosens D., Coupremanne M., Davis R., Hautala H., Heughebaert A., Jacques C., Lee T., Rerig G. \u0026amp; Ungvári J. (2019). Guidance document for scientists on data management, open data, and the production of Data Management Plans. BiodivERsA report. Zenodo. https://doi.org/10.5281/zenodo.3448251.\n Ibanez L., Schroeder W.J. \u0026amp; Hanwell M.D. (2014). Practicing open science. In: Stodden V., Leisch F. \u0026amp; Peng R.D. (eds.). Implementing reproducible research. CRC Press, Boca Raton, FL.\n Marwick B., Boettiger C. \u0026amp; Mullen L. (2018). Packaging Data Analytical Work Reproducibly Using R (and Friends). The American Statistician 72 (1): 80–88. https://doi.org/10.1080/00031305.2017.1375986.\n Ross Z., Wickham H. \u0026amp; Robinson D. (2017). Declutter your R workflow with tidy tools. PeerJ Preprints 5: e3180v1. https://doi.org/10.7287/peerj.preprints.3180v1.\n Wilson G., Bryan J., Cranston K., Kitzes J., Nederbragt L. \u0026amp; Teal T.K. (2017). Good enough practices in scientific computing. PLOS Computational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510.\n  ","href":"/articles/computing/","title":"Scientific computing and data handling workflows"},{"content":"This page lists selected literature and online resources. Some are related to existing tutorial pages, while others are not. They are supposed to be of high interest to this site’s users.\nSeveral of the resources were added based on an inspiring talk by Julia Lowndes at the SAFRED conference, Brussels, 27 Feb 2018.\nThese resources focus on the discipline as a whole, its perception, principles, etc., while they often also give an overview of scientific computing workflows.\n Ibanez et al. (2014): open and reproducible science: vision, routine practices, collaboration, literate computing Hampton et al. (2015): workflows, tools, obstacles and needed mindshifts for open science Donati \u0026amp; Woolston (2017): how data science is becoming a large discipline  Focus on reproducible research  Stodden et al. (2014): book on computational reproducibility and (experiment) replicability; the three parts are Tools, Practices and Guidelines, Platforms Stodden \u0026amp; Miguez (2014): a formalized set of best practice recommendations for reproducible research Begley et al. (2015): current irreproducibility and good institutional practice ROpenSci Contributors (2018): a comprehensive overview of aspects and R-oriented tools in reproducible research  Bibliography Begley C.G., Buchan A.M. \u0026amp; Dirnagl U. (2015). Institutions must do their part for reproducibility. Nature 525 (7567): 25–27. https://doi.org/10.1038/525025a.\nDonati G. \u0026amp; Woolston C. (2017). Information management: Data domination. Nature 548 (7669): 613–614. https://doi.org/10.1038/nj7669-613a.\nHampton S.E., Anderson S.S., Bagby S.C., Gries C., Han X., Hart E.M., Jones M.B., Lenhardt W.C., MacDonald A., Michener W.K., Mudge J., Pourmokhtarian A., Schildhauer M.P., Woo K.H. \u0026amp; Zimmerman N. (2015). The Tao of open science for ecology. Ecosphere 6 (7): 1–13. https://doi.org/10.1890/ES14-00402.1.\nIbanez L., Schroeder W.J. \u0026amp; Hanwell M.D. (2014). Practicing open science. In: Stodden V., Leisch F. \u0026amp; Peng R.D. (eds.). Implementing reproducible research. CRC Press, Boca Raton, FL.\nROpenSci Contributors (2018). Reproducibility Guide [WWW document]. https://ropensci.github.io/reproducibility-guide/ (accessed September 22, 2020).\nStodden V., Leisch F. \u0026amp; Peng R.D. (2014). Implementing reproducible research. CRC Press, Boca Raton, FL.\nStodden V. \u0026amp; Miguez S. (2014). Best Practices for Computational Science: Software Infrastructure and Environments for Reproducible and Extensible Research. Journal of Open Research Software 2 (1). https://doi.org/10.5334/jors.ay.\n","href":"/articles/open_science/","title":"The discipline of open science"},{"content":"During the current transition period of supporting both old and new PROJ/GDAL, you may get a decent portion of proj4string-related warnings when running the below code, but you can safely ignore them. For more information, see the CRS tutorial.\nThis tutorial uses a few basic functions from the dplyr and raster packages. While only a few functions are used, you can use the previous hyperlinks to access the tutorials (vignettes) of these packages for more functions and information.\noptions(stringsAsFactors = FALSE) library(raster) library(tidyverse) library(inborutils) You will find a bit more background about ‘why and what’, regarding the considered open standards, in a separate post on this website.\nIn short, the GeoTIFF and GeoPackage formats are ideal for exchange, publication, interoperability \u0026amp; durability and to open science in general.\nThe below table compares a few raster formats that are currently used a lot. This tutorial focuses on the open formats.\n   Property GeoTIFF GeoPackage ESRI geodatabase     Open standard? yes yes no   Write support by GDAL yes yes no   Supported OS cross-platform cross-platform Windows   Extends non-spatial format: TIFF SQLite MS Access (for personal gdb)   Text or binary? binary binary binary   Number of files 1 1 1 (personal gdb) / many (file gdb)   Inspect version’s differences in git version control? no no no   Can store multiple layers? yes yes yes   Do layers need same extent and resolution? yes no no   Coordinate reference system (CRS) in file same as input CRS same as input CRS same as input CRS    How to make and use GeoTIFF files (*.tif) Making a mono-layered GeoTIFF file from a RasterLayer R object Let’s create a small dummy RasterLayer object from scratch, for some area in Belgium (using the CRS 1 Belgian Lambert 72, i.e. EPSG-code 31370):\nartwork \u0026lt;- raster(extent(188500, 190350, 227550, 229550), # xmin, xmax, ymin, ymax res = 50, # resolution of 50 meters crs = 31370) %\u0026gt;% setValues(runif(ncell(.))) # fill with random values What does this look like?\nartwork ## class : RasterLayer ## dimensions : 40, 37, 1480 (nrow, ncol, ncell) ## resolution : 50, 50 (x, y) ## extent : 188500, 190350, 227550, 229550 (xmin, xmax, ymin, ymax) ## crs : +proj=lcc +lat_0=90 +lon_0=4.36748666666667 +lat_1=51.1666672333333 +lat_2=49.8333339 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +units=m +no_defs ## source : memory ## names : layer ## values : 0.0001557397, 0.9997439 (min, max)  A simple trick to plot this raster:\nspplot(artwork) To write this RasterLayer object as a GeoTIFF, you can use the raster::writeRaster() function. In the background, it uses the GeoTIFF driver of the powerful GDAL library.\nartwork %\u0026gt;% writeRaster(\u0026#34;artwork.tif\u0026#34;) And now?\nSay HURRAY!!\nMaking a multi-layered GeoTIFF file from a RasterBrick R object Let’s create a RasterBrick object of three layers:\narts \u0026lt;- brick(artwork) # RasterBrick with one layer (the RasterLayer from above) arts[[2]] \u0026lt;- artwork + 10 # Add second layer, e.g. based on first one arts[[3]] \u0026lt;- calc(arts[[2]], function(x) {20 ^ x}) # Making third layer from second names(arts) \u0026lt;- paste0(\u0026#34;layer\u0026#34;, 1:3) Note: for complex formulas on large datasets, the calc() function is more efficient than simple algebraic expressions such as for layer2 (see ?raster::calc).\nHow does the result look like?\narts ## class : RasterBrick ## dimensions : 40, 37, 1480, 3 (nrow, ncol, ncell, nlayers) ## resolution : 50, 50 (x, y) ## extent : 188500, 190350, 227550, 229550 (xmin, xmax, ymin, ymax) ## crs : +proj=lcc +lat_0=90 +lon_0=4.36748666666667 +lat_1=51.1666672333333 +lat_2=49.8333339 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +units=m +no_defs ## source : memory ## names : layer1, layer2, layer3 ## min values : 1.557397e-04, 1.000016e+01, 1.024478e+13 ## max values : 9.997439e-01, 1.099974e+01, 2.046429e+14  arts %\u0026gt;% as.list %\u0026gt;% lapply(spplot) ## [[1]]  ## ## [[2]]  ## ## [[3]]  So now what?\nLet’s write it!\narts %\u0026gt;% writeRaster(\u0026#34;arts.tif\u0026#34;) But, I want to add 20 extra layers!\n(…😣😮…)\narts2 \u0026lt;- calc(artwork, function(x) {-1:-20 * x}, # first layer = -1 * artwork # second layer = -2 * artwork # .... forceapply = TRUE) names(arts2) \u0026lt;- paste0(\u0026#34;neg_layer\u0026#34;, 1:20) # adding it to arts: arts \u0026lt;- brick(list(arts, arts2)) # saving layer names for later use: mynames \u0026lt;- names(arts) nlayers(arts) ## [1] 23  names(arts) ## [1] \u0026quot;layer1\u0026quot; \u0026quot;layer2\u0026quot; \u0026quot;layer3\u0026quot; \u0026quot;neg_layer1\u0026quot; \u0026quot;neg_layer2\u0026quot; ## [6] \u0026quot;neg_layer3\u0026quot; \u0026quot;neg_layer4\u0026quot; \u0026quot;neg_layer5\u0026quot; \u0026quot;neg_layer6\u0026quot; \u0026quot;neg_layer7\u0026quot; ## [11] \u0026quot;neg_layer8\u0026quot; \u0026quot;neg_layer9\u0026quot; \u0026quot;neg_layer10\u0026quot; \u0026quot;neg_layer11\u0026quot; \u0026quot;neg_layer12\u0026quot; ## [16] \u0026quot;neg_layer13\u0026quot; \u0026quot;neg_layer14\u0026quot; \u0026quot;neg_layer15\u0026quot; \u0026quot;neg_layer16\u0026quot; \u0026quot;neg_layer17\u0026quot; ## [21] \u0026quot;neg_layer18\u0026quot; \u0026quot;neg_layer19\u0026quot; \u0026quot;neg_layer20\u0026quot;  Overwrite the earlier written file:\narts %\u0026gt;% writeRaster(\u0026#34;arts.tif\u0026#34;, overwrite = TRUE) That’s about it!\nReading a GeoTIFF file Nothing can be more simple…\nReading a mono-layered GeoTIFF file with raster() gives back the RasterLayer:\nartwork_test \u0026lt;- raster(\u0026#34;artwork.tif\u0026#34;) artwork_test ## class : RasterLayer ## dimensions : 40, 37, 1480 (nrow, ncol, ncell) ## resolution : 50, 50 (x, y) ## extent : 188500, 190350, 227550, 229550 (xmin, xmax, ymin, ymax) ## crs : +proj=lcc +lat_0=90 +lon_0=4.36748666666667 +lat_1=51.1666672333333 +lat_2=49.8333339 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +units=m +no_defs ## source : artwork.tif ## names : artwork ## values : 0.0001557397, 0.9997439 (min, max)  Reading a multi-layered GeoTIFF file with brick() returns the RasterBrick:\narts_test \u0026lt;- brick(\u0026#34;arts.tif\u0026#34;) However:\nnames(arts_test) ## [1] \u0026quot;arts.1\u0026quot; \u0026quot;arts.2\u0026quot; \u0026quot;arts.3\u0026quot; \u0026quot;arts.4\u0026quot; \u0026quot;arts.5\u0026quot; \u0026quot;arts.6\u0026quot; \u0026quot;arts.7\u0026quot; ## [8] \u0026quot;arts.8\u0026quot; \u0026quot;arts.9\u0026quot; \u0026quot;arts.10\u0026quot; \u0026quot;arts.11\u0026quot; \u0026quot;arts.12\u0026quot; \u0026quot;arts.13\u0026quot; \u0026quot;arts.14\u0026quot; ## [15] \u0026quot;arts.15\u0026quot; \u0026quot;arts.16\u0026quot; \u0026quot;arts.17\u0026quot; \u0026quot;arts.18\u0026quot; \u0026quot;arts.19\u0026quot; \u0026quot;arts.20\u0026quot; \u0026quot;arts.21\u0026quot; ## [22] \u0026quot;arts.22\u0026quot; \u0026quot;arts.23\u0026quot;  As you see, layer names are not saved in the GeoTIFF. You define them in R:\nnames(arts_test) \u0026lt;- mynames arts_test ## class : RasterBrick ## dimensions : 40, 37, 1480, 23 (nrow, ncol, ncell, nlayers) ## resolution : 50, 50 (x, y) ## extent : 188500, 190350, 227550, 229550 (xmin, xmax, ymin, ymax) ## crs : +proj=lcc +lat_0=90 +lon_0=4.36748666666667 +lat_1=51.1666672333333 +lat_2=49.8333339 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +units=m +no_defs ## source : arts.tif ## names : layer1, layer2, layer3, neg_layer1, neg_layer2, neg_layer3, neg_layer4, neg_layer5, neg_layer6, neg_layer7, neg_layer8, neg_layer9, neg_layer10, neg_layer11, neg_layer12, ... ## min values : 1.557397e-04, 1.000016e+01, 1.024478e+13, -9.997439e-01, -1.999488e+00, -2.999232e+00, -3.998976e+00, -4.998719e+00, -5.998463e+00, -6.998207e+00, -7.997951e+00, -8.997695e+00, -9.997439e+00, -1.099718e+01, -1.199693e+01, ... ## max values : 9.997439e-01, 1.099974e+01, 2.046429e+14, -1.557397e-04, -3.114794e-04, -4.672192e-04, -6.229589e-04, -7.786986e-04, -9.344383e-04, -1.090178e-03, -1.245918e-03, -1.401657e-03, -1.557397e-03, -1.713137e-03, -1.868877e-03, ...  That’s what we wanted!\nThe actual data are not loaded into memory, but read in chunks when performing operations. This makes it convenient when using larger rasters:\ninMemory(arts_test) ## [1] FALSE  Selecting a specific layer by its name:\narts_test$neg_layer20 ## class : RasterLayer ## band : 23 (of 23 bands) ## dimensions : 40, 37, 1480 (nrow, ncol, ncell) ## resolution : 50, 50 (x, y) ## extent : 188500, 190350, 227550, 229550 (xmin, xmax, ymin, ymax) ## crs : +proj=lcc +lat_0=90 +lon_0=4.36748666666667 +lat_1=51.1666672333333 +lat_2=49.8333339 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +units=m +no_defs ## source : arts.tif ## names : neg_layer20 ## values : -19.99488, -0.003114794 (min, max)  How to make and use GeoPackages with raster layers (*.gpkg) ‘GeoPackage’ may sound new and unfamiliar to you – more information can be found in a separate post on this website.\nWhile its vector capabilities are already beautifully supported by GDAL and the sf package (demonstrated in the other tutorial)), its raster capabilities are still less supported by GDAL and dependent applications such as the R-packages raster and stars. This is something we can expect to grow in the future.\nGDAL’s GPKG-raster driver itself is still less worked out than its drivers for GeoTIFF or GPKG-vector (note that one GPKG file can accommodate both layer types). For example, only the Byte, Int16, UInt16 and Float32 datatypes can be written by GDAL, while for GeoTIFFs these are Byte UInt16, Int16, UInt32, Int32, Float32, Float64, CInt16, CInt32, CFloat32 and CFloat64 2.\nFrom my experience, raster GeoPackage files are smaller than GeoTIFF files in the case of larger rasters. This, and the capability to combine raster and vector layers, certainly make it worthwile to consider the GeoPackage format for rasters, if you’re not hindered by the supported data types.\nMaking a single-raster GeoPackage from a RasterLayer R object This it is no more difficult than:\nartwork %\u0026gt;% writeRaster(\u0026#34;artwork.gpkg\u0026#34;, format = \u0026#34;GPKG\u0026#34;) ## Warning in .gd_SetNoDataValue(object, ...): setting of missing value not ## supported by this driver  A bit more information on the ‘missing value’ warning can be found in GDAL’s documentation of GeoPackage raster. You should know that the raster package does not yet officially support the GeoPackage! (see ?writeFormats())\nHowever, the stars package (see further)) fully supports GDAL’s capabilities, and therefore is able to write multiple raster layers, as we will do in a minute. Anyway, raster::writeRaster already works fine for single RasterLayer objects.\nReading the GeoPackage:\nartwork_gpkg \u0026lt;- raster(\u0026#34;artwork.gpkg\u0026#34;) artwork_gpkg ## class : RasterLayer ## dimensions : 40, 37, 1480 (nrow, ncol, ncell) ## resolution : 50, 50 (x, y) ## extent : 188500, 190350, 227550, 229550 (xmin, xmax, ymin, ymax) ## crs : +proj=lcc +lat_0=90 +lon_0=4.36748666666667 +lat_1=51.1666672333333 +lat_2=49.8333339 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +units=m +no_defs ## source : artwork.gpkg ## names : artwork ## values : 0.0001557397, 0.9997439 (min, max)  Let’s make sure: are the data we’ve read from the GeoTIFF identical to those from the GeoPackage?\nall.equal(artwork_test[], artwork_gpkg[]) ## [1] TRUE  Yeah!\nGiven that the GPKG-support of raster is limited, we’re lucky that Edzer Pebesma – the creator of sf – has also made the amazing package stars!!\nunlink(\u0026#34;artwork.gpkg\u0026#34;) # delete gpkg; we\u0026#39;re going to create it here again Sys.setenv(GDAL_PAM_ENABLED = \u0026#34;NO\u0026#34;) # prevents an auxiliary file being written next to *.gpkg library(stars) ## Loading required package: abind ## Loading required package: sf ## Linking to GEOS 3.10.1, GDAL 3.4.0, PROJ 8.2.0; sf_use_s2() is TRUE  We could as well have written artwork to a GeoPackage with stars, so let’s just see what we get by converting the RasterLayer object to a stars object and then apply write_stars(), hm?\nartwork %\u0026gt;% st_as_stars %\u0026gt;% # this converts the RasterLayer to a stars object write_stars(\u0026#34;artwork.gpkg\u0026#34;, driver = \u0026#34;GPKG\u0026#34;) Reading it back with stars::read_stars(), followed by back-conversion to a RasterLayer:\nartwork_gpkg_stars \u0026lt;- read_stars(\u0026#34;artwork.gpkg\u0026#34;) %\u0026gt;% as(\u0026#34;Raster\u0026#34;) artwork_gpkg_stars ## class : RasterLayer ## dimensions : 40, 37, 1480 (nrow, ncol, ncell) ## resolution : 50, 50 (x, y) ## extent : 188500, 190350, 227550, 229550 (xmin, xmax, ymin, ymax) ## crs : +proj=lcc +lat_0=90 +lon_0=4.36748666666667 +lat_1=51.1666672333333 +lat_2=49.8333339 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +units=m +no_defs ## source : memory ## names : layer ## values : 0.0001557397, 0.9997439 (min, max)  And yes again, the data we’ve read from the GeoTIFF file are identical to those from the GeoPackage:\nall.equal(artwork_test[], artwork_gpkg_stars[]) ## [1] TRUE  That’s it!\nKnowing how to write and read with stars will help us for the multi-layer case!\nMaking a multi-raster GeoPackage Indeed, just as with vector layers, GeoPackage can accommodate multiple raster layers (or vector + raster layers).\nLet’s suppose we’d like to add layer2 (a RasterLayer) from the RasterBrick object arts.\narts$layer2 ## class : RasterLayer ## dimensions : 40, 37, 1480 (nrow, ncol, ncell) ## resolution : 50, 50 (x, y) ## extent : 188500, 190350, 227550, 229550 (xmin, xmax, ymin, ymax) ## crs : +proj=lcc +lat_0=90 +lon_0=4.36748666666667 +lat_1=51.1666672333333 +lat_2=49.8333339 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +units=m +no_defs ## source : memory ## names : layer2 ## values : 10.00016, 10.99974 (min, max)  Unfortunately, the raster package does not support GDAL’s options to add extra raster layers in a GPKG file:\ntry( arts$layer2 %\u0026gt;% writeRaster(\u0026#34;artwork.gpkg\u0026#34;, format = \u0026#34;GPKG\u0026#34;, options = c(\u0026#34;RASTER_TABLE=layer2\u0026#34;, \u0026#34;APPEND_SUBDATASET=YES\u0026#34;)) ) ## Error in .getGDALtransient(x, filename = filename, options = options, : ## filename exists; use overwrite=TRUE  So let’s proceed with stars!\narts$layer2 %\u0026gt;% st_as_stars %\u0026gt;% write_stars(\u0026#34;artwork.gpkg\u0026#34;, driver = \u0026#34;GPKG\u0026#34;, options = c(\u0026#34;RASTER_TABLE=layer2\u0026#34;, \u0026#34;APPEND_SUBDATASET=YES\u0026#34;)) Mind the options argument: those options are passed directly to GDAL’s GPKG-raster driver, and they’re documented at GDAL.\nOver there we read:\n RASTER_TABLE=string. Name of tile user table. By default, based on the filename (i.e. if filename is foo.gpkg, the table will be called “foo”).\nAPPEND_SUBDATASET=YES/NO: If set to YES, an existing GeoPackage will not be priorly destroyed, such as to be able to add new content to it. Defaults to NO.\n Ahaa!\nWe got no errors above, but no feedback either…\nThrilling!\nLet’s peek:\ngdalUtils::gdalinfo(\u0026#34;artwork.gpkg\u0026#34;) %\u0026gt;% cat(sep = \u0026#34;\\n\u0026#34;) ## Driver: GPKG/GeoPackage ## Files: artwork.gpkg ## Size is 512, 512 ## Subdatasets: ## SUBDATASET_1_NAME=GPKG:artwork.gpkg:artwork ## SUBDATASET_1_DESC=artwork - artwork ## SUBDATASET_2_NAME=GPKG:artwork.gpkg:layer2 ## SUBDATASET_2_DESC=layer2 - layer2 ## Corner Coordinates: ## Upper Left ( 0.0, 0.0) ## Lower Left ( 0.0, 512.0) ## Upper Right ( 512.0, 0.0) ## Lower Right ( 512.0, 512.0) ## Center ( 256.0, 256.0)  Yay!\nIt’s interesting to see how the info at this level disregards CRS and extent.\nWhen we query the metadata of one sublayer, it is seen that CRS and extent are layer-specific:\ngdalUtils::gdalinfo(\u0026#34;artwork.gpkg\u0026#34;, # provide metadata of first subdataset: sd=1, # the following arguments just control formatting of the output: approx_stats = TRUE, mm = TRUE, proj4 = TRUE) %\u0026gt;% cat(sep = \u0026#34;\\n\u0026#34;) ## Driver: GPKG/GeoPackage ## Files: none associated ## Size is 37, 40 ## Coordinate System is: ## PROJCRS[\u0026quot;BD72 / Belgian Lambert 72\u0026quot;, ## BASEGEOGCRS[\u0026quot;BD72\u0026quot;, ## DATUM[\u0026quot;Reseau National Belge 1972\u0026quot;, ## ELLIPSOID[\u0026quot;International 1924\u0026quot;,6378388,297, ## LENGTHUNIT[\u0026quot;metre\u0026quot;,1]]], ## PRIMEM[\u0026quot;Greenwich\u0026quot;,0, ## ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433]], ## ID[\u0026quot;EPSG\u0026quot;,4313]], ## CONVERSION[\u0026quot;Belgian Lambert 72\u0026quot;, ## METHOD[\u0026quot;Lambert Conic Conformal (2SP)\u0026quot;, ## ID[\u0026quot;EPSG\u0026quot;,9802]], ## PARAMETER[\u0026quot;Latitude of false origin\u0026quot;,90, ## ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433], ## ID[\u0026quot;EPSG\u0026quot;,8821]], ## PARAMETER[\u0026quot;Longitude of false origin\u0026quot;,4.36748666666667, ## ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433], ## ID[\u0026quot;EPSG\u0026quot;,8822]], ## PARAMETER[\u0026quot;Latitude of 1st standard parallel\u0026quot;,51.1666672333333, ## ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433], ## ID[\u0026quot;EPSG\u0026quot;,8823]], ## PARAMETER[\u0026quot;Latitude of 2nd standard parallel\u0026quot;,49.8333339, ## ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433], ## ID[\u0026quot;EPSG\u0026quot;,8824]], ## PARAMETER[\u0026quot;Easting at false origin\u0026quot;,150000.013, ## LENGTHUNIT[\u0026quot;metre\u0026quot;,1], ## ID[\u0026quot;EPSG\u0026quot;,8826]], ## PARAMETER[\u0026quot;Northing at false origin\u0026quot;,5400088.438, ## LENGTHUNIT[\u0026quot;metre\u0026quot;,1], ## ID[\u0026quot;EPSG\u0026quot;,8827]]], ## CS[Cartesian,2], ## AXIS[\u0026quot;easting (X)\u0026quot;,east, ## ORDER[1], ## LENGTHUNIT[\u0026quot;metre\u0026quot;,1]], ## AXIS[\u0026quot;northing (Y)\u0026quot;,north, ## ORDER[2], ## LENGTHUNIT[\u0026quot;metre\u0026quot;,1]], ## USAGE[ ## SCOPE[\u0026quot;Engineering survey, topographic mapping.\u0026quot;], ## AREA[\u0026quot;Belgium - onshore.\u0026quot;], ## BBOX[49.5,2.5,51.51,6.4]], ## ID[\u0026quot;EPSG\u0026quot;,31370]] ## Data axis to CRS axis mapping: 1,2 ## PROJ.4 string is: ## '+proj=lcc +lat_0=90 +lon_0=4.36748666666667 +lat_1=51.1666672333333 +lat_2=49.8333339 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +units=m +no_defs' ## Origin = (188500.000000000000000,229550.000000000000000) ## Pixel Size = (50.000000000000000,-50.000000000000000) ## Metadata: ## AREA_OR_POINT=Point ## IDENTIFIER=artwork ## ZOOM_LEVEL=0 ## Image Structure Metadata: ## INTERLEAVE=PIXEL ## Corner Coordinates: ## Upper Left ( 188500.000, 229550.000) ( 4d55'13.29\u0026quot;E, 51d22'29.75\u0026quot;N) ## Lower Left ( 188500.000, 227550.000) ( 4d55'12.52\u0026quot;E, 51d21'25.04\u0026quot;N) ## Upper Right ( 190350.000, 229550.000) ( 4d56'48.92\u0026quot;E, 51d22'29.30\u0026quot;N) ## Lower Right ( 190350.000, 227550.000) ( 4d56'48.12\u0026quot;E, 51d21'24.59\u0026quot;N) ## Center ( 189425.000, 228550.000) ( 4d56' 0.71\u0026quot;E, 51d21'57.17\u0026quot;N) ## Band 1 Block=256x256 Type=Float32, ColorInterp=Undefined ## Description = Height ## Computed Min/Max=0.000,1.000 ## Minimum=0.000, Maximum=1.000, Mean=0.502, StdDev=0.289 ## Metadata: ## STATISTICS_MAXIMUM=0.9997438788414 ## STATISTICS_MEAN=0.50228321918167 ## STATISTICS_MINIMUM=0.00015573971904814 ## STATISTICS_STDDEV=0.28903126880165 ## STATISTICS_VALID_PERCENT=100  raster will not help us for reading the layers. But read_stars() is there to assist us!!\n# brick(\u0026#34;artwork.gpkg\u0026#34;) ## this won\u0026#39;t work... # but this will work: artwork_gpkg2 \u0026lt;- read_stars(\u0026#34;artwork.gpkg\u0026#34;, sub = \u0026#34;artwork\u0026#34;, quiet = TRUE) %\u0026gt;% as(\u0026#34;Raster\u0026#34;) artwork_gpkg2 ## class : RasterLayer ## dimensions : 40, 37, 1480 (nrow, ncol, ncell) ## resolution : 50, 50 (x, y) ## extent : 188500, 190350, 227550, 229550 (xmin, xmax, ymin, ymax) ## crs : +proj=lcc +lat_0=90 +lon_0=4.36748666666667 +lat_1=51.1666672333333 +lat_2=49.8333339 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +units=m +no_defs ## source : memory ## names : layer ## values : 0.0001557397, 0.9997439 (min, max)  Wow!\nChecking data again with GeoTIFF result:\nall.equal(artwork_test[], artwork_gpkg2[]) ## [1] TRUE  Same story for the other layer:\nread_stars(\u0026#34;artwork.gpkg\u0026#34;, sub = \u0026#34;layer2\u0026#34;, quiet = TRUE) %\u0026gt;% as(\u0026#34;Raster\u0026#34;) ## class : RasterLayer ## dimensions : 40, 37, 1480 (nrow, ncol, ncell) ## resolution : 50, 50 (x, y) ## extent : 188500, 190350, 227550, 229550 (xmin, xmax, ymin, ymax) ## crs : +proj=lcc +lat_0=90 +lon_0=4.36748666666667 +lat_1=51.1666672333333 +lat_2=49.8333339 +x_0=150000.013 +y_0=5400088.438 +ellps=intl +units=m +no_defs ## source : memory ## names : layer ## values : 10.00016, 10.99974 (min, max)  Splendid.\nBy the way, this is how the full stars object looks like – it holds information similar to a RasterBrick:\nread_stars(\u0026#34;artwork.gpkg\u0026#34;, quiet = TRUE) ## stars object with 2 dimensions and 2 attributes ## attribute(s): ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## artwork 1.557397e-04 0.2475555 0.5052805 0.5022832 0.7533531 0.9997439 ## layer2 1.000016e+01 10.2475555 10.5052805 10.5022832 10.7533531 10.9997435 ## dimension(s): ## from to offset delta refsys point values x/y ## x 1 37 188500 50 BD72 / Belgian Lambert 72 TRUE NULL [x] ## y 1 40 229550 -50 BD72 / Belgian Lambert 72 TRUE NULL [y]  Homework: further explore the amazing stars package Enter deep hyperspace and explore the stars package, which stores multidimensional hypercubes… Really, visit its website and never look (or turn?) back!\nlibrary(stars) For now, my time’s up and I’ll just demonstrate how easy it is to transform a Raster* object into a stars object:\ninterstellar \u0026lt;- arts[[1:5]] %\u0026gt;% st_as_stars() interstellar ## stars object with 3 dimensions and 1 attribute ## attribute(s): ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## layer1 -1.999488 -0.5021041 0.5052805 1.305765e+13 10.75335 2.046429e+14 ## dimension(s): ## from to offset delta refsys point values ## x 1 37 188500 50 BD72 / Belgian Lambert 72 NA NULL ## y 1 40 229550 -50 BD72 / Belgian Lambert 72 NA NULL ## band 1 5 NA NA NA NA layer1,...,neg_layer2 ## x/y ## x [x] ## y [y] ## band  It does make sense, right?\nWhat about:\ninterstellar %\u0026gt;% split(\u0026#34;band\u0026#34;) ## stars object with 2 dimensions and 5 attributes ## attribute(s): ## Min. 1st Qu. Median Mean ## layer1 1.557397e-04 2.475555e-01 5.052805e-01 5.022832e-01 ## layer2 1.000016e+01 1.024756e+01 1.050528e+01 1.050228e+01 ## layer3 1.024478e+13 2.149703e+13 4.652487e+13 6.528827e+13 ## neg_layer1 -9.997439e-01 -7.533531e-01 -5.052805e-01 -5.022832e-01 ## neg_layer2 -1.999488e+00 -1.506706e+00 -1.010561e+00 -1.004566e+00 ## 3rd Qu. Max. ## layer1 7.533531e-01 9.997439e-01 ## layer2 1.075335e+01 1.099974e+01 ## layer3 9.782164e+13 2.046429e+14 ## neg_layer1 -2.475555e-01 -1.557397e-04 ## neg_layer2 -4.951110e-01 -3.114794e-04 ## dimension(s): ## from to offset delta refsys point values x/y ## x 1 37 188500 50 BD72 / Belgian Lambert 72 NA NULL [x] ## y 1 40 229550 -50 BD72 / Belgian Lambert 72 NA NULL [y]  The stars package has a number of efficient geospatial algorithms that make it worth using, even for simple raster layers!\nAnd sure, as seen above, you can read from files with read_stars(), write to files with write_stars(), convert to Raster* objects with as(\u0026quot;Raster\u0026quot;) and backconvert with st_as_stars()!\n  CRS = coordinate reference system\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n See the GDAL datatype definitions – note that raster uses its own abbreviations: ?raster::dataType\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","href":"/tutorials/spatial_standards_raster/","title":"How to use open raster file formats in R: GeoTIFF \u0026 GeoPackage"},{"content":"General note: migration to the more actively developed sf package is currently advised by the sp maintainer. The sp package, used in this tutorial, is still maintained in order to support the newest versions of the GDAL and PROJ backends.\nSometimes we have a layer in one coordinate reference system (CRS) and need to transform it into another coordinate reference system. The first thing we need to do is identifying both coordinate reference systems. Let’s create an example and identify the coordinate reference system with wkt(). We used the coordinates posted on the contact page of NGI.\nlibrary(sp) library(leaflet) library(widgetframe) ## Loading required package: htmlwidgets  ngi \u0026lt;- data.frame(x = 650381.78, y = 667603.12) coordinates(ngi) \u0026lt;- ~x + y wkt(ngi) # show the CRS of an sp object ## Warning in wkt(ngi): CRS object has no comment ## NULL  NULL indicates that the coordinate reference system isn’t set. So we need to set it manually. In this case we know it is “Lambert 2008”. We need to know the related ‘WKT2 string’. The WKT2 string (well known text) is a recent open standard by the Open Geospatial Consortium to represent a coordinate reference system (CRS) - and it replaces the older (deprecated) PROJ.4 string.\nMost coordinate reference systems have an EPSG code which you can find at http://epsg.io/. The EPSG code for “Lambert 2008” is 3812. Let’s set this coordinate reference system to our dataset. CRS() defines the coordinate reference system.\nproj4string(ngi) \u0026lt;- CRS(SRS_string = \u0026#34;EPSG:3812\u0026#34;) ## Warning in showSRID(SRS_string, format = \u0026quot;PROJ\u0026quot;, multiline = \u0026quot;NO\u0026quot;, prefer_proj ## = prefer_proj): Discarded datum European Terrestrial Reference System 1989 in ## Proj4 definition  The proj4string() function refers to the older PROJ.4 strings to represent a CRS, which the sp package can still return for backward compatibility. The warning above demonstrates that some PROJ.4 information is not supported anymore. sp now uses the much better WKT2 representation, which it will derive directly from the EPSG-code that we provided (even though the function to assign a CRS to an object is still called proj4string()). The WKT2 string is what the geospatial GDAL and PROJ libraries (the background workhorses) work with since PROJ \u0026gt;= 6, and it looks like this:\nwkt_lambert2008 \u0026lt;- wkt(ngi) cat(wkt_lambert2008) ## PROJCRS[\u0026quot;ETRS89 / Belgian Lambert 2008\u0026quot;, ## BASEGEOGCRS[\u0026quot;ETRS89\u0026quot;, ## DATUM[\u0026quot;European Terrestrial Reference System 1989\u0026quot;, ## ELLIPSOID[\u0026quot;GRS 1980\u0026quot;,6378137,298.257222101, ## LENGTHUNIT[\u0026quot;metre\u0026quot;,1]]], ## PRIMEM[\u0026quot;Greenwich\u0026quot;,0, ## ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433]], ## ID[\u0026quot;EPSG\u0026quot;,4258]], ## CONVERSION[\u0026quot;Belgian Lambert 2008\u0026quot;, ## METHOD[\u0026quot;Lambert Conic Conformal (2SP)\u0026quot;, ## ID[\u0026quot;EPSG\u0026quot;,9802]], ## PARAMETER[\u0026quot;Latitude of false origin\u0026quot;,50.797815, ## ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433], ## ID[\u0026quot;EPSG\u0026quot;,8821]], ## PARAMETER[\u0026quot;Longitude of false origin\u0026quot;,4.35921583333333, ## ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433], ## ID[\u0026quot;EPSG\u0026quot;,8822]], ## PARAMETER[\u0026quot;Latitude of 1st standard parallel\u0026quot;,49.8333333333333, ## ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433], ## ID[\u0026quot;EPSG\u0026quot;,8823]], ## PARAMETER[\u0026quot;Latitude of 2nd standard parallel\u0026quot;,51.1666666666667, ## ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433], ## ID[\u0026quot;EPSG\u0026quot;,8824]], ## PARAMETER[\u0026quot;Easting at false origin\u0026quot;,649328, ## LENGTHUNIT[\u0026quot;metre\u0026quot;,1], ## ID[\u0026quot;EPSG\u0026quot;,8826]], ## PARAMETER[\u0026quot;Northing at false origin\u0026quot;,665262, ## LENGTHUNIT[\u0026quot;metre\u0026quot;,1], ## ID[\u0026quot;EPSG\u0026quot;,8827]]], ## CS[Cartesian,2], ## AXIS[\u0026quot;easting (X)\u0026quot;,east, ## ORDER[1], ## LENGTHUNIT[\u0026quot;metre\u0026quot;,1]], ## AXIS[\u0026quot;northing (Y)\u0026quot;,north, ## ORDER[2], ## LENGTHUNIT[\u0026quot;metre\u0026quot;,1]], ## USAGE[ ## SCOPE[\u0026quot;Engineering survey, topographic mapping.\u0026quot;], ## AREA[\u0026quot;Belgium - onshore.\u0026quot;], ## BBOX[49.5,2.5,51.51,6.4]], ## ID[\u0026quot;EPSG\u0026quot;,3812]]  We could verify the correctness of this position by plotting it on a map. Here we use the leaflet package which requires the data to be in the “WGS84” coordinate reference system. Therefore we use spTransform() to do this transformation. “WGS84” has EPSG code 4326.\nngi_ll \u0026lt;- spTransform(ngi, CRS(SRS_string = \u0026#34;EPSG:4326\u0026#34;)) cat(wkt(ngi_ll)) ## GEOGCRS[\u0026quot;WGS 84 (with axis order normalized for visualization)\u0026quot;, ## DATUM[\u0026quot;World Geodetic System 1984\u0026quot;, ## ELLIPSOID[\u0026quot;WGS 84\u0026quot;,6378137,298.257223563, ## LENGTHUNIT[\u0026quot;metre\u0026quot;,1]], ## ID[\u0026quot;EPSG\u0026quot;,6326]], ## PRIMEM[\u0026quot;Greenwich\u0026quot;,0, ## ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433], ## ID[\u0026quot;EPSG\u0026quot;,8901]], ## CS[ellipsoidal,2], ## AXIS[\u0026quot;geodetic longitude (Lon)\u0026quot;,east, ## ORDER[1], ## ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433, ## ID[\u0026quot;EPSG\u0026quot;,9122]]], ## AXIS[\u0026quot;geodetic latitude (Lat)\u0026quot;,north, ## ORDER[2], ## ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433, ## ID[\u0026quot;EPSG\u0026quot;,9122]]]]  leaflet(ngi_ll) %\u0026gt;% addTiles() %\u0026gt;% addMarkers() # %\u0026gt;% #frameWidget() Note: run the code to see the interactive map.\n   CRS EPSG     WGS 84 4326   Belge 1972 / Belgian Lambert 72 31370   ETRS89 / Belgian Lambert 2008 3812   WGS 84 / Pseudo-Mercator 3857    Relevant projections for Belgian data. CRS = Coordinate reference system, EPSG = CRS code in the EPSG dataset.\n","href":"/tutorials/spatial_transform_crs/","title":"Transforming spatial objects"},{"content":"","href":"/authors/aaikedewever/","title":"aaikedewever"},{"content":"","href":"/tags/database/","title":"database"},{"content":"","href":"/authors/joloos/","title":"joloos"},{"content":"Introduction (shamelessly taken from wikipedia)\nKerberos is a computer network authentication protocol that works on the basis of tickets to allow nodes communicating over a non-secure network to prove their identity to one another in a secure manner.\nWindows 2000 and later uses Kerberos as its default authentication method. Many UNIX and UNIX-like operating systems, including FreeBSD, Apple\u0026rsquo;s Mac OS X, Red Hat Enterprise Linux, Oracle\u0026rsquo;s Solaris, IBM\u0026rsquo;s AIX and Z/OS, HP\u0026rsquo;s HP-UX and OpenVMS and others, include software for Kerberos authentication of users or services.\nHence, we can use the protocol to have an OS independent solution for authentication across different databases. In this document, the installation and configuration for linux/mac users is provided as well as an introduction to the usage of the authentication service to connect to databases. For windows users (in the domain) the authentication is provided by default.\nInstallation Kerberos client For debian/ubuntu users (make sure you belong to the sudo group):\nsudo apt-get install krb5-user sudo apt-get install openssl # if not yet available on your system (it probably is) During installation, you may be asked for extra configuration input. To answer that, see next section: Configure Kerberos client.\nNote for Mac users: Heimdal Kerberos is installed by default, so there is no need to install a client. Mac users can jump to the paragraph \u0026ldquo;Next, adapt the krb5.conf\u0026rdquo;\nConfigure Kerberos client (again, the commands assume root privileges)\nStart with the Kerberos configuration dialogue:\ndpkg-reconfigure krb5-config Use INBO.BE as the realm (this is the realm of the kerberos servers):\nMake sure to use DNS to find these servers, so choose \u0026lsquo;NO\u0026rsquo; if you get the below question:\nNext, adapt the krb5.conf, probably available in the /etc directory. Add the following sections with configurations to the file:\n[realms] INBO.BE = { kdc = DNS_Name_DomainController1.domain.be kdc = DNS_Name_DomainController2.domain.be kdc = DNS_Name_DomainController3.domain.be kdc = DNS_Name_DomainController4.domain.be kdc = DNS_Name_DomainController5.domain.be default_domain = domain.be } [logging] default = FILE:/var/log/krblibs.log kdc = FILE:/var/log/krbkdc.log admin_server = FILE:/var/log/kadmind.log [libdefaults] default_realm = DOMAIN.BE dns_lookup_realm = false dns_lookup_kdc = false ticket_lifetime = 24h renew_lifetime = 7d forwardable= true INBO staff can download a preconfigured krb5.conf file here: https://drive.google.com/a/inbo.be/file/d/1q4MOWl3i-DDy1s3vwOeqPkpToa1S-3zE/view?usp=sharing.\nNote for Mac users:\n When using the preconfigured krb5.conf file, also comment out (add #) the lines following # The following krb5.conf variables are only for MIT Kerberos. You can skip the paragraph \u0026ldquo;Time synchronisation\u0026rdquo; and jump to the next section: MS SQL Server ODBC driver and tools.  Time synchronization This is needed in order to sync the timing of the domain controller server and client side.\nCheck whether the systemd-timesyncd daemon is already active on your system:\n$ systemctl status time-sync.target ● time-sync.target - System Time Synchronized Loaded: loaded (/lib/systemd/system/time-sync.target; static; vendor preset: disabled) Active: active since Mon 2020-09-07 08:27:03 CEST; 59min ago Docs: man:systemd.special(7) $ timedatectl status Local time: ma 2020-09-07 09:27:00 CEST Universal time: ma 2020-09-07 07:27:00 UTC RTC time: ma 2020-09-07 07:27:00 Time zone: Europe/Brussels (CEST, +0200) System clock synchronized: yes NTP service: active RTC in local TZ: no Note the line NTP service: active.\nIf you do have the systemd-timesyncd package but the unit is not active, run systemctl enable --now time-sync.target. Further information can be found here.\nOnly if you don\u0026rsquo;t have the systemd-timesyncd package, install ntp instead:\nsudo apt-get install ntp If you installed ntp, check whether the following two files do exist:\n /etc/ntp.conf /etc/ntp.conf.dhcp (empty file, just make sure there is a file)  MS SQL Server ODBC driver and tools As most of the databases at INBO are SQL Server, an appropriate driver and the command line toolset is required to fully support database connections to SQL Server.\nApart from the ODBC driver, we will also install following tools:\n sqlcmd: Command-line query utility. bcp: Bulk import-export utility.  For Linux, follow these installation instructions.1 For Mac, installation instructions can be found here.2\nAlso follow the \u0026lsquo;optional\u0026rsquo; instructions, as these will install the tools.\nHence, for Ubuntu 20.04 or Linux Mint 20 you would do:\nsudo su curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add - #Ubuntu 20.04 curl https://packages.microsoft.com/config/ubuntu/20.04/prod.list \u0026gt; /etc/apt/sources.list.d/mssql-release.list exit sudo apt-get update sudo ACCEPT_EULA=Y apt-get install msodbcsql17 mssql-tools echo \u0026#39;export PATH=\u0026#34;$PATH:/opt/mssql-tools/bin\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bash_profile echo \u0026#39;export PATH=\u0026#34;$PATH:/opt/mssql-tools/bin\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc sudo apt-get install unixodbc-dev On Ubuntu 20.04, if installing msodbcsql17 and mssql-tools fails because of incompatible unixodbc version, you can first try apt install unixodbc=2.3.6-0.1build1 unixodbc-dev=2.3.6-0.1build1.\nTest installation Kerberos ticket system To check if the Kerberos configuration is successful, ask for a ticket by initiating with kinit:\nkinit your_user_name If no errors are prodused, check the existing tickets with klist:\nklist This should produce a list of successfully granted tickets, so something similar as:\nValid starting Expires Service principal 03/01/18 15:42:08 04/01/18 01:42:08 krbtgt/INBO.BE@INBO.BE renew until 10/01/18 15:42:08 For Mac users, note that you can also use the Ticketviewer application which can be found in /System/Library/CoreServices/Applications. Click Add Identity and enter your_user_name@INBO.BE and password.\nSQL database connections When the ticketing is working, the next step is to use the authentication to connect to the databases itself. To test this, we\u0026rsquo;ll use the sqlcmd command line tool. In a next section, we\u0026rsquo;ll focus on the ODBC settings.\nTesting with sqlcmd (make sure you have an active ticket). Type quit to exit.\nInbo staff can consult a list of connection strings ( including server names ) for a server to query link\nsqlcmd -S DBServerName -E 1\u0026gt; Select top 10 name from sys.databases; 2\u0026gt; Go On Ubuntu 20.04, you may get an error:\nSqlcmd: Error: Microsoft ODBC Driver 17 for SQL Server : TCP Provider: Error code 0x2746. Sqlcmd: Error: Microsoft ODBC Driver 17 for SQL Server : Client unable to establish connection. If you are in that case, have a look at working solutions in this GitHub issue.\nSQL ODBC connections To support database connections from other applications (e.g. GUI environments, but also R, Python,\u0026hellip;), the configuration of database drivers and connections should be provided in the /etc/odbc.ini and /etc/odbcinst.ini.\nMake sure the ODBC driver for SQL Server is available with a recognizable name in the /etc/odbcinst.ini file:\n[ODBC Driver 17 for SQL Server] Description=Microsoft ODBC Driver 17 for SQL Server Driver=/opt/microsoft/msodbcsql17/lib64/libmsodbcsql-17.6.so.1.1 UsageCount=1 Connecting by explicitly providing the SQL connection string to ODBC libraries/packages INBO staff can consult a list of connection strings here. At this moment, you can actually connect using typical ODBC libraries/packages provided by R or Python:\nlibrary(DBI) connection \u0026lt;- dbConnect( odbc::odbc(), .connection_string = \u0026#34;Driver={ODBC Driver 17 for SQL Server};Server=DBServername;Database=DBName;Trusted_Connection=yes;\u0026#34; ) dbListTables(connection) However, most recommended for INBO staff using R is the use of the inbodb package, most notably the connect_inbo_dbase() function.\nimport pyodbc conn = pyodbc.connect(\u0026#34;Driver={ODBC Driver 17 for SQL Server};Server=DBServername;Database=DBName;Trusted_Connection=yes;\u0026#34;) In RStudio, you can also make the connection with the GUI:\n Go to the Connections pane and click \u0026lsquo;New Connection\u0026rsquo;. In the window that opens, choose the ODBC Driver for SQL Server. In the Parameters field that comes next, add Server=DBServerName;Database=DBName;Trusted_Connection=yes;.  Note that the DBI connection statement is visible at the bottom field of the dialog window.   Click Test to verify successful connection.  If connection is unsuccessful, try again after explicitly adding your username to the connection string: User ID=your_username;   If the test is successful, click OK to make the connection.  Beside the fact that the connection has been made (see RStudio\u0026rsquo;s R console), you also get a list of all databases (of the specific SQL Server) in the Connections pane. You can use this for exploratory purposes. Click here for more information on using RStudio\u0026rsquo;s Connections pane.\nUNTESTED: Connecting after configuring odbc.ini However, it is probably easier to provide the configuration to specific databases directly, using the /etc/odbc.ini file. For example, the DBName database can be defined as follows:\n[nbn_ipt] Driver = ODBC Driver 17 for SQL Server Description = odbc verbinding naar db Trace = No Server = DBServername Database = DBName Port = 1433 Next, add the DBServername\nTODO: -\u0026gt; example in R/Python -\u0026gt; also available in Rstudio!\n  You can also find the debian packages of Microsoft ODBC Driver for SQL Server here. You can find separate installation instructions for sqlcmd, bcp and unixodbc-dev here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The installation on Mac is done through Homebrew. The commands suggested in the link need to be entered in 2 turns: first install Homebrew and next issue the remaining commands.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","href":"/installation/user/user_install_kerberos/","title":"Using Kerberos authentication for database connection"},{"content":"","href":"/authors/elsdebie/","title":"elsdebie"},{"content":"See the vignette/tutorial at https://inbo.github.io/inbodb/articles/get_data_inboveg.html\n","href":"/tutorials/vignette_inbodb_get_data_inboveg/","title":"inbodb: How to retrieve data from the INBOVEG database"},{"content":"Original figure library(ggplot2) ggplot(mtcars, aes(x = factor(cyl), y = mpg)) + geom_boxplot() “Zoom in” with scales Data points outside the limits are considered to be NA. Note that this will alter all calculated geoms.\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) + geom_boxplot() + scale_y_continuous(limits = c(20, 25)) ## Warning: Removed 24 rows containing non-finite values (stat_boxplot).  “Zoom in” with coord This doesn’t affect the values:\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) + geom_boxplot() + coord_cartesian(ylim = c(20, 25)) Note that you can use only one coord. Only the last one will have an effect on the plot.\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) + geom_boxplot() + coord_cartesian(ylim = c(20, 25)) + coord_flip() ## Coordinate system already present. Adding new coordinate system, which will replace the existing one.  ggplot(mtcars, aes(x = factor(cyl), y = mpg)) + geom_boxplot() + coord_flip() + coord_cartesian(ylim = c(20, 25)) ## Coordinate system already present. Adding new coordinate system, which will replace the existing one.  Set the limits in coord_flip to get the effects of both.\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) + geom_boxplot() + coord_flip(ylim = c(20, 25)) ","href":"/tutorials/r_zoom_in_ggplot/","title":"'Zoom in' on `ggplot2` figures"},{"content":"","href":"/tags/analysis/","title":"analysis"},{"content":"","href":"/tags/ggplot2/","title":"ggplot2"},{"content":"What you will learn In this tutorial we explain the analogy between the paired t-test and the corresponding mixed model formulation.\nUsed packages library(knitr) library(lme4) library(tidyr) library(broom) library(DHARMa) Data  plot: identifies paired measurements response: measurement values treatment: identifies two treatments (a and b)  set.seed(124) paired_data \u0026lt;- data.frame( plot = rep(1:10, 2), response = c(rnorm(10), rnorm(10, 3, 1.5)), treatment = rep(c(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;), each = 10) ) paired_data$treatment \u0026lt;- as.factor(paired_data$treatment) paired_data$plot \u0026lt;- as.factor(paired_data$plot) # in wide format paired_data_wide \u0026lt;- pivot_wider( paired_data, id_cols = plot, names_from = treatment, values_from = response ) kable(paired_data)    plot response treatment     1 -1.3850706 a   2 0.0383232 a   3 -0.7630302 a   4 0.2123061 a   5 1.4255380 a   6 0.7444798 a   7 0.7002294 a   8 -0.2293546 a   9 0.1970939 a   10 1.2071538 a   1 3.4775051 b   2 0.8643017 b   3 2.3923637 b   4 4.4930798 b   5 4.4382267 b   6 4.3771318 b   7 2.7735456 b   8 1.1653968 b   9 1.6967636 b   10 1.4362720 b    kable(paired_data_wide)    plot a b     1 -1.3850706 3.4775051   2 0.0383232 0.8643017   3 -0.7630302 2.3923637   4 0.2123061 4.4930798   5 1.4255380 4.4382267   6 0.7444798 4.3771318   7 0.7002294 2.7735456   8 -0.2293546 1.1653968   9 0.1970939 1.6967636   10 1.2071538 1.4362720    The paired t-test ttest \u0026lt;- with( paired_data_wide, t.test(y = a, x = b, paired = TRUE) ) kable(tidy(ttest))    estimate statistic p.value parameter conf.low conf.high method alternative     2.496692 5.157401 0.0005972 9 1.401584 3.591799 Paired t-test two.sided    Alternative, but equivalent formulation via a lineair mixed model Plot identifies the paired measurements. A random effect for plot allows us to take this dependence into account.\nmm \u0026lt;- lmer(response ~ treatment + (1 | plot), data = paired_data ) The parameter estimates for treatment b gives the difference compared to treatment a (= intercept), accounting for the paired nature of the data. This difference is the same as the estimate for the paired t-test.\nkable(tidy(mm))    term estimate std.error statistic group     (Intercept) 0.2147669 0.3711260 0.578690 fixed   treatmentb 2.4966918 0.4840988 5.157401 fixed   sd_(Intercept).plot 0.4534165 NA NA plot   sd_Observation.Residual 1.0824778 NA NA Residual    The anova output gives us a test for treatment in terms of an F-test. The t-test is based on the t-statistic. Both test statistics are related: (F = t^2).\nkable(anova(mm))     npar Sum Sq Mean Sq F value     treatment 1 31.16735 31.16735 26.59879    anova(mm)[[\u0026#34;F value\u0026#34;]] ## [1] 26.59879  unname(ttest[[\u0026#34;statistic\u0026#34;]])^2 ## [1] 26.59879  We can calculate the confidence interval given as part of the t-test output, based on the t-distributie.\ndifference \u0026lt;- data.frame( diff = summary(mm)$coefficients[2, 1], se = summary(mm)$coefficients[2, 2] ) difference$lwr \u0026lt;- difference$diff - qt(p = 1 - 0.05 / 2, df = 9) * difference$se difference$upr \u0026lt;- difference$diff + qt(p = 1 - 0.05 / 2, df = 9) * difference$se kable(difference)    diff se lwr upr     2.496692 0.4840988 1.401584 3.591799    The recommended procedure to calculate a confidence interval for parameters of mixed models is, however, to use the `confint function. Either an approximation (Wald statistic) or a profile likelihood confidence interval can be calculated. These intervals are slightly different from the t-distribution based confidence interval.\nVia profile likelihood:\nkable(confint(mm, parm = \u0026#34;treatmentb\u0026#34;, method = \u0026#34;profile\u0026#34;)) ## Computing profile confidence intervals ...      2.5 % 97.5 %     treatmentb 1.505743 3.487635    Wald-type confidence interval:\nkable(confint(mm, parm = \u0026#34;treatmentb\u0026#34;, method = \u0026#34;Wald\u0026#34;))     2.5 % 97.5 %     treatmentb 1.547876 3.445508    Were model assuptions met? Yes.\nDHARMa::plotQQunif(mm) Take home message The standard paired t-test is typically used to test for a significant differences between two paired treatments. We can formulate the test in terms of a mixed model. The benefit is that we get more informative model output, which allows us among other things to check if model assumptions were met. For the paired t-test, one assumption is that the paired differences between treatments follow a normal distribution. When these assumptions are not met, the flexibility of the mixed model framework allows to improve the model to better fit the requirements for the data at hand. For instance, one can choose from a number of parametric statistical distributions that are likely to fit the data (for counts, the Poisson or negative binomial distribution can be chosen, and for binary or proportional data, a binomial distribution is an obvious choice).\n","href":"/tutorials/r_paired_t_test/","title":"Mixed model formulation of a paired t-test"},{"content":"Used packages library(ggplot2) ## Warning: package 'ggplot2' was built under R version 4.0.2  library(lme4) ## Loading required package: Matrix  Dummy data For the sake of this demontration we use a very simple dataset with a very high signal versus noise ratio. Let’s look at a simple timeseries with multiple observations per timepoint.\nset.seed(213354) n.year \u0026lt;- 30 n.replicate \u0026lt;- 10 sd.noise \u0026lt;- 0.1 dataset \u0026lt;- expand.grid( Replicate = seq_len(n.replicate), Year = seq_len(n.year) ) dataset$fYear \u0026lt;- factor(dataset$Year) dataset$Noise \u0026lt;- rnorm(nrow(dataset), mean = 0, sd = sd.noise) dataset$Linear \u0026lt;- 1 + 2 * dataset$Year + dataset$Noise dataset$Quadratic \u0026lt;- 1 + 20 * dataset$Year - 0.5 * dataset$Year ^ 2 + dataset$Noise ggplot(dataset, aes(x = Year, y = Linear)) + geom_point() ggplot(dataset, aes(x = Year, y = Quadratic)) + geom_point() Quadratic trend but fit as a linear trend Assume that we assume that the trend is linear but in reality it is quadratic. In this case the random intercept of year will pickup the differences with the linear trend.\nmodel.quadratric \u0026lt;- lmer(Quadratic ~ Year + (1|Year), data = dataset) summary(model.quadratric) ## Linear mixed model fit by REML ['lmerMod'] ## Formula: Quadratic ~ Year + (1 | Year) ## Data: dataset ## ## REML criterion at convergence: -90.8 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.13929 -0.65021 -0.00099 0.55503 3.14673 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Year (Intercept) 1198.1591 34.6144 ## Residual 0.0111 0.1054 ## Number of obs: 300, groups: Year, 30 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 83.6622 12.9622 6.454 ## Year 4.4999 0.7301 6.163 ## ## Correlation of Fixed Effects: ## (Intr) ## Year -0.873  dataset$QuadraticPredict \u0026lt;- predict(model.quadratric) dataset$QuadraticFixed \u0026lt;- predict(model.quadratric, re.form = ~0) ggplot(dataset, aes(x = Year, y = Quadratic)) + geom_point() + geom_line(aes(y = QuadraticFixed), colour = \u0026#34;red\u0026#34;) + geom_line(aes(y = QuadraticPredict), colour = \u0026#34;blue\u0026#34;) The random effects will have a strong pattern. Indicating that a second order polynomial makes more sense than a linear trend.\nrf \u0026lt;- data.frame( Year = seq_len(n.year), RandomIntercept = ranef(model.quadratric)$Year[, 1] ) ggplot(rf, aes(x = Year, y = RandomIntercept)) + geom_point() What if the trend is perfectly linear? Both the fixed effect and the random effect can perfectly model the pattern in the data. So the model seems to be unidentifiable. However the likelihood of the model contains a penalty term for the random intercept. The stronger the absolute value of the random effect, the stronger the penalty. The fixed effects have no penalty term. Hence, model with strong fixed effect will have a higher likelihood than exactly the same fit generated by strong random effects.\nIn this case the linear trend is very strong compared to the noise. So the linear trend in the fixed effect fits the data very well. Note that the random effect variance is zero.\nmodel.linear \u0026lt;- lmer(Linear ~ Year + (1|Year), data = dataset) ## boundary (singular) fit: see ?isSingular  summary(model.linear) ## Linear mixed model fit by REML ['lmerMod'] ## Formula: Linear ~ Year + (1 | Year) ## Data: dataset ## ## REML criterion at convergence: -483.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.3565 -0.6669 -0.0309 0.5717 3.0381 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Year (Intercept) 0.00000 0.0000 ## Residual 0.01095 0.1046 ## Number of obs: 300, groups: Year, 30 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 0.995497 0.012392 80.34 ## Year 1.999948 0.000698 2865.22 ## ## Correlation of Fixed Effects: ## (Intr) ## Year -0.873 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular  dataset$LinearPredict \u0026lt;- predict(model.linear) dataset$LinearFixed \u0026lt;- predict(model.linear, re.form = ~0) ggplot(dataset, aes(x = Year, y = Linear)) + geom_point() + geom_line(aes(y = LinearFixed), colour = \u0026#34;red\u0026#34;) + geom_line(aes(y = LinearPredict), colour = \u0026#34;blue\u0026#34;) What about fitting year as a factor in the fixed effects Combining the same variables as a factor in the fixed effects and as a random intercept doesn’t make sense. They allow exactly the same model fit and thus the random intercept will always shrink to zero.\n","href":"/tutorials/r_fixed_random/","title":"Same variable in fixed and random effects"},{"content":"This document is a quick and dirty illustration of how spatially correlated random effect can be fit with INLA. It is based on the question and the data posted on the R-Sig-mixedmodels mailing list: https://stat.ethz.ch/pipermail/r-sig-mixed-models/2016q3/024938.html.\nlibrary(dplyr) ## ## Attaching package: 'dplyr' ## The following objects are masked from 'package:stats': ## ## filter, lag ## The following objects are masked from 'package:base': ## ## intersect, setdiff, setequal, union  library(tidyr) library(ggplot2) ## Warning: package 'ggplot2' was built under R version 4.0.2  library(scales) library(INLA) ## Loading required package: Matrix ## ## Attaching package: 'Matrix' ## The following objects are masked from 'package:tidyr': ## ## expand, pack, unpack ## Loading required package: sp ## Loading required package: parallel ## Loading required package: foreach ## This is INLA_20.03.17 built 2020-05-05 07:15:25 UTC. ## See www.r-inla.org/contact-us for how to get help.  library(rprojroot) Data import and cleaning dataset \u0026lt;- readRDS(find_root_file( \u0026#34;content/tutorials/r_spde/\u0026#34;, \u0026#34;data.Rd\u0026#34;, criterion = is_git_root)) summary(dataset) ## prec_nov_apr t_min_nov_apr srad_nov_apr age date ## Min. : 47.3 Min. :0.9167 Min. : 869.2 Min. : 1.00 2009:494 ## 1st Qu.:118.5 1st Qu.:3.2500 1st Qu.:1232.0 1st Qu.: 9.00 2010:330 ## Median :137.7 Median :3.9917 Median :1386.2 Median :16.00 2011:143 ## Mean :131.6 Mean :3.9667 Mean :1359.5 Mean :17.73 2012: 97 ## 3rd Qu.:146.1 3rd Qu.:4.7833 3rd Qu.:1448.4 3rd Qu.:22.00 NA's:106 ## Max. :152.2 Max. :6.9167 Max. :1769.0 Max. :48.00 ## evaluation hail rlat rlon ## 1:1050 0:929 Min. :4704304 Min. :466044 ## 2: 120 1:241 1st Qu.:4770128 1st Qu.:507622 ## Median :4781687 Median :528596 ## Mean :4777992 Mean :532296 ## 3rd Qu.:4789497 3rd Qu.:553595 ## Max. :4810808 Max. :602423  dataset \u0026lt;- dataset %\u0026gt;% filter(!is.na(date)) %\u0026gt;% mutate( hail = as.integer(hail == \u0026#34;1\u0026#34;), rx = rlat * 1e-3, ry = rlon * 1e-3, srad = srad_nov_apr * 1e-3 - 1, prec = prec_nov_apr * 1e-2 - 1, temp = t_min_nov_apr - 4 ) summary(dataset) ## prec_nov_apr t_min_nov_apr srad_nov_apr age date ## Min. : 47.3 Min. :0.9167 Min. : 869.2 Min. : 1.00 2009:494 ## 1st Qu.:117.2 1st Qu.:3.2333 1st Qu.:1225.2 1st Qu.:10.00 2010:330 ## Median :136.3 Median :3.9833 Median :1388.0 Median :16.00 2011:143 ## Mean :130.7 Mean :3.9575 Mean :1359.4 Mean :17.68 2012: 97 ## 3rd Qu.:145.8 3rd Qu.:4.7833 3rd Qu.:1448.5 3rd Qu.:22.00 ## Max. :152.2 Max. :6.9167 Max. :1769.0 Max. :48.00 ## evaluation hail rlat rlon rx ## 1:980 Min. :0.0000 Min. :4705059 Min. :466044 Min. :4705 ## 2: 84 1st Qu.:0.0000 1st Qu.:4769384 1st Qu.:505849 1st Qu.:4769 ## Median :0.0000 Median :4781435 Median :528114 Median :4781 ## Mean :0.2246 Mean :4777523 Mean :531519 Mean :4778 ## 3rd Qu.:0.0000 3rd Qu.:4789367 3rd Qu.:553620 3rd Qu.:4789 ## Max. :1.0000 Max. :4810808 Max. :602403 Max. :4811 ## ry srad prec temp ## Min. :466.0 Min. :-0.1308 Min. :-0.5270 Min. :-3.08333 ## 1st Qu.:505.8 1st Qu.: 0.2252 1st Qu.: 0.1725 1st Qu.:-0.76667 ## Median :528.1 Median : 0.3880 Median : 0.3628 Median :-0.01667 ## Mean :531.5 Mean : 0.3594 Mean : 0.3072 Mean :-0.04245 ## 3rd Qu.:553.6 3rd Qu.: 0.4485 3rd Qu.: 0.4583 3rd Qu.: 0.78333 ## Max. :602.4 Max. : 0.7690 Max. : 0.5220 Max. : 2.91667  EDA ggplot(dataset, aes(x = age)) + geom_histogram(binwidth = 1) ggplot(dataset, aes(x = srad)) + geom_density() ggplot(dataset, aes(x = temp)) + geom_density() ggplot(dataset, aes(x = age, y = hail)) + geom_point() + geom_smooth( method = \u0026#34;gam\u0026#34;, formula = y ~ s(x, bs = \u0026#34;cs\u0026#34;, k = 4), method.args = list(family = binomial) ) ggplot(dataset, aes(x = srad, y = hail)) + geom_point() + geom_smooth( method = \u0026#34;gam\u0026#34;, formula = y ~ s(x, bs = \u0026#34;cs\u0026#34;, k = 4), method.args = list(family = binomial) ) ggplot(dataset, aes(x = temp, y = hail)) + geom_point() + geom_smooth( method = \u0026#34;gam\u0026#34;, formula = y ~ s(x, bs = \u0026#34;cs\u0026#34;, k = 4), method.args = list(family = binomial) ) ggplot(dataset, aes(x = prec, y = hail)) + geom_point() + geom_smooth( method = \u0026#34;gam\u0026#34;, formula = y ~ s(x, bs = \u0026#34;cs\u0026#34;, k = 4), method.args = list(family = binomial) ) ggplot(dataset, aes(x = prec, colour = date)) + geom_density() ggplot(dataset, aes(x = temp, colour = date)) + geom_density() ggplot(dataset, aes(x = rx, y = ry, colour = factor(hail))) + geom_point() + coord_fixed() + facet_wrap(~date) ggplot(dataset, aes(x = rx, y = ry, colour = temp)) + geom_point() + coord_fixed() + scale_colour_gradientn(colors = rainbow(5)) + facet_wrap(~date) ggplot(dataset, aes(x = rx, y = ry, colour = prec)) + geom_point() + coord_fixed() + scale_colour_gradientn(colors = rainbow(5)) + facet_wrap(~date) ggplot(dataset, aes(x = rx, y = ry, colour = srad)) + geom_point() + coord_fixed() + scale_colour_gradientn(colors = rainbow(5)) + facet_wrap(~date) Model without spatial correlation m1 \u0026lt;- inla( hail ~ prec + t_min_nov_apr + srad + age + date, family = \u0026#34;binomial\u0026#34;, data = dataset ) summary(m1) ## ## Call: ## c(\u0026quot;inla(formula = hail ~ prec + t_min_nov_apr + srad + age + date, \u0026quot;, \u0026quot; ## family = \\\u0026quot;binomial\\\u0026quot;, data = dataset)\u0026quot;) ## Time used: ## Pre = 0.959, Running = 0.525, Post = 0.475, Total = 1.96 ## Fixed effects: ## mean sd 0.025quant 0.5quant 0.975quant mode kld ## (Intercept) -4.470 0.537 -5.548 -4.461 -3.440 -4.444 0 ## prec 8.522 0.824 6.951 8.507 10.184 8.476 0 ## t_min_nov_apr 0.175 0.098 -0.016 0.175 0.369 0.174 0 ## srad 0.258 0.546 -0.815 0.258 1.330 0.257 0 ## age 0.000 0.008 -0.016 0.000 0.015 0.000 0 ## date2010 -1.220 0.228 -1.676 -1.217 -0.780 -1.212 0 ## date2011 -1.244 0.252 -1.748 -1.241 -0.757 -1.234 0 ## date2012 -2.975 0.422 -3.863 -2.955 -2.202 -2.914 0 ## ## Expected number of effective parameters(stdev): 8.00(0.00) ## Number of equivalent replicates : 133.01 ## ## Marginal log-Likelihood: -467.48  Model with spatial random intercept coordinates \u0026lt;- dataset %\u0026gt;% select(rx, ry) %\u0026gt;% as.matrix() boundary \u0026lt;- inla.nonconvex.hull(coordinates) mesh \u0026lt;- inla.mesh.2d( loc = coordinates, boundary = boundary, max.edge = 20, cutoff = 5 ) plot(mesh) points(coordinates, col = \u0026#34;red\u0026#34;) spde \u0026lt;- inla.spde2.matern(mesh = mesh) A \u0026lt;- inla.spde.make.A(mesh = mesh, loc = coordinates) s.index \u0026lt;- inla.spde.make.index(name = \u0026#34;spatial.field\u0026#34;, n.spde = spde$n.spde) stack \u0026lt;- inla.stack( data = dataset %\u0026gt;% select(hail) %\u0026gt;% as.list(), A = list(A, 1), effects = list( c( s.index, list(intercept = rep(1, spde$n.spde)) ), dataset %\u0026gt;% select(temp, prec, srad, age) %\u0026gt;% as.list() ) ) m2 \u0026lt;- inla( hail ~ 0 + intercept + temp + prec + srad + age + f(spatial.field, model = spde), data = inla.stack.data(stack), family = \u0026#34;binomial\u0026#34;, control.predictor = list( A = inla.stack.A(stack), compute = TRUE ) ) summary(m2) ## ## Call: ## c(\u0026quot;inla(formula = hail ~ 0 + intercept + temp + prec + srad + age + \u0026quot;, ## \u0026quot; f(spatial.field, model = spde), family = \\\u0026quot;binomial\\\u0026quot;, data = ## inla.stack.data(stack), \u0026quot;, \u0026quot; control.predictor = list(A = ## inla.stack.A(stack), compute = TRUE))\u0026quot; ) ## Time used: ## Pre = 1.55, Running = 28.9, Post = 1.42, Total = 31.9 ## Fixed effects: ## mean sd 0.025quant 0.5quant 0.975quant mode kld ## intercept -2.476 0.847 -4.204 -2.456 -0.862 -2.417 0 ## temp 1.070 0.238 0.616 1.066 1.551 1.056 0 ## prec 0.516 1.902 -3.348 0.552 4.166 0.611 0 ## srad -1.961 0.836 -3.625 -1.953 -0.340 -1.938 0 ## age -0.014 0.011 -0.036 -0.014 0.007 -0.014 0 ## ## Random effects: ## Name Model ## spatial.field SPDE2 model ## ## Model hyperparameters: ## mean sd 0.025quant 0.5quant 0.975quant mode ## Theta1 for spatial.field -2.103 0.559 -3.30 -2.059 -1.119 -1.899 ## Theta2 for spatial.field -0.745 0.344 -1.36 -0.768 -0.018 -0.853 ## ## Expected number of effective parameters(stdev): 76.40(3.25) ## Number of equivalent replicates : 13.93 ## ## Marginal log-Likelihood: -365.15 ## Posterior marginals for the linear predictor and ## the fitted values are computed  Predict values on a grid n.grid \u0026lt;- 50 dx \u0026lt;- diff(pretty(dataset$rx, n.grid)[1:2]) dy \u0026lt;- diff(pretty(dataset$ry, n.grid)[1:2]) delta \u0026lt;- max(dx, dy) grid \u0026lt;- expand.grid( rx = seq( floor(min(dataset$rx) / delta) * delta, max(dataset$rx) + delta, by = delta ), ry = seq( floor(min(dataset$ry) / delta) * delta, max(dataset$ry) + delta, by = delta ) ) A.grid \u0026lt;- inla.spde.make.A(mesh = mesh, loc = as.matrix(grid)) stack.grid \u0026lt;- inla.stack( data = list(hail = NA), A = list(A.grid), effects = list( c( s.index, list(intercept = rep(1, spde$n.spde)) ) ), tag = \u0026#34;grid\u0026#34; ) stack.join \u0026lt;- inla.stack(stack, stack.grid) m3 \u0026lt;- inla( hail ~ 0 + intercept + temp + prec + srad + age + f(spatial.field, model = spde), data = inla.stack.data(stack.join), family = \u0026#34;binomial\u0026#34;, control.predictor = list( A = inla.stack.A(stack.join), compute = TRUE ) ) summary(m3) ## ## Call: ## c(\u0026quot;inla(formula = hail ~ 0 + intercept + temp + prec + srad + age + \u0026quot;, ## \u0026quot; f(spatial.field, model = spde), family = \\\u0026quot;binomial\\\u0026quot;, data = ## inla.stack.data(stack.join), \u0026quot;, \u0026quot; control.predictor = list(A = ## inla.stack.A(stack.join), compute = TRUE))\u0026quot; ) ## Time used: ## Pre = 1.42, Running = 96.7, Post = 1.5, Total = 99.6 ## Fixed effects: ## mean sd 0.025quant 0.5quant 0.975quant mode kld ## intercept -2.476 0.848 -4.204 -2.456 -0.861 -2.417 0 ## temp 1.070 0.238 0.616 1.066 1.551 1.056 0 ## prec 0.516 1.902 -3.349 0.552 4.166 0.611 0 ## srad -1.961 0.836 -3.625 -1.953 -0.340 -1.938 0 ## age -0.014 0.011 -0.036 -0.014 0.007 -0.014 0 ## ## Random effects: ## Name Model ## spatial.field SPDE2 model ## ## Model hyperparameters: ## mean sd 0.025quant 0.5quant 0.975quant mode ## Theta1 for spatial.field -2.103 0.559 -3.30 -2.059 -1.121 -1.899 ## Theta2 for spatial.field -0.745 0.344 -1.36 -0.768 -0.018 -0.853 ## ## Expected number of effective parameters(stdev): 76.40(3.25) ## Number of equivalent replicates : 13.93 ## ## Marginal log-Likelihood: -365.15 ## Posterior marginals for the linear predictor and ## the fitted values are computed  grid.index \u0026lt;- inla.stack.index(stack.join, tag = \u0026#34;grid\u0026#34;)$data grid$mean \u0026lt;- m3$summary.fitted.values[grid.index, \u0026#34;mean\u0026#34;] ggplot(grid, aes(x = rx, y = ry, fill = mean)) + geom_tile() + scale_fill_gradient2() + coord_fixed() grid$lcl \u0026lt;- m3$summary.fitted.values[grid.index, \u0026#34;0.025quant\u0026#34;] ggplot(grid, aes(x = rx, y = ry, fill = lcl)) + geom_tile() + scale_fill_gradient2() + coord_fixed() grid$ucl \u0026lt;- m3$summary.fitted.values[grid.index, \u0026#34;0.975quant\u0026#34;] ggplot(grid, aes(x = rx, y = ry, fill = ucl)) + geom_tile() + scale_fill_gradient2() + coord_fixed() grid %\u0026gt;% gather(key = \u0026#34;type\u0026#34;, value = \u0026#34;estimate\u0026#34;, mean:ucl) %\u0026gt;% mutate(estimate = plogis(estimate)) %\u0026gt;% ggplot(aes(x = rx, y = ry, fill = estimate)) + geom_tile() + scale_fill_gradient2( \u0026#34;Probabily of hail\\nat reference values\u0026#34;, midpoint = 0.5, limits = 0:1, label = percent ) + coord_fixed() + facet_wrap(~type, nrow = 1) ","href":"/tutorials/r_spde/","title":"Spatially correlated random effects with INLA"},{"content":"Used packages library(ggplot2) ## Warning: package 'ggplot2' was built under R version 4.0.2  library(scales) library(pscl) ## Warning: package 'pscl' was built under R version 4.0.2 ## Classes and Methods for R developed in the ## Political Science Computational Laboratory ## Department of Political Science ## Stanford University ## Simon Jackman ## hurdle and zeroinfl functions by Achim Zeileis  library(MASS) library(tidyr) library(dplyr) ## ## Attaching package: 'dplyr' ## The following object is masked from 'package:MASS': ## ## select ## The following objects are masked from 'package:stats': ## ## filter, lag ## The following objects are masked from 'package:base': ## ## intersect, setdiff, setequal, union  library(lme4) ## Loading required package: Matrix ## ## Attaching package: 'Matrix' ## The following objects are masked from 'package:tidyr': ## ## expand, pack, unpack  Lots of zero’s doesn’t imply zero-inflation set.seed(1234) n \u0026lt;- 1e4 n.sim \u0026lt;- 1e3 mean.poisson \u0026lt;- 0.05 mean.zeroinfl \u0026lt;- 10000 prop.zeroinfl \u0026lt;- 0.1 dataset \u0026lt;- data.frame( Poisson = rpois(n, lambda = mean.poisson) ) ggplot() + geom_linerange( data = dataset %\u0026gt;% filter(Poisson == 0) %\u0026gt;% count(Poisson), mapping = aes( x = Poisson, ymin = 0, ymax = n ) ) + geom_histogram( data = dataset %\u0026gt;% filter(Poisson \u0026gt; 0), mapping = aes(x = Poisson), boundary = 0 ) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  table(dataset$Poisson) ## ## 0 1 2 ## 9533 462 5  The example above generates values from a Poisson distribution with mean 0.05. Although it has no zero-inflation, 95.3% of the values are zero.\nZero-inflation can also occur with low number of zero’s dataset$ZeroInflatedPoisson \u0026lt;- rbinom(n, size = 1, prob = 1 - prop.zeroinfl) * rpois(n, lambda = mean.zeroinfl) ggplot() + geom_linerange( data = dataset %\u0026gt;% filter(ZeroInflatedPoisson == 0) %\u0026gt;% count(ZeroInflatedPoisson), mapping = aes( x = ZeroInflatedPoisson, ymin = 0, ymax = n ) ) + geom_histogram( data = dataset %\u0026gt;% filter(ZeroInflatedPoisson \u0026gt; 0), mapping = aes(x = ZeroInflatedPoisson), boundary = 0 ) table(dataset$ZeroInflatedPoisson == 0) ## ## FALSE TRUE ## 9007 993  The second example generates a data from a zero-inflated Poisson distribution with mean 10^{4} and 10, % excess zero’s. The actual proportion of zero’s is 9.9%.\nHow to test for zero-inflation dataset \u0026lt;- expand.grid( Mean = c(mean.poisson, mean.zeroinfl), Rep = seq_len(n) ) dataset$Poisson \u0026lt;- rpois(nrow(dataset), lambda = dataset$Mean) dataset$ZeroInflatedPoisson \u0026lt;- rbinom(nrow(dataset), size = 1, prob = 1 - prop.zeroinfl) * rpois(nrow(dataset), lambda = dataset$Mean) ggplot() + geom_linerange( data = dataset %\u0026gt;% filter(Poisson == 0) %\u0026gt;% count(Poisson), mapping = aes( x = Poisson, ymin = 0, ymax = n ) ) + geom_histogram( data = dataset %\u0026gt;% filter(Poisson \u0026gt; 0), mapping = aes(x = Poisson), boundary = 0 ) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  ggplot() + geom_linerange( data = dataset %\u0026gt;% filter(ZeroInflatedPoisson == 0) %\u0026gt;% count(ZeroInflatedPoisson), mapping = aes( x = ZeroInflatedPoisson, ymin = 0, ymax = n ) ) + geom_histogram( data = dataset %\u0026gt;% filter(ZeroInflatedPoisson \u0026gt; 0), mapping = aes(x = ZeroInflatedPoisson), boundary = 0 ) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  For this example we generate a new dataset with two levels of Mean: 0.05, 10000.00. We generate both a Poisson and a zero-inflated Poisson variable. The latter has 10, % excess zero’s. The proportion of zero’s the variables are respectively 47.3% and 52.7%.\nLet’s assume that our hypothesis is that all observations share the same mean. Note that is clearly not the case. If this hypothesis would hold, then we fit a simple intercept-only model.\nm.pois.simple \u0026lt;- glm(Poisson ~ 1, data = dataset, family = poisson) m.zero.simple \u0026lt;- glm(ZeroInflatedPoisson ~ 1, data = dataset, family = poisson) summary(m.pois.simple) ## ## Call: ## glm(formula = Poisson ~ 1, family = poisson, data = dataset) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -100.00 -100.00 -20.85 62.15 66.33 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) 8.5172 0.0001 85172 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 138627905 on 19999 degrees of freedom ## Residual deviance: 138627905 on 19999 degrees of freedom ## AIC: 138739486 ## ## Number of Fisher Scoring iterations: 6  summary(m.zero.simple) ## ## Call: ## glm(formula = ZeroInflatedPoisson ~ 1, family = poisson, data = dataset) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -94.96 -94.96 -94.96 70.19 74.27 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) 8.4137957 0.0001053 79899 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 143653204 on 19999 degrees of freedom ## Residual deviance: 143653204 on 19999 degrees of freedom ## AIC: 143753725 ## ## Number of Fisher Scoring iterations: 6  After fitting the model we generate at random a new response variable based on the models and count the number of zero’s. This is repeated several times so that we can get a distribion of zero’s based on the model. Finally we compare the number of zero’s in the original dataset with this distribution.\nsimulated \u0026lt;- data.frame( PoissonSimple = apply( simulate(m.pois.simple, nsim = n.sim) == 0, 2, mean ), ZeroInflatedSimple = apply( simulate(m.zero.simple, nsim = n.sim) == 0, 2, mean ) ) ggplot(simulated, aes(x = PoissonSimple)) + geom_histogram(binwidth = 0.01) + geom_vline(xintercept = mean(dataset$Poisson == 0), colour = \u0026#34;red\u0026#34;) + scale_x_continuous(label = percent) ggplot(simulated, aes(x = ZeroInflatedSimple)) + geom_histogram(binwidth = 0.01) + geom_vline(xintercept = mean(dataset$Poisson == 0), colour = \u0026#34;red\u0026#34;) + scale_x_continuous(label = percent) In both cases none of the simulated values contains zero’s. The red line indicates the observed proportion of zero’s with is clearly greater. This indicates that we have either a poor model fit or zero-inflation.\nImproving the model In the case of this example we have two groups with very different means: one group with low mean generating lots of zero’s and one group with large mean generating no zero’s. Adding this grouping improves the model.\nm.pois.complex \u0026lt;- glm(Poisson ~ factor(Mean), data = dataset, family = poisson) m.zero.complex \u0026lt;- glm(ZeroInflatedPoisson ~ factor(Mean), data = dataset, family = poisson) summary(m.pois.complex) ## ## Call: ## glm(formula = Poisson ~ factor(Mean), family = poisson, data = dataset) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.6720 -0.3353 -0.3353 0.1405 4.2399 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -2.87884 0.04218 -68.25 \u0026lt;2e-16 *** ## factor(Mean)10000 12.08917 0.04218 286.59 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 138627905 on 19999 degrees of freedom ## Residual deviance: 13118 on 19998 degrees of freedom ## AIC: 124702 ## ## Number of Fisher Scoring iterations: 6  summary(m.zero.complex) ## ## Call: ## glm(formula = ZeroInflatedPoisson ~ factor(Mean), family = poisson, ## data = dataset) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -134.295 -0.301 -0.301 10.023 13.695 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -3.09224 0.04691 -65.93 \u0026lt;2e-16 *** ## factor(Mean)10000 12.19918 0.04691 260.08 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 143653204 on 19999 degrees of freedom ## Residual deviance: 18653539 on 19998 degrees of freedom ## AIC: 18754062 ## ## Number of Fisher Scoring iterations: 5  simulated$PoissonComplex \u0026lt;- apply( simulate(m.pois.complex, nsim = n.sim) == 0, 2, mean ) simulated$ZeroInflatedComplex \u0026lt;- apply( simulate(m.zero.complex, nsim = n.sim) == 0, 2, mean ) ggplot(simulated, aes(x = PoissonComplex)) + geom_histogram(binwidth = 0.0005) + geom_vline(xintercept = mean(dataset$Poisson == 0), colour = \u0026#34;red\u0026#34;) + scale_x_continuous(label = percent) ggplot(simulated, aes(x = ZeroInflatedComplex)) + geom_histogram(binwidth = 0.0005) + geom_vline(xintercept = mean(dataset$ZeroInflatedPoisson == 0), colour = \u0026#34;red\u0026#34;) + scale_x_continuous(label = percent) In case of the Poisson variable, 45.1% of the simulated dataset from the complex model has more zero’s than the observed dataset. So this model can captures the zero’s and there is not zero-inflation. In case of the zero-inflated Poisson variable 0.0% of the simulated dataset from the complex model has more zero’s than the observed dataset. Hence we conclude that the zero’s are not modelled properly. We can’t improve the model further with covariates, hence we’ll have to treat is a a zero-inflated distribution.\nm.zero.zi \u0026lt;- zeroinfl( ZeroInflatedPoisson ~ factor(Mean) | 1, data = dataset, dist = \u0026#34;poisson\u0026#34; ) summary(m.zero.zi) ## ## Call: ## zeroinfl(formula = ZeroInflatedPoisson ~ factor(Mean) | 1, data = dataset, ## dist = \u0026quot;poisson\u0026quot;) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -3.0282 -0.2126 -0.2126 0.3297 9.1506 ## ## Count model coefficients (poisson with log link): ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -2.98879 0.04716 -63.37 \u0026lt;2e-16 *** ## factor(Mean)10000 12.19909 0.04716 258.65 \u0026lt;2e-16 *** ## ## Zero-inflation model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -2.2170 0.0336 -65.98 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Number of iterations in BFGS optimization: 13 ## Log-likelihood: -5.938e+04 on 3 Df  sim.zero.zi \u0026lt;- function(model) { eta \u0026lt;- predict(model, type = \u0026#34;count\u0026#34;) prob \u0026lt;- predict(model, type = \u0026#34;zero\u0026#34;) new.value \u0026lt;- rbinom(length(eta), size = 1, prob = 1 - prob) * rpois(length(eta), lambda = eta) mean(new.value == 0) } simulated$ZeroInflatedZI \u0026lt;- replicate(n.sim, sim.zero.zi(m.zero.zi)) ggplot(simulated, aes(x = ZeroInflatedZI)) + geom_histogram(binwidth = 0.0005) + geom_vline(xintercept = mean(dataset$ZeroInflatedPoisson == 0), colour = \u0026#34;red\u0026#34;) + scale_x_continuous(label = percent) When we fit a proper zero-inflated model to the zero-inflated Poisson variable, then 45.5% of the simulated datasets have more zero’s than the observed dataset. So the zero-inflation is properly handled.\nModelling zero-inflation by overdispersion m.zero.nb \u0026lt;- glm.nb(ZeroInflatedPoisson ~ factor(Mean), data = dataset) summary(m.zero.nb) ## ## Call: ## glm.nb(formula = ZeroInflatedPoisson ~ factor(Mean), data = dataset, ## init.theta = 0.6535490762, link = log) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.5298 -0.2963 -0.2963 0.0850 2.8393 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -3.09224 0.04854 -63.71 \u0026lt;2e-16 *** ## factor(Mean)10000 12.19918 0.05009 243.56 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for Negative Binomial(0.6535) family taken to be 1) ## ## Null deviance: 130864 on 19999 degrees of freedom ## Residual deviance: 14676 on 19998 degrees of freedom ## AIC: 204749 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 0.65355 ## Std. Err.: 0.00868 ## ## 2 x log-likelihood: -204742.76400  dataset$ID \u0026lt;- seq_along(dataset$Mean) m.zero.olre \u0026lt;- glmer( ZeroInflatedPoisson ~ factor(Mean) + (1 | ID), data = dataset, family = poisson ) summary(m.zero.olre) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: poisson ( log ) ## Formula: ZeroInflatedPoisson ~ factor(Mean) + (1 | ID) ## Data: dataset ## ## AIC BIC logLik deviance df.resid ## 218344.4 218368.1 -109169.2 218338.4 19997 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -0.99401 -0.06651 -0.06651 0.00113 0.81490 ## ## Random effects: ## Groups Name Variance Std.Dev. ## ID (Intercept) 8.377 2.894 ## Number of obs: 20000, groups: ID, 20000 ## ## Fixed effects: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -5.38382 0.06961 -77.34 \u0026lt;2e-16 *** ## factor(Mean)10000 13.64883 0.07490 182.23 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Correlation of Fixed Effects: ## (Intr) ## fct(M)10000 -0.921  simulated$ZeroInflatedNB \u0026lt;- apply( simulate(m.zero.nb, nsim = n.sim) == 0, 2, mean ) ggplot(simulated, aes(x = ZeroInflatedNB)) + geom_histogram(binwidth = 0.0005) + geom_vline(xintercept = mean(dataset$ZeroInflatedPoisson == 0), colour = \u0026#34;red\u0026#34;) + scale_x_continuous(label = percent) simulated$ZeroInflatedOLRE \u0026lt;- apply( simulate(m.zero.olre, nsim = n.sim) == 0, 2, mean ) ggplot(simulated, aes(x = ZeroInflatedOLRE)) + geom_histogram(binwidth = 0.0005) + geom_vline(xintercept = mean(dataset$ZeroInflatedPoisson == 0), colour = \u0026#34;red\u0026#34;) + scale_x_continuous(label = percent) mean.2 \u0026lt;- 5 dataset2 \u0026lt;- expand.grid( Mean = mean.2, Rep = seq_len(100) ) dataset2$ZeroInflatedPoisson \u0026lt;- rbinom(nrow(dataset2), size = 1, prob = 1 - prop.zeroinfl) * rpois(nrow(dataset2), lambda = dataset2$Mean) ggplot(dataset2, aes(x = ZeroInflatedPoisson)) + geom_histogram(binwidth = 1) m.zero2 \u0026lt;- glm(ZeroInflatedPoisson ~ 1, data = dataset2, family = poisson) summary(m.zero2) ## ## Call: ## glm(formula = ZeroInflatedPoisson ~ 1, family = poisson, data = dataset2) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.9326 -0.6633 -0.1464 0.7731 2.6952 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) 1.45862 0.04822 30.25 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 217.63 on 99 degrees of freedom ## Residual deviance: 217.63 on 99 degrees of freedom ## AIC: 508.69 ## ## Number of Fisher Scoring iterations: 5  simulated$ZeroInflated2 \u0026lt;- apply( simulate(m.zero2, nsim = n.sim) == 0, 2, mean ) ggplot(simulated, aes(x = ZeroInflated2)) + geom_histogram(binwidth = 0.01) + geom_vline(xintercept = mean(dataset2$ZeroInflatedPoisson == 0), colour = \u0026#34;red\u0026#34;) + scale_x_continuous(label = percent) m.zero.nb2 \u0026lt;- glm.nb(ZeroInflatedPoisson ~ 1, data = dataset2) summary(m.zero.nb2) ## ## Call: ## glm.nb(formula = ZeroInflatedPoisson ~ 1, data = dataset2, init.theta = 4.249064467, ## link = log) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4375 -0.4811 -0.1038 0.5292 1.7364 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) 1.4586 0.0684 21.32 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for Negative Binomial(4.2491) family taken to be 1) ## ## Null deviance: 130.63 on 99 degrees of freedom ## Residual deviance: 130.63 on 99 degrees of freedom ## AIC: 489.84 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 4.25 ## Std. Err.: 1.44 ## ## 2 x log-likelihood: -485.839  simulated$ZeroInflatedNB2 \u0026lt;- apply( simulate(m.zero.nb2, nsim = n.sim) == 0, 2, mean ) ggplot(simulated, aes(x = ZeroInflatedNB2)) + geom_histogram(binwidth = 0.01) + geom_vline(xintercept = mean(dataset2$ZeroInflatedPoisson == 0), colour = \u0026#34;red\u0026#34;) + scale_x_continuous(label = percent) dataset2$ID \u0026lt;- seq_along(dataset2$Mean) m.zero.olre2 \u0026lt;- glmer( ZeroInflatedPoisson ~ (1 | ID), data = dataset2, family = poisson ) simulated$ZeroInflatedOLRE2 \u0026lt;- apply( simulate(m.zero.olre2, nsim = n.sim) == 0, 2, mean ) ggplot(simulated, aes(x = ZeroInflatedOLRE2)) + geom_histogram(binwidth = 0.01) + geom_vline(xintercept = mean(dataset2$ZeroInflatedPoisson == 0), colour = \u0026#34;red\u0026#34;) + scale_x_continuous(label = percent) m.zero.zi2 \u0026lt;- zeroinfl( ZeroInflatedPoisson ~ 1, data = dataset2, dist = \u0026#34;poisson\u0026#34; ) summary(m.zero.zi2) ## ## Call: ## zeroinfl(formula = ZeroInflatedPoisson ~ 1, data = dataset2, dist = \u0026quot;poisson\u0026quot;) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -1.5786 -0.4772 -0.1101 0.6241 2.4596 ## ## Count model coefficients (poisson with log link): ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) 1.61454 0.04905 32.92 \u0026lt;2e-16 *** ## ## Zero-inflation model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -1.7794 0.2912 -6.111 9.9e-10 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Number of iterations in BFGS optimization: 5 ## Log-likelihood: -225.2 on 2 Df  simulated$ZeroInflatedZI2 \u0026lt;- replicate(n.sim, sim.zero.zi(m.zero.zi2)) ggplot(simulated, aes(x = ZeroInflatedZI2)) + geom_histogram(binwidth = 0.01) + geom_vline(xintercept = mean(dataset2$ZeroInflatedPoisson == 0), colour = \u0026#34;red\u0026#34;) + scale_x_continuous(label = percent) A negative binomial distribution can capture some of the zero-inflation by overdispersion. Especially in cases with low mean and few data. When the mean is low, a reasonable portion of the zero origination comes from the poisson distribution. However, the shape of the distribution is different.\ndistribution \u0026lt;- data.frame( Count = 0:20 ) distribution$Truth \u0026lt;- dpois( distribution$Count, lambda = mean.2 ) + ifelse( distribution$Count == 0, prop.zeroinfl, 0 ) distribution$Poisson \u0026lt;- dpois( distribution$Count, lambda = exp(coef(m.zero2)) ) distribution$ZIPoisson \u0026lt;- dpois( distribution$Count, lambda = exp(coef(m.zero.zi2)[1]) ) + ifelse( distribution$Count == 0, plogis(coef(m.zero.zi2)[2]), 0 ) se \u0026lt;- sqrt(VarCorr(m.zero.olre2)$ID) z \u0026lt;- seq(qnorm(.001, sd = se), qnorm(.999, sd = se), length = 101) dz \u0026lt;- dnorm(z, sd = se) dz \u0026lt;- dz / sum(dz) delta \u0026lt;- outer( distribution$Count, exp(z + fixef(m.zero.olre2)), FUN = dpois ) distribution$OLRE \u0026lt;- delta %*% dz distribution$NegBin \u0026lt;- dnbinom( distribution$Count, size = m.zero.nb2$theta, mu = exp(coef(m.zero.nb2)) ) long \u0026lt;- gather(distribution, key = \u0026#34;Model\u0026#34;, value = \u0026#34;Density\u0026#34;, -Count) ## Warning: attributes are not identical across measure variables; ## they will be dropped  long$Truth \u0026lt;- long$Model == \u0026#34;Truth\u0026#34; ggplot(long, aes(x = Count, y = Density, colour = Model, linetype = Truth)) + geom_line() ","href":"/tutorials/r_zero_inflation/","title":"Zero-inflation"},{"content":"","href":"/authors/gertvanspaendonk/","title":"gertvanspaendonk"},{"content":"","href":"/tags/sql/","title":"SQL"},{"content":"","href":"/tags/styleguide/","title":"styleguide"},{"content":"SQL is a standard language for storing, manipulating and retrieving data in databases. This is not a SQL-course but a styleguide, describing how to enhance the readability of your SQL-scripts. Focus of the styleguide is on scripts for retrieving data.\nIn short  --This is how a basic SQL-script should look like /** Description: Lijst met broedvogels per UTM1-hok sinds 2010 Created: 2015-08-12 Created by: Frederic Piesschaert **/ SELECT w.WRNG_JAR AS jaar , w.WRNG_UTM1_CDE AS utm1 , s.SPEC_NAM_WET AS wetenschappelijke_naam , s.SPEC_NAM_NED AS nederlandse_naam , t.TOPO_DES AS locatie FROM tblWaarneming w INNER JOIN tblWaarnemingMeting wm ON w.WRNG_ID = wm.WRME_WRNG_ID INNER JOIN tblSoort s ON wm.WRME_SPEC_CDE = s.SPEC_CDE LEFT JOIN tblToponiem t on t.TOPO_ID = w.WRNG_TOPO_ID --toponiemen werden niet altijd ingevuld WHERE 1 = 1 AND w.WRNG_UTM1_CDE IS NOT NULL AND w.WRNG_JAR \u0026gt; 2010 GROUP BY w.WRNG_JAR , w.WRNG_UTM1_CDE , s.SPEC_NAM_WET , s.SPEC_NAM_NED , t.TOPO_DES ORDER BY s.SPEC_NAM_NED  SQL-keywords (SELECT, FROM, JOIN, WHERE, GROUP BY, \u0026hellip;) are written in capitals Table names and field names are capitalized as they are defined in the database Use short and meaningful aliases and write them in lowercase Use a new line for each field in the SELECT-statement and each argument in the WHERE and GROUP BY clause Put the comma in front of the line in SELECT and GROUP BY statements Indent each field in the SELECT-statement and each argument in the WHERE and GROUP BY clause When multiple arguments are used in the WHERE clause, AND/OR keywords are always placed at the front Use full INNER JOIN statements JOINS should be indented Subqueries should be indented and properly named Put whitespaces around relational operators (= \u0026gt; \u0026hellip;) Document your scripts  Layout   SQL-keywords (SELECT, FROM, JOIN, WHERE, GROUP BY, \u0026hellip;) are written in capitals\n--Good SELECT * FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 --Bad Select * From person p Inner join address a on a.personid = p.id where p.age \u0026gt; 50 --Ugly :)   Table names and field names are capitalized as they are defined in the database, i.e. lowercase when lowercase in the database, capitals when capitals in the database\n  Aliases are written in lowercase\n--Good SELECT p.* , a.city FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 --Bad SELECT P.* , A.city FROM person P INNER JOIN address A ON A.personid = P.id WHERE P.age \u0026gt; 50   Table aliases are short and meaningful in the context of the query\n--Good SELECT p.* , a.city FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 --Bad SELECT x.* , placewheresomeonelives.city FROM person x INNER JOIN address placewheresomeonelives ON placewheresomeonelives.personid = x.id WHERE x.age \u0026gt; 50   Always use aliases\n  Use a new line for each field in the SELECT-statement and each argument in the WHERE and GROUP BY clause.\n--Good SELECT p.firstname , p.lastname , a.city FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 AND a.city like ‘Ar%’ --Bad SELECT p.firstname, p.lastname, a.city FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 AND a.city like ‘Ar%’   Put the comma in front of the line in SELECT and GROUP BY statements\n--Good SELECT p.firstname , p.lastname , a.city , COUNT(*) AS Aantal FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 AND a.city like ‘Ar%’ GROUP BY p.firstname , p.lastname , a.city --Less good SELECT p.firstname, p.lastname, a.city, COUNT(*) AS Aantal FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 AND a.city like ‘Ar%’ GROUP BY p.firstname, p.lastname, a.city   Indent each field in the SELECT-statement and each argument in the WHERE and GROUP BY clause\n--Good SELECT p.firstname , p.lastname , a.city FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 AND a.city like ‘Ar%’ --Bad SELECT p.firstname , p.lastname , a.city FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 AND a.city like ‘Ar%’   When multiple arguments are used in the WHERE clause, AND/OR keywords are always placed at the front\n--Good SELECT * FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 AND a.city like ‘Ar%’ AND p.firstname = 'Billy' --Bad SELECT * FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 AND a.city like ‘Ar%’ AND p.firstname = 'Billy'   Multiple constraints for a JOIN are on the same line\n--Good layout (for a poorly designed database) SELECT * FROM person p INNER JOIN address a ON a.firstname = p.firstname AND a.lastname = p.lastname WHERE p.age \u0026gt; 50 AND a.city like ‘Ar%’ AND p.firstname = 'Billy' --Bad SELECT * FROM person p INNER JOIN address a ON a.firstname = p.firstname AND a.lastname = p.lastname WHERE p.age \u0026gt; 50 AND a.city like ‘Ar%’ AND p.firstname = 'Billy'   Use a full INNER JOIN statement\n--Good SELECT p.* , a.city FROM person p INNER JOIN address a ON a.personid = p.id LEFT JOIN hobby h ON h.personid = p.id --Bad SELECT p.* , a.city FROM person p JOIN address a ON a.personid = p.id LEFT JOIN hobby h ON h.personid = p.id   Joins should be indented\n--Good SELECT p.* , a.city FROM person p INNER JOIN address a ON a.personid = p.id LEFT OUTER JOIN hobby h ON h.personid = p.id WHERE p.age \u0026gt; 50 --Bad SELECT p.* , a.city FROM person p INNER JOIN address a ON a.personid = p.id LEFT OUTER JOIN hobby h ON h.personid = p.id WHERE p.age \u0026gt; 50   Subqueries should be indented and properly named. Opening bracket is on a new line and aligned with the JOIN. Closing bracket is aligned with the opening bracket.\n--Good SELECT ppds.ppnt_cde , ds.drso_ser_nbr , dsth.dsth_dsha_cde , dsth.dsth_ocr_dte FROM tblDruksondetoestandhistoriek dsth INNER JOIN tblDruksonde ds ON ds.drso_id = dsth.dsth_drso_id INNER JOIN ( SELECT p.ppnt_cde , pd.ppds_drso_id FROM relPeilpuntdruksonde pd INNER JOIN tblPeilpunt p ON p.ppnt_id = pd.ppds_ppnt_id WHERE p.ppnt_cde like 'KAMP%' )ppds ON ppds.ppds_drso_id = dsth.dsth_drso_id WHERE dsth.dsth_dsha_cde = 'PROG’ --Bad SELECT ppds.ppnt_cde , ds.drso_ser_nbr , dsth.dsth_dsha_cde , dsth.dsth_ocr_dte FROM tblDruksondetoestandhistoriek dsth INNER JOIN tblDruksonde ds ON ds.drso_id = dsth.dsth_drso_id INNER JOIN ( SELECT p.ppnt_cde , pd.ppds_drso_id FROM relPeilpuntdruksonde pd INNER JOIN tblPeilpunt p ON p.ppnt_id = pd.ppds_ppnt_id WHERE p.ppnt_cde like 'KAMP%')ppds ON ppds.ppds_drso_id = dsth.dsth_drso_id WHERE dsth.dsth_dsha_cde = 'PROG’   Documentation   Rename your output fields when necessary. It makes the output comprehensible for users that are not familiar with the datamodel.\n--Good SELECT mpnt_cde AS meetpunt , mpnt_mpst_cde AS meetpuntstatus , mpnt_mptp_cde AS meetpuntype FROM tblmeetpunt --Bad SELECT mpnt_cde , mpnt_mpst_cde , mpnt_mptp_cde FROM tblmeetpunt   Use /** and **/ for comment blocks, e.g. a description at the beginning of the query\n--Example /** Deze query haalt naam en gemeente op van de werknemers boven de 50 jaar CreateDate: 21/05/2015 Created by: Bill Gates **/ SELECT p.name , a.city , p.age FROM person p LEFT OUTER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 AND p.firstname = ‘Piet’   Use \u0026ndash; for small comments in the query\n--Example SELECT mpnt_cde AS meetpunt , mpnt_mpst_cde AS meetpuntstatus , mpnt_mptp_cde AS meetpuntype FROM tblmeetpunt WHERE mpnt_mpst_cde = ‘VLD’ --only validated points   Tips and tricks   Use TOP 10 (or LIMIT 10 in Postgres) when designing queries with a large resultset (taking a long time to run). It saves a lot of time in the design stage.\n  Use 1 = 1 as the first line of the WHERE clause. This allows you to easily turn on and off all restrictions while designing your query. Beware of OR: where 1 = 1 OR age \u0026gt; 50 doesn’t mean that everybody is +50.\n--Example SELECT p.firstname , p.lastname , a.city FROM person p INNER JOIN address a ON a.personid = p.id WHERE 1 = 1 --AND p.age \u0026gt; 50 AND a.city like ‘Ar%’ --Try to turn on and off the age constraint in this case. Pretty annoying, isn’t it? SELECT p.firstname , p.lastname , a.city FROM person p INNER JOIN address a ON a.personid = p.id WHERE p.age \u0026gt; 50 AND a.city like ‘Ar%’   Ordering the output records can be done by explicitly using the name of the fields or by using the field number as you have defined them in the SELECT statement\n--Example SELECT p.firstname , p.lastname , a.city FROM person p INNER JOIN address a ON a.personid = p.id ORDER BY a.city, p.lastname --This will return the same result SELECT p.firstname , p.lastname , a.city FROM person p INNER JOIN address a ON a.personid = p.id ORDER BY 3, 2   For advanced users: use Common Table Expressions instead of complex subqueries. It makes your query modular and easier to understand for other users\n  ","href":"/tutorials/sql_styleguide/","title":"Styleguide SQL-scripts"},{"content":"","href":"/tags/effectclass/","title":"effectclass"},{"content":"See the vignette/tutorial at https://effectclass.netlify.app/articles/classification.html\n","href":"/tutorials/vignette_effectclass_classification/","title":"effectclass: Standardised classification of effects based on their uncertainty"},{"content":"See the vignette/tutorial at https://inbo.github.io/n2khab/articles/v022_example.html\n","href":"/tutorials/vignette_n2khab_v022_example/","title":"n2khab: Demonstrating package \u0026 data setup and handling of sf objects: a case with read_soilmap()"},{"content":"Naming   Use lowercase for repository, directory, and file names. For R-related files, use uppercase R.\n https://github.com/inbo/data-publication ../tutorials/gis/leaflet-R.Rmd    Use dash (-) to separate words in directory and file names. Don\u0026rsquo;t use underscores.\n .../datasets/bird-tracking-gull-occurrences/mapping/dwc-occurrence.sql    Avoid the use of dash (-) in the name of repositories that you intend for R package development (it is OK to use a dash in repository names for other purposes). Especially if you intend to publish the package at CRAN at some point. CRAN demands package names to comply with the following: \u0026ldquo;contain only (ASCII) letters, numbers and dot, have at least two characters and start with a letter and not end in a dot\u0026rdquo;. Few packages include a dot in their name, so avoid this too.\n  READMEs ","href":"/tutorials/styleguide_repos/","title":"Styleguide new git repositories"},{"content":"This tutorial uses a few basic functions from the dplyr and sf packages. While only a few functions are used, you can use the previous hyperlinks to access the tutorials (vignettes) of these packages for more functions and information.\noptions(stringsAsFactors = FALSE) library(tidyverse) library(sf) library(inborutils) You will find a bit more background about ‘why and what’, regarding the considered open standards, in a separate post on this website.\nIn short, the GeoPackage and GeoJSON formats are ideal for exchange, publication, interoperability \u0026amp; durability and to open science in general.\nThe below table compares a few vector formats that are currently used a lot. This tutorial focuses on the open formats.\n   Property GeoPackage GeoJSON RFC7946 Shapefile ESRI geodatabase     Open standard? yes yes no no   Write support by GDAL (OGR) yes yes yes no   Supported OS cross-platform cross-platform cross-platform Windows   Extends non-spatial format: SQLite JSON dBase IV MS Access (for personal gdb)   Text or binary? binary text binary binary   Number of files 1 1 3 or more 1 (personal gdb) / many (file gdb)   Inspect version’s differences in git version control? no yes (but care must be taken) no no   Can store multiple layers? yes no no yes   Multiple geometry types allowed per layer? yes yes no yes   Coordinate reference system (CRS) in file same as input CRS WGS84 same as input CRS same as input CRS    How to make and use GeoPackages (*.gpkg) Making a GeoPackage from a geospatial sf object in R As an example, we download a geospatial layer of Special Areas of Conservation in Flanders (version sac_2013-01-18) from Zenodo:\n# meeting a great function from the \u0026#39;inborutils\u0026#39; package: download_zenodo(doi = \u0026#34;10.5281/zenodo.3386815\u0026#34;) Did you know this: you can visit a website of this dataset by just prefixing the DOI 1 with doi.org/!\nThe data source is a shapefile, in this case consisting of 6 different files. Read the geospatial data into R as an sf object, and let’s just keep the essentials (though it doesn’t matter for the GeoPackage):\nsac \u0026lt;- read_sf(\u0026#34;sac.shp\u0026#34;) %\u0026gt;% select(sac_code = GEBCODE, sac_name = NAAM, subsac_code = DEELGEBIED, polygon_id = POLY_ID) Have a look at its contents by printing the object:\nsac ## Simple feature collection with 616 features and 4 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: 22084.25 ymin: 153207.4 xmax: 258865 ymax: 243333 ## Projected CRS: BD72 / Belgian Lambert 72 ## # A tibble: 616 × 5 ## sac_code sac_name subsac_code polygon_id geometry ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;POLYGON [m]\u0026gt; ## 1 BE2100020 Heesbossen, V… BE2100020-4 1 ((180272.3 243198.7, 180275.… ## 2 BE2100020 Heesbossen, V… BE2100020-2 2 ((178655.5 241042.4, 178602.… ## 3 BE2100024 Vennen, heide… BE2100024-… 3 ((197492.4 234451.4, 197286.… ## 4 BE2100015 Kalmthoutse H… BE2100015-1 4 ((153735.8 228386, 153838.5 … ## 5 BE2100024 Vennen, heide… BE2100024-… 5 ((198272.4 234699, 198568.8 … ## 6 BE2100020 Heesbossen, V… BE2100020-6 6 ((181098 233705.3, 181395.1 … ## 7 BE2100024 Vennen, heide… BE2100024-… 7 ((199185.8 233540.2, 199122.… ## 8 BE2100024 Vennen, heide… BE2100024-… 8 ((199553.4 233061.2, 199141.… ## 9 BE2100024 Vennen, heide… BE2100024-5 9 ((192190.9 232648.7, 192196 … ## 10 BE2100024 Vennen, heide… BE2100024-2 10 ((187597 231264.9, 187549.3 … ## # … with 606 more rows  To write the GeoPackage, we just use the GPKG driver of the powerful GDAL library (supporting most open and some closed formats), which can be elegantly accessed through sf::st_write():\nsac %\u0026gt;% st_write(\u0026#34;sac.gpkg\u0026#34;) ## Writing layer `sac' to data source `sac.gpkg' using driver `GPKG' ## Writing 616 features with 4 fields and geometry type Polygon.  Is that all?\nYES :-)\nReally?\nYES :-)\nWell, hmmm, if you really want to know a little bit more…\nA GeoPackage can contain many layers. So, it is good practice to explicitly define the layer name within the GeoPackage (above, it was automatically called ‘sac’). For example:\nsac %\u0026gt;% st_write(\u0026#34;sac.gpkg\u0026#34;, layer = \u0026#34;special_areas_conservation\u0026#34;, delete_dsn = TRUE) ## Deleting source `sac.gpkg' using driver `GPKG' ## Writing layer `special_areas_conservation' to data source ## `sac.gpkg' using driver `GPKG' ## Writing 616 features with 4 fields and geometry type Polygon.  Note, delete_dsn was set as TRUE to replace the whole GeoPackage. (There is also a delete_layer argument to overwrite an existing layer with the same name.)\nLet’s extract a selection of features from the special_areas_conservation layer, and add it as a second layer into the GeoPackage file:\nsac %\u0026gt;% filter(str_detect(sac_name, \u0026#34;Turnhout\u0026#34;)) %\u0026gt;% # only polygons having \u0026#39;Turnhout\u0026#39; in their name field st_write(\u0026#34;sac.gpkg\u0026#34;, layer = \u0026#34;turnhout\u0026#34;) ## Writing layer `turnhout' to data source `sac.gpkg' using driver `GPKG' ## Writing 16 features with 4 fields and geometry type Polygon.  So yes, adding layers to a GeoPackage is done simply by st_write() again to the same GeoPackage file (by default, delete_dsn is FALSE), and defining the new layer’s name.\nSo, which layers are available in the GeoPackage?\nst_layers(\u0026#34;sac.gpkg\u0026#34;) ## Driver: GPKG ## Available layers: ## layer_name geometry_type features fields ## 1 special_areas_conservation Polygon 616 4 ## 2 turnhout Polygon 16 4  You see?\nReading a GeoPackage file Can it become more simple than this?\n# (note: the \u0026#39;layer\u0026#39; argument is unneeded if there\u0026#39;s just one layer) sac_test \u0026lt;- st_read(\u0026#34;sac.gpkg\u0026#34;, layer = \u0026#34;special_areas_conservation\u0026#34;) ## Reading layer `special_areas_conservation' from data source ## `/media/floris/DATA/git_repositories/tutorials/content/tutorials/spatial_standards_vector/sac.gpkg' ## using driver `GPKG' ## Simple feature collection with 616 features and 4 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: 22084.25 ymin: 153207.4 xmax: 258865 ymax: 243333 ## Projected CRS: BD72 / Belgian Lambert 72  Ready!\nst_read() is a function of the great sf package – hence the result is an sf object again.\nAlso other geospatial software will (or should) be able to open the GeoPackage format. It is an open standard, after all!\nHow to make and use GeoJSON files (*.geojson) Making a GeoJSON file from a geospatial sf object in R As another example, let’s download a shapefile of stream habitat 3260 in Flanders (version 1.6):\ndownload_zenodo(doi = \u0026#34;10.5281/zenodo.3386246\u0026#34;) Again: you can visit a website of this dataset by just prefixing the DOI with doi.org/!\nThe data source is a shapefile again, in this case consisting of 4 different files. Similar as above, we read the geospatial data into R as an sf object and select a few attributes to work with:\nhabitatstreams \u0026lt;- read_sf(\u0026#34;habitatstreams.shp\u0026#34;) %\u0026gt;% select(river_name = NAAM, source = BRON) habitatstreams ## Simple feature collection with 560 features and 2 fields ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: 33097.92 ymin: 157529.6 xmax: 254039 ymax: 243444.6 ## Projected CRS: BD72 / Belgian Lambert 72 ## # A tibble: 560 × 3 ## river_name source geometry ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;LINESTRING [m]\u0026gt; ## 1 WOLFPUTBEEK VMM (127857.1 167681.2, 127854.9 167684.5, 127844 167688… ## 2 OUDE KALE VMM (95737.01 196912.9, 95732.82 196912.4, 95710.38 1969… ## 3 VENLOOP EcoInv (169352.7 209314.9, 169358.8 209290.5, 169326.2 2092… ## 4 VENLOOP EcoInv (169633.6 209293.5, 169625 209289.2, 169594.4 209321… ## 5 KLEINE NETE EcoInv (181087.1 208607.2, 181088.6 208608.1, 181089 208608… ## 6 KLEINE NETE EcoInv (180037.4 208360.4, 180038.3 208377.5, 180038.3 2083… ## 7 KLEINE NETE EcoInv (180520 208595.7, 180540.5 208607.4, 180541.2 208607… ## 8 KLEINE NETE EcoInv (187379.9 209998.8, 187381.3 209998.5, 187381.6 2099… ## 9 RAAMDONKSEBEEK extrapol (183545.5 192409, 183541.9 192406.7, 183541.9 192403… ## 10 KLEINE NETE EcoInv (183516.4 208261.7, 183567.3 208279.2, 183567.3 2082… ## # … with 550 more rows  Nowadays, it is recommended to use the more recent and strict RFC7946 implementation of GeoJSON. The previous ‘GeoJSON 2008’ implementation is now obsoleted (see the post on this tutorials website for a bit more background).\nThe RFC7946 standard is well supported by GDAL’s GeoJSON driver, however GDAL must be given the explicit option RFC7946=YES in order to use it already 2.\nWrite the GeoJSON file as follows:\nhabitatstreams %\u0026gt;% st_write(\u0026#34;habitatstreams.geojson\u0026#34;, layer_options = \u0026#34;RFC7946=YES\u0026#34;) ## Writing layer `habitatstreams' to data source ## `habitatstreams.geojson' using driver `GeoJSON' ## options: RFC7946=YES ## Writing 560 features with 2 fields and geometry type Line String.  Done creating!\nDo I look good? Hey wait, wasn’t a GeoJSON file just a text file?\nIndeed.\nSo I can just open it as a text file to get an idea of its contents?\nWell seen :-)\nHence, also use it in versioned workflows?\nDidn’t hear that. (Cool, though…)\nLet’s just look at the top 7 lines of the file:\n{ \u0026quot;type\u0026quot;: \u0026quot;FeatureCollection\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;habitatstreams\u0026quot;, \u0026quot;features\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;Feature\u0026quot;, \u0026quot;properties\u0026quot;: { \u0026quot;river_name\u0026quot;: \u0026quot;WOLFPUTBEEK\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;VMM\u0026quot; }, \u0026quot;geometry\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;LineString\u0026quot;, \u0026quot;coordinates\u0026quot;: [ [ 4.05452, 50.8191468 ], [ 4.0544892, 50.8191765 ], [ 4.0543343, 50.8192157 ], [ 4.0541273, 50.8193985 ], [ 4.0540904, 50.8196061 ], [ 4.0541332, 50.8199122 ] ] } }, { \u0026quot;type\u0026quot;: \u0026quot;Feature\u0026quot;, \u0026quot;properties\u0026quot;: { \u0026quot;river_name\u0026quot;: \u0026quot;OUDE KALE\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;VMM\u0026quot; }, \u0026quot;geometry\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;LineString\u0026quot;, \u0026quot;coordinates\u0026quot;: [ [ 3.5944035, 51.0797983 ], [ 3.5943438, 51.0797931 ], [ 3.5940242, 51.0797446 ], [ 3.593868, 51.0797255 ], [ 3.5938178, 51.079713 ], [ 3.5937577, 51.0796981 ], [ 3.5935501, 51.0796061 ], [ 3.5933518, 51.0794966 ], [ 3.5932562, 51.0794359 ], [ 3.5932225, 51.0794097 ], [ 3.5931799, 51.0793764 ], [ 3.5931636, 51.0793498 ] ] } }, { \u0026quot;type\u0026quot;: \u0026quot;Feature\u0026quot;, \u0026quot;properties\u0026quot;: { \u0026quot;river_name\u0026quot;: \u0026quot;VENLOOP\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;EcoInv\u0026quot; }, \u0026quot;geometry\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;LineString\u0026quot;, \u0026quot;coordinates\u0026quot;: [ [ 4.6455959, 51.1934881 ], [ 4.6456818, 51.1932687 ], [ 4.6452152, 51.1932051 ], [ 4.6451504, 51.1931441 ], [ 4.6451933, 51.1928692 ] ] } },  You can see it basically lists the feature attributes and the coordinates of the lines’ vertices, with each feature starting on a new line.\nCompare the coordinates with those of the sf object habitatstreams above: the data have automatically been transformed to WGS 84!\nNote: in order to be still manageable (text file size, usage in versioning systems) it seems wise to use GeoJSON for more simple cases – points and rather simple lines and polygons – and use the binary GeoPackage format for larger (more complex) cases.\nReading a GeoJSON file Just do this:\nhabitatstreams_test \u0026lt;- st_read(\u0026#34;habitatstreams.geojson\u0026#34;) ## Reading layer `habitatstreams' from data source ## `/media/floris/DATA/git_repositories/tutorials/content/tutorials/spatial_standards_vector/habitatstreams.geojson' ## using driver `GeoJSON' ## Simple feature collection with 560 features and 2 fields ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: 2.698642 ymin: 50.7282 xmax: 5.855562 ymax: 51.49979 ## Geodetic CRS: WGS 84  Same story as for the GeoPackage: other geospatial software will (or should) be able to open the GeoJSON format as well, as it’s an open and well established standard.\nFrom the message of st_read() you can see the CRS is WGS 84 (EPSG-code 4326) - this is always expected when reading a GeoJSON file.\nIf you want to transform the data to another CRS, e.g. Belgian Lambert 72 (EPSG-code 31370), use sf::st_transform():\nhabitatstreams_test %\u0026gt;% st_transform(31370) ## Simple feature collection with 560 features and 2 fields ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: 33097.92 ymin: 157529.6 xmax: 254039 ymax: 243444.6 ## Projected CRS: BD72 / Belgian Lambert 72 ## First 10 features: ## river_name source geometry ## 1 WOLFPUTBEEK VMM LINESTRING (127857.1 167681... ## 2 OUDE KALE VMM LINESTRING (95737.01 196912... ## 3 VENLOOP EcoInv LINESTRING (169352.7 209314... ## 4 VENLOOP EcoInv LINESTRING (169633.6 209293... ## 5 KLEINE NETE EcoInv LINESTRING (181087.1 208607... ## 6 KLEINE NETE EcoInv LINESTRING (180037.4 208360... ## 7 KLEINE NETE EcoInv LINESTRING (180519.9 208595... ## 8 KLEINE NETE EcoInv LINESTRING (187379.9 209998... ## 9 RAAMDONKSEBEEK extrapol LINESTRING (183545.6 192409... ## 10 KLEINE NETE EcoInv LINESTRING (183516.4 208261...    DOI = Digital Object Identifier. See https://www.doi.org.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Though GeoJSON 2008 is obsoleted, the now recommended RFC7946 standard is still officially in a proposal stage. That is probably the reason why GDAL does not yet default to RFC7946. A somehow confusing stage, it seems.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","href":"/tutorials/spatial_standards_vector/","title":"How to use open vector file formats in R: GeoPackage \u0026 GeoJSON"},{"content":"Some inspiration for this post came from the beautiful books of Lovelace et al. (2020), Pebesma \u0026amp; Bivand (2019) and Hijmans (2019), and from various websites.\nWhy use open standards?  Open file standards ease collaboration, portability and compatibility between users, machines and applications. Their (file) structure is fully documented.  Consequently, scientists and programmers can build new software / packages and make innovations that use these standards, while maintaining interoperability with existing applications. And, there is a much higher chance that your data will still be readable in a hundred years from now. The standard’s open documentation makes it relatively easy to build tools that can read an ancient open-standard file!    Luckily, quite a list of open standards is available! Below, some powerful and widely-used single-file formats are introduced. Single-file data sources are readily amenable to exchange and publication.\nI see you can’t wait to start practicing, so you can also head straight over to the tutorial on vector formats and the tutorial on raster formats! In these tutorials, a comparison table of vector/raster file formats is also presented.\nA few words on the GDAL library GDAL (Geospatial Data Abstraction Library) is by far the most used collection of open-source drivers for:\n a lot of raster formats; a lot of vector formats.  In other words, it is the preferred workhorse for reading and writing many geospatial file formats, used in the background by a lot of geospatial applications . Using GDAL is the easiest way to conform to open standards.\nSo, in R we use packages that use GDAL in the background, such as rgdal, sp, sf, raster and stars.\nThe GeoPackage file format  Its website is https://www.geopackage.org. It is a standardized implementation of an SQLite database for geospatial data. Hence, a GeoPackage is a binary file (filename.gpkg). It shares this property with shapefiles, which however pose multiple limitations,1 so the GeoPackage is a more than suitable replacement. The GeoPackage can store one or multiple vector layers (points, lines, polygons and related feature types). Besides vector data, it can also store raster data or extra standalone tables. These properties make it somehow comparable to the ‘personal geodatabase’ of ArcGIS – ESRI’s closed, Windows-only format.2 The GeoPackage standard is maintained by the Open Geospatial Consortium (OGC), which stands out as a reference when it comes to open geospatial standards.  The GeoJSON file format  One GeoJSON file (filename.geojson) contains one vector layer. Note that one vector layer can combine different feature geometry types, e.g. points and linestrings. JSON itself is a common and straightforward open data format. It is a text file readable both by humans and machines (see the tutorial for an example). GeoJSON adds the necessary specification to JSON for standardized storage of geographic feature data, but it is still a plain JSON text file. The GeoJSON standard is maintained by the Internet Engineering Task Force (IETF), a large open standards organization that develops Internet standards under the auspices of the Internet Society. Although the previous version of the GeoJSON standard – GeoJSON 2008 – is still a lot in use, it is obsoleted and a new version RFC7946 is establishing.  This version is strict about the coordinate reference system (CRS) – it is always WGS 84 – and it also differs on a few other aspects (such as the recommendation for applications not to inflate decimal coordinate precision). RFC7946 solves the problem that quite a few libraries – including GDAL – simply assumed WGS 84 in GeoJSON 2008 (without checking or transforming), even though WGS 84 was not a requirement of GeoJSON 2008 (it did support an explicit crs declaration). This resulted in inconveniences (cf. this post in the sf-repository). A specific section in the documentation of GDAL’s GeoJSON driver gives a summary of the differences between both GeoJSON versions.   While GDAL by default still follows the GeoJSON 2008 format,3 RFC7946 is supported by the option RFC7946=YES. Here, reprojection to WGS 84 will happen automatically. It applies 7 decimal places for coordinates, i.e. approximately 1 cm. Given the advantages, we advise to explicitly use RFC7946. Several functions in R allow the user to provide options that are passed to GDAL, so we can ask to deliver RFC7946 (see the tutorial). In order to keep it manageable (text file size, usage in versioning systems4 ) it can be wise to use GeoJSON for more simple cases (points and rather simple lines and polygons), and use the binary GeoPackage format for larger (more complex) cases.  The GeoTIFF file format  GeoTIFF is the preferred single-file open standard for raster data. It adheres to the open TIFF specification; hence it is a TIFF image file (filename.tif). It uses a small set of reserved TIFF tags to store information about CRS, extent and resolution of the raster. A GeoTIFF file can contain one or multiple rasters with the same CRS, extent and resolution. The GeoTIFF standard is maintained by the Open Geospatial Consortium (OGC), which stands out as a reference when it comes to open geospatial standards.  Literature Hijmans R. (2019). Spatial Data Science with R. https://rspatial.org/.\n Lovelace R., Nowosad J. \u0026amp; Muenchow J. (2020). Geocomputation with R. https://geocompr.robinlovelace.net.\n Pebesma E. \u0026amp; Bivand R. (2019). Spatial Data Science. https://www.r-spatial.org/book.\n    Some problems with shapefiles are: they’re not an open format, they consist of multiple files and they have restrictions regarding file size, column name length, number of columns and the feature types that can be accommodated.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Note that personal geodatabases have their size limited to 250-500 MB; a GeoPackage can have a size of about 140 TB if the filesystem can handle it.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Though GeoJSON 2008 is obsoleted, the now recommended RFC7946 standard is still officially in a proposal stage. That is probably the reason why GDAL does not yet default to RFC7946. A somehow confusing stage, it seems.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n When versioning GeoJSON files, mind the order of your data when rewriting them: reordering could produce large diffs. Interested in combining GeoJSON and GitHub? Surprise yourself!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","href":"/articles/geospatial_standards/","title":"Meet some popular open geospatial standards!"},{"content":"Here are the slides of a presentation about the effectclass package.\n","href":"/tutorials/r_effectclass/","title":"Classification and visualisation of estimates and their uncertainty"},{"content":"See the vignette/tutorial at https://effectclass.netlify.app/articles/visualisation.html\n","href":"/tutorials/vignette_effectclass_visualisation/","title":"effectclass: Visualising effects"},{"content":"See the vignette/tutorial at https://inbo.github.io/watina/articles/v210_chem.html\n","href":"/tutorials/vignette_watina_v210_chem/","title":"watina: Using hydrochemical data to characterize and select locations"},{"content":"See the vignette/tutorial at https://inbo.github.io/watina/articles/v110_xg3.html\n","href":"/tutorials/vignette_watina_v110_xg3/","title":"watina: Using XG3 values to characterize and select locations"},{"content":"","href":"/authors/dimitribrosens/","title":"dimitribrosens"},{"content":"Introduction This tutorial will explain how you can match a list of scientific names against the GBIF backbone taxonomy.\nIt is important that you have the most recent version of inborutils installed and available:\nremotes::install_github(\u0026#34;inbo/inborutils\u0026#34;) # install inborutils library(tidyverse) # To do datascience library(rgbif) # To lookup names in the GBIF backbone taxonomy library(inborutils) # To wrap GBIF API data library(knitr) Read data file containing the scientific names Read file containing the scientific names you want to check against the GBIF taxonomic backbone:\nspecies_df \u0026lt;- read_csv(\u0026#34;https://raw.githubusercontent.com/inbo/inbo-pyutils/master/gbif/gbif_name_match/sample.csv\u0026#34;, trim_ws = TRUE, col_types = cols()) Take a look at the data:\nkable(species_df)    name kingdom euConcernStatus     Alopochen aegyptiaca Animalia under consideration   Cotoneaster ganghobaensis Plantae NA   Cotoneaster hylmoei Plantae NA   Cotoneaster x suecicus Plantae NA   Euthamia graminifolia Plantae under preparation    Request taxonomic information Given a data.frame, you can match the column containing the scientific name against GBIF Backbone Taxonomy, using the gbif_species_name_match function from the inborutils package. You need to pass a data.frame, df and a column name, name:\nspecies_df_matched \u0026lt;- gbif_species_name_match(df = species_df, name = \u0026#34;name\u0026#34;) ## [1] \u0026quot;All column names present\u0026quot;  As the name argument has \u0026quot;name\u0026quot; as default value, the code above is equivalent to:\nspecies_df_matched \u0026lt;- gbif_species_name_match(species_df) or using pipe %\u0026gt;%:\nspecies_df_matched \u0026lt;- species_df_matched %\u0026gt;% gbif_species_name_match() By default gbif_species_name_match returns the following GBIF fields: usageKey, scientificName, rank, order, matchType, phylum, kingdom, genus, class, confidence, synonym, status, family.\nTake a look at the updated data:\nkable(species_df_matched)    name kingdom euConcernStatus usageKey scientificName rank order matchType phylum kingdom1 genus class confidence synonym status family     Alopochen aegyptiaca Animalia under consideration 2498252 Alopochen aegyptiaca (Linnaeus, 1766) SPECIES Anseriformes EXACT Chordata Animalia Alopochen Aves 98 FALSE ACCEPTED Anatidae   Cotoneaster ganghobaensis Plantae NA 3025989 Cotoneaster ganghobaensis J.Fryer \u0026amp; B.HylmÃ¶ SPECIES Rosales EXACT Tracheophyta Plantae Cotoneaster Magnoliopsida 98 FALSE ACCEPTED Rosaceae   Cotoneaster hylmoei Plantae NA 3025758 Cotoneaster hylmoei K.E.Flinck \u0026amp; J.Fryer SPECIES Rosales EXACT Tracheophyta Plantae Cotoneaster Magnoliopsida 98 FALSE ACCEPTED Rosaceae   Cotoneaster x suecicus Plantae NA 3026040 Cotoneaster suecicus G.Klotz SPECIES Rosales EXACT Tracheophyta Plantae Cotoneaster Magnoliopsida 98 FALSE ACCEPTED Rosaceae   Euthamia graminifolia Plantae under preparation 3092782 Euthamia graminifolia (L.) Nutt. SPECIES Asterales EXACT Tracheophyta Plantae Euthamia Magnoliopsida 98 FALSE ACCEPTED Asteraceae    Notice that GBIF fields whose name is already used as column name are automatically renamed by adding suffix 1. In our case, input data.frame species_df contains already a column called kingdom. The GBIF kingdom values are returned in column kingdom1:\nspecies_df_matched %\u0026gt;% select(kingdom, kingdom1) ## # A tibble: 5 x 2 ## kingdom kingdom1 ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Animalia Animalia ## 2 Plantae Plantae ## 3 Plantae Plantae ## 4 Plantae Plantae ## 5 Plantae Plantae  You can also specify which GBIF fields you would like to have:\nspecies_df %\u0026gt;% gbif_species_name_match( gbif_terms = c( \u0026#39;scientificName\u0026#39;, \u0026#39;family\u0026#39;, \u0026#39;order\u0026#39;, \u0026#39;rank\u0026#39;, \u0026#39;matchType\u0026#39;, \u0026#39;confidence\u0026#39;, \u0026#39;status\u0026#39;)) %\u0026gt;% kable() ## [1] \u0026quot;All column names present\u0026quot;     name kingdom euConcernStatus scientificName family order rank matchType confidence status     Alopochen aegyptiaca Animalia under consideration Alopochen aegyptiaca (Linnaeus, 1766) Anatidae Anseriformes SPECIES EXACT 98 ACCEPTED   Cotoneaster ganghobaensis Plantae NA Cotoneaster ganghobaensis J.Fryer \u0026amp; B.HylmÃ¶ Rosaceae Rosales SPECIES EXACT 98 ACCEPTED   Cotoneaster hylmoei Plantae NA Cotoneaster hylmoei K.E.Flinck \u0026amp; J.Fryer Rosaceae Rosales SPECIES EXACT 98 ACCEPTED   Cotoneaster x suecicus Plantae NA Cotoneaster suecicus G.Klotz Rosaceae Rosales SPECIES EXACT 98 ACCEPTED   Euthamia graminifolia Plantae under preparation Euthamia graminifolia (L.) Nutt. Asteraceae Asterales SPECIES EXACT 98 ACCEPTED    The function inborutils::gbif_species_name_match is a wrapper around rgbif::name_backbone, so you can pass any argument of name_backbone. For example, you can set strict = TRUE to fuzzy match only the given names, but never a taxon in the upper classification:\nspecies_df %\u0026gt;% gbif_species_name_match(strict = TRUE) %\u0026gt;% kable() ## [1] \u0026quot;All column names present\u0026quot;     name kingdom euConcernStatus usageKey scientificName rank order matchType phylum kingdom1 genus class confidence synonym status family     Alopochen aegyptiaca Animalia under consideration 2498252 Alopochen aegyptiaca (Linnaeus, 1766) SPECIES Anseriformes EXACT Chordata Animalia Alopochen Aves 99 FALSE ACCEPTED Anatidae   Cotoneaster ganghobaensis Plantae NA 3025989 Cotoneaster ganghobaensis J.Fryer \u0026amp; B.HylmÃ¶ SPECIES Rosales EXACT Tracheophyta Plantae Cotoneaster Magnoliopsida 99 FALSE ACCEPTED Rosaceae   Cotoneaster hylmoei Plantae NA 3025758 Cotoneaster hylmoei K.E.Flinck \u0026amp; J.Fryer SPECIES Rosales EXACT Tracheophyta Plantae Cotoneaster Magnoliopsida 99 FALSE ACCEPTED Rosaceae   Cotoneaster x suecicus Plantae NA 3026040 Cotoneaster suecicus G.Klotz SPECIES Rosales EXACT Tracheophyta Plantae Cotoneaster Magnoliopsida 99 FALSE ACCEPTED Rosaceae   Euthamia graminifolia Plantae under preparation 3092782 Euthamia graminifolia (L.) Nutt. SPECIES Asterales EXACT Tracheophyta Plantae Euthamia Magnoliopsida 98 FALSE ACCEPTED Asteraceae    These are all accepted parameters of name_backbone: ‘rank’, ‘kingdom’, ‘phylum’, ‘class’, ‘order’, ‘family’, ‘genus’, ‘strict’, ‘verbose’, ‘start’, ‘limit’, ‘curlopts’. See ?name_backbone for more details.\nFor Python users, there is a similar (but no longer maintained) function in inbo-pyutils.\n","href":"/tutorials/r_gbif_name_matching/","title":"Match scientific names with the GBIF Backbone Taxonomy"},{"content":"See the vignette/tutorial at https://inbo.github.io/n2khab/articles/v020_datastorage.html\n","href":"/tutorials/vignette_n2khab_v020_datastorage/","title":"n2khab: Where to store the N2KHAB input data sources"},{"content":"See the vignette/tutorial at https://inbo.github.io/inborutils/articles/gbif_name_match.html\n","href":"/tutorials/vignette_inborutils_gbif_name_match/","title":"inborutils: GBIF scientific name matching"},{"content":"General  All SQL objects are prefixed, except for databases, schemas, tables and columns and users/logins. The prefixes used depend on the type of object:     Prefix Object type     PK_ primary key constraint   FK_ foreign key constraint   CU_ unique key constraint   CC_ check constraint   CD_ default   IN_ index   VW_ view   TR_ trigger   SQ_ sequence   DR_ database role     choose English or Dutch for the naming of objects and be consistent  Database  The database is created by the DBA. Database name is provided by the DBA. For a list of DB\u0026rsquo;s following the new name conventions, see List of databases.  Schemas  dbo only. unless special requirements.  Tables  Meaningful name . CamelCase. Singular No pre- or suffixes. [ a-b ], [ A-B ], [ 0-9 ].  Columns  Meaningful name. CamelCase. No pre- or suffixes. [ a-z ], [ A-Z ], [ 0-9 ] and [ -,_ ] if necessary. specify the kind of column at the end of the name in case of potential doubt, e.g. CreateUser, UpdateDate, BiotopeKey, TaxonCode, SpeciesID. do not repeat the table name in the column name; a column only exists in the context of the table. primary key column  always 1 per table, non functional, datatype: int or bigint, not null, identity (autonumber), name: ID   foreign key column  name: table name + primary key column (ID) of column to which the foreign key refers, e.g. Measurement.SpeciesID refers to Species.ID clearify in case of multiple relations between 2 tables, e.g. Observation.ObserverPersonID and Observation.ValidatorPersonID both refer to Person.ID   minimum columns in each table (see Code Snippets):  ID CreateUser CreateDate UpdateUser UpdateDate RV    Primary key constraints  always. single column, non-composite. non functional. naming: PK_\u0026lt;table name\u0026gt;, e.g.:  PK_Species for Species table, PK_Measurement for Measurement table    Foreign key constraints  use db referential integrity when necessary and when possible, do not rely on application logic solely. single column, non composite. naming: FK_\u0026lt;referring table name\u0026gt;_\u0026lt;referred table name\u0026gt;(_\u0026lt;discriminator\u0026gt;), e.g.:  FK_Measurement_Species for foreign key from Measurement table to Species table, FK_Observation_Person_Observer and FK_Observation_Person_Validator for foreign keys from Observation table to Person table ,   good practice to add an index on foreign key columns; this is not autmatically done; use the index naming logic: IN_\u0026lt;table name\u0026gt;_\u0026lt;specification\u0026gt;  Unique key constraints  always use constraints instead of indexes to enforce uniqueness. can be composite. naming: CU_\u0026lt;table name\u0026gt;_\u0026lt;functional description\u0026gt;, e.g.:  CU_ObservationPerson_ObservationPerson for unique constraint on ObservationID and PersonID columns in ObservationPerson table, CU_SampleOnvolledigReden_SampleOnvolledigReden for unique constraint on SampleID and OnvolledigRedenID columns in SampleOnvolledigReden table.    Check constraints  not often used, it might be better to have this kind of logic in the business layer. naming CC_\u0026lt;table name\u0026gt;_\u0026lt;functional description\u0026gt;, e.g.:  CC_Measurement_PositiveNumber for check constraint that validates the Number column in Measurement table being positive.    Defaults  always provide a default value for bit-columns. always provide a default value for standard columns CreateDate an CreateUser, resp GETDATE() and SUSER_NAME(). naming: CD_\u0026lt;table name\u0026gt;_\u0026lt;column name\u0026gt;, e.g.  CD_Measurement_CreateDate for default value on CreateDate column in Measurement table.   This code example adds defaults for columns CreateUser and CreateDate of table Method:   ALTER TABLE Method ADD CONSTRAINT CD_Method_CreateUser DEFAULT (SUSER_NAME()) FOR CreateUser , CONSTRAINT CD_Method_CreateDate DEFAULT (GETDATE()) FOR CreateDate Triggers  only used for low level database logic. naming: TR_\u0026lt;table name\u0026gt;_\u0026lt;trigger action\u0026gt;, where trigger action can be AU (after update), AI (after insert), AD (after delete) or any combination (A(U)(I)(D)) or IO (instead of), e.g.  TR_Measurement_AU for after update trigger on Measurement table .   This code example creates an after update trigger for table Measurement. Columns UpdateDate and UpdateUser are updated on each update:  CREATE TRIGGER [dbo].[TR_Measurement_AU] ON [dbo].[Measurement] AFTER UPDATE AS BEGIN SET NOCOUNT ON; UPDATE t SET t.UpdateDate = GETDATE() , t.UpdateUser = SUSER_SNAME() FROM inserted i INNER JOIN Measurement t ON t.ID = i.ID; END Indexes  Indexes are used to speed up lookups and sorting. Do not use indexes to enforce uniqueness; use unique key constraint instead, which creates an index itself. Exception to this: when uniqueness is required in combination with a filtering criterium. As a rule of thumb create an index on each foreign key column. As a rule of thumb make sure that the primary key constraint uses a clustered index. Clustered indexes are created on DATA filegroup; non-clustered indexes on INDEXES filegroup. Leave the rest of the indexing up to the DBA. In order to help the DBA in choosing the correct indexes, explain him the (search and sorting) behaviour of the application. Performance issues might require the creation (or deletion) of indexes during the lifetime of the db. Indexes do not contribute to the versioning of the database. Naming: IN_\u0026lt;table name\u0026gt;_\u0026lt;specification\u0026gt; where specification describes function of index or columns included IN_Measurement_MeasurementStatusID for an index on the MeasurementStatusID column of the Measurement table. This example creates an index on the SpeciesID column of the Measurement table. Non-clustered indexes are always created on the INDEXES filegroup:  CREATE NONCLUSTERED INDEX IN_Measurement_SpeciesID ON Measurement(SpeciesID) ON INDEXES;  Example on how to enforce the primary key using a clustered index. Clustered indexes ares always created on the DATA filegroup:  ALTER TABLE Measurement ADD CONSTRAINT PK_Measurement PRIMARY KEY CLUSTERED (ID) ON DATA; Views  Stored queries. Views can be updatable. For creation of indexed views (e.g. on calculated columns) consult the DBA. Don\u0026rsquo;t use ORDER BY in views. Naming: VW_\u0026lt;logical name\u0026gt;, e.g.  VW_ActiveMesearumentStatus, retrieves the currently active measurement statusses    Synonyms  Do not use synonyms; they get the databases intertwined.  Stored procedures  no guidelines yet  Functions  no guidelines yet  Sequences  Use IDENTITY column instead of sequence to get an auto-increment behaviour at the table level. Use sequence to get an auto-increment behaviour across multiple tables (rare). Naming: SQ_\u0026lt;logical name\u0026gt;  Security  DBA\u0026rsquo;s task to implement security. Good to know how it\u0026rsquo;s normally implemented. Permissions are granted to database roles and users are assigned to database roles; the user gets the permissions of the database role to which it is assigned. Permissions are never granted to users When possible a user should be assigned to 1 database role only  Database Roles  following custom database roles per database:  DR_Admin: has dba permissions on the database; dba can be implement this using following steps  grant create permissions for all objects the dba wants to allow to be created (at database level), eg create table   grant alter any schema (at database level)     DR_\u0026lt;schema\u0026gt;_Admin: has dba permissions on the schema  of the database; dba can be implement this using following steps  grant create permissions for all objects the dba wants to allow to be created (at database level), eg create table     grant alter (at \u0026lt;schema\u0026gt; level)   DR_Reader: has readonly (R) permissions on all tables and views DR_App\u0026lt;Application name\u0026gt;: has application specific permissions: developer determines which permissions should be applied on the individual tables DR_Rpt\u0026lt;Reporting name\u0026gt;   When required extra database roles can be provided  Users / Logins  The terms login (server level) and user (database level) are separate concepts in SQL Server; it\u0026rsquo;s the login that\u0026rsquo;s used in connection string; the login is mapped to a user at the database level; we map both 1 to 1 and refer to it as user. Each component of an application that interacts with the database needs its own, unique user:  unique at INBO level dedicated user : tracebility when an application uses multiple services, which interact with the db, each service should have its own user. when an application connects in different ways to the db (e.g. transactional vs reporting) each functional component should have its own user. the (application)user is assigned to the DR_Approle.   Naming: \u0026lt;Application context\u0026gt;_\u0026lt;BusinessRole\u0026gt;, where  \u0026lt;application context\u0026gt;: a unique and clear reference to the application that\u0026rsquo;s adressing the database; for INBO-applications this might be the accepted name for the application (e.g. watinawsbusiness, the business service of Watina); for ACD-applications this might be the Nexus group ID (e.g be.inbo.wstaxon, the wstaxon service). \u0026lt;BusinessRole\u0026gt;: reporter, user, admin    Documentation  Database self explanatory through use of extended properties on  database: properties Code, Name, Description and Version tables: property Description columns: property Description   Meta about database model and deployed instances in wiki: List of databases  Code snippets  Minimum required for normal table\u0026hellip;  /* create table */ CREATE TABLE Measurement ( ID int IDENTITY(1,1) NOT NULL /* ... table specific columns , MeasurementStatusID int NOT NULL , SpeciesID int NOT NULL ...*/ , CreateUser nvarchar(150) NOT NULL , CreateDate datetime2(7) NOT NULL , UpdateUser nvarchar(150) NULL , UpdateDate datetime2(7) NULL , RV timestamp NOT NULL); GO /* add primary key constraint */ ALTER TABLE Measurement ADD CONSTRAINT PK_Measurement PRIMARY KEY CLUSTERED (ID) ON DATA; GO /* add defaults */ ALTER TABLE Measurement ADD CONSTRAINT CD_Measurement_CreateUser DEFAULT (SUSER_NAME()) FOR CreateUser , CONSTRAINT CD_Measurement_CreateDate DEFAULT (GETDATE()) FOR CreateDate; GO /* add foreign key constraints */ /* eg ALTER TABLE Measurement ADD CONSTRAINT FK_Measurement_MeasurementStatus FOREIGN KEY(MeasurementStatusID) REFERENCES MeasurementStatus (ID) , CONSTRAINT FK_Measurement_Species FOREIGN KEY(SpeciesID) REFERENCES Species (ID); GO */ /* add indexes */ /* eg on foreign key columns */ /* CREATE NONCLUSTERED INDEX IN_Measurement_MeasurementStatusID ON Measurement(MeasurementStatusID) ON INDEXES; CREATE NONCLUSTERED INDEX IN_Measurement_SpeciesID ON Measurement(SpeciesID) ON INDEXES; GO */ /* add trigger that updates audit columns */ CREATE TRIGGER TR_Measurement_AU ON Measurement AFTER UPDATE AS BEGIN SET NOCOUNT ON; UPDATE t SET t.UpdateDate = GETDATE() , t.UpdateUser = SUSER_SNAME() FROM Measurement t INNER JOIN inserted i ON i.ID = t.ID; END ; GO /* add data dictionary */ /* (you can leave this up to the dba if you have a properly formatted list of table and column names with their description */ EXEC sys.sp_addextendedproperty @name=N'Description', @value=N'The measurements of ....lorem ipsum dolor.' , @level0type=N'SCHEMA',@level0name=N'dbo' , @level1type=N'TABLE',@level1name=N'Measurement'; GO EXEC sys.sp_addextendedproperty @name=N'Description', @value=N'Measurement ID.' , @level0type=N'SCHEMA',@level0name=N'dbo' , @level1type=N'TABLE',@level1name=N'Measurement' , @level2type=N'COLUMN',@level2name=N'ID'; GO EXEC sys.sp_addextendedproperty @name=N'Description', @value=N'Creation user.' , @level0type=N'SCHEMA',@level0name=N'dbo' , @level1type=N'TABLE',@level1name=N'Measurement' , @level2type=N'COLUMN',@level2name=N'CreateUser'; GO EXEC sys.sp_addextendedproperty @name=N'Description', @value=N'Creation date.' , @level0type=N'SCHEMA',@level0name=N'dbo' , @level1type=N'TABLE',@level1name=N'Measurement' , @level2type=N'COLUMN',@level2name=N'CreateDate'; GO EXEC sys.sp_addextendedproperty @name=N'Description', @value=N'Last update user.' , @level0type=N'SCHEMA',@level0name=N'dbo' , @level1type=N'TABLE',@level1name=N'Measurement' , @level2type=N'COLUMN',@level2name=N'UpdateUser'; GO EXEC sys.sp_addextendedproperty @name=N'Description', @value=N'Last update date.' , @level0type=N'SCHEMA',@level0name=N'dbo' , @level1type=N'TABLE',@level1name=N'Measurement' , @level2type=N'COLUMN',@level2name=N'UpdateDate'; GO EXEC sys.sp_addextendedproperty @name=N'Description', @value=N'Row version.' , @level0type=N'SCHEMA',@level0name=N'dbo' , @level1type=N'TABLE',@level1name=N'Measurement' , @level2type=N'COLUMN',@level2name=N'RV'; GO  Minimum required for code tables, controlled vocabularies, \u0026hellip;  /* create table */ CREATE TABLE Method ( ID int IDENTITY(1,1) NOT NULL , Code nvarchar(10) NOT NULL , [Description] nvarchar(200) NOT NULL , SortOrder int NOT NULL , CreateUser nvarchar(50) NULL , CreateDate datetime2(7) NOT NULL , UpdateUser nvarchar(50) NULL , UpdateDate datetime2(7) NULL , RV timestamp NOT NULL ); GO /* add primary key constraint */ ALTER TABLE Method ADD CONSTRAINT PK_Method PRIMARY KEY CLUSTERED (ID) ON DATA; GO /* add defaults */ ALTER TABLE Method ADD CONSTRAINT CD_Method_SortOrder DEFAULT (0) FOR SortOrder , CONSTRAINT CD_Method_CreateUser DEFAULT (SUSER_NAME()) FOR CreateUser , CONSTRAINT CD_Method_CreateDate DEFAULT (GETDATE()) FOR CreateDate GO /* add trigger that updates audit columns */ CREATE TRIGGER [TR_Method_AU] ON [Method] AFTER UPDATE AS BEGIN SET NOCOUNT ON; UPDATE t SET t.UpdateDate = GETDATE() , t.UpdateUser = SUSER_SNAME() FROM Method t INNER JOIN inserted i ON i.ID = t.ID; END; GO /* add data dictionary */ /* (you can leave this up to the dba if you have a properly formatted list of table and column names with their description */ EXEC sys.sp_addextendedproperty @name=N'Description', @value=N'The observation methods.' , @level0type=N'SCHEMA',@level0name=N'dbo' , @level1type=N'TABLE',@level1name=N'Method'; GO EXEC sys.sp_addextendedproperty @name=N'Description', @value=N'Method ID.' , @level0type=N'SCHEMA',@level0name=N'dbo' , @level1type=N'TABLE',@level1name=N'Method' , @level2type=N'COLUMN',@level2name=N'ID'; GO EXEC sys.sp_addextendedproperty @name=N'Description', @value=N'Method code. This codes must be unique at a certain point of time.' , @level0type=N'SCHEMA',@level0name=N'dbo' , @level1type=N'TABLE',@level1name=N'Method' , @level2type=N'COLUMN',@level2name=N'Code'; GO EXEC sys.sp_addextendedproperty @name=N'Description', @value=N'Method description.' , @level0type=N'SCHEMA',@level0name=N'dbo' , @level1type=N'TABLE',@level1name=N'Method' , @level2type=N'COLUMN',@level2name=N'Description'; GO EXEC sys.sp_addextendedproperty @name=N'Description', @value=N'The index of the item when the list is sorted, eg in dropdownboxes.' , @level0type=N'SCHEMA',@level0name=N'dbo' , @level1type=N'TABLE',@level1name=N'Method' , @level2type=N'COLUMN',@level2name=N'SortOrder'; GO EXEC sys.sp_addextendedproperty @name=N'Description', @value=N'Creation user.' , @level0type=N'SCHEMA',@level0name=N'dbo' , @level1type=N'TABLE',@level1name=N'Method' , @level2type=N'COLUMN',@level2name=N'CreateUser'; GO EXEC sys.sp_addextendedproperty @name=N'Description', @value=N'Creation date.' , @level0type=N'SCHEMA',@level0name=N'dbo' , @level1type=N'TABLE',@level1name=N'Method' , @level2type=N'COLUMN',@level2name=N'CreateDate'; GO EXEC sys.sp_addextendedproperty @name=N'Description', @value=N'Last update user.' , @level0type=N'SCHEMA',@level0name=N'dbo' , @level1type=N'TABLE',@level1name=N'Method' , @level2type=N'COLUMN',@level2name=N'UpdateUser'; GO EXEC sys.sp_addextendedproperty @name=N'Description', @value=N'Last update date.' , @level0type=N'SCHEMA',@level0name=N'dbo' , @level1type=N'TABLE',@level1name=N'Method' , @level2type=N'COLUMN',@level2name=N'UpdateDate'; GO EXEC sys.sp_addextendedproperty @name=N'Description', @value=N'Row version.' , @level0type=N'SCHEMA',@level0name=N'dbo' , @level1type=N'TABLE',@level1name=N'Method' , @level2type=N'COLUMN',@level2name=N'RV'; ","href":"/tutorials/database_conventions/","title":"Database conventions (MS SQL Server)"},{"content":"library(R.utils) library(rgdal) library(tidyverse) library(leaflet) library(sp) library(sf) library(rgbif) library(DBI) What we want to do In this short tutorial, we explore various options to deal with the situation where we have (1) a spatially referenced GIS file with polygons and (2) a spatially referenced set of points that overlaps with the GIS polygons.\nTypically, both data sources contain information (apart from the spatial locations) that needs to be related to each other in some way. In this case study, we want to know for each point in which polygon it is located.\nGet some data to work with For the point data, we will work with data on the invasive species - Chinese mitten crab (Eriocheir sinensis) in Flanders, Belgium, from the year 2008 (GBIF.org (20th June 2019) GBIF Occurrence Download https://doi.org/10.15468/dl.decefb).\nWe will use convenience functions from the rgbif package to download the data as a zip file and to import the data as a tibble in the R environment.\ninvasive_species \u0026lt;- occ_download_get(\u0026#34;0032582-190415153152247\u0026#34;, path = tempdir(), overwrite = TRUE) %\u0026gt;% occ_download_import() %\u0026gt;% filter(!is.na(decimalLongitude), !is.na(decimalLatitude)) ## Download file size: 0.01 MB ## On disk at /var/folders/c2/7_7qg3r93993m4kk3kjqgj2n76h_tn/T//RtmpYC9cBo/0032582-190415153152247.zip  We will use the European reference grid system from the European Environmental Agency as an example of a GIS vector layer (each grid cell is a polygon). The Belgian part of the grid system can be downloaded as a sqlite/spatialite database from the EEA website using the following code:\n# explicitly set mode = \u0026#34;wb\u0026#34;, otherwise zip file will be corrupt download.file(\u0026#34;https://www.eea.europa.eu/data-and-maps/data/eea-reference-grids-2/gis-files/belgium-spatialite/at_download/file\u0026#34;, destfile = file.path(tempdir(), \u0026#34;Belgium_spatialite.zip\u0026#34;), mode = \u0026#34;wb\u0026#34;) # this will extract a file Belgium.sqlite to the temporary folder unzip(zipfile = file.path(tempdir(), \u0026#34;Belgium_spatialite.zip\u0026#34;), exdir = tempdir()) Point in polygon with the sf package The spatial query can be done with the aid of the sf package. The package has built-in functions to read spatial data (which uses GDAL as backbone).\nWe will project the data to Belgian Lambert 72 (https://epsg.io/31370), because the join assumes planar coordinates.\nbe10grid \u0026lt;- read_sf(file.path(tempdir(), \u0026#34;Belgium.sqlite\u0026#34;), layer = \u0026#34;be_10km\u0026#34;) %\u0026gt;% # convert to Belgian Lambert 72 st_transform(crs = 31370) We now get a sf object which is also a data.frame and a tbl:\nclass(be10grid) ## [1] \u0026quot;sf\u0026quot; \u0026quot;tbl_df\u0026quot; \u0026quot;tbl\u0026quot; \u0026quot;data.frame\u0026quot;  Let’s have a look at this object:\nbe10grid ## Simple feature collection with 580 features and 3 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -22402.56 ymin: -1449.985 xmax: 311353.3 ymax: 305932.2 ## Projected CRS: Belge 1972 / Belgian Lambert 72 ## # A tibble: 580 × 4 ## cellcode eoforigin noforigin GEOMETRY ## * \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;POLYGON [m]\u0026gt; ## 1 10kmE376N318 3760000 3180000 ((-20851.02 240718.4, -21626.25 250679.5, -… ## 2 10kmE376N319 3760000 3190000 ((-21626.25 250679.5, -22402.56 260640.9, -… ## 3 10kmE377N314 3770000 3140000 ((-7781.475 201650, -8552.253 211611, 1426.… ## 4 10kmE377N315 3770000 3150000 ((-8552.253 211611, -9324.075 221572.1, 655… ## 5 10kmE377N316 3770000 3160000 ((-9324.075 221572.1, -10096.95 231533.3, -… ## 6 10kmE377N317 3770000 3170000 ((-10096.95 231533.3, -10870.87 241494.6, -… ## 7 10kmE377N318 3770000 3180000 ((-10870.87 241494.6, -11645.85 251456.1, -… ## 8 10kmE377N319 3770000 3190000 ((-11645.85 251456.1, -12421.9 261417.9, -2… ## 9 10kmE377N320 3770000 3200000 ((-12421.9 261417.9, -13199.02 271379.8, -3… ## 10 10kmE377N321 3770000 3210000 ((-13199.02 271379.8, -13977.21 281342.1, -… ## # … with 570 more rows  We can see that the spatial information resides in a GEOMETRY list column.\nSimilarly, the package has built-in functions to convert a data.frame containing coordinates to a spatial sf object:\ninvasive_spatial \u0026lt;- st_as_sf(invasive_species, coords = c(\u0026#34;decimalLongitude\u0026#34;, \u0026#34;decimalLatitude\u0026#34;), crs = 4326) %\u0026gt;% # convert to Lambert72 st_transform(crs = 31370) Resulting in:\ninvasive_spatial ## Simple feature collection with 513 features and 43 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 35125.7 ymin: 188235.3 xmax: 153743.2 ymax: 220135 ## Projected CRS: Belge 1972 / Belgian Lambert 72 ## # A tibble: 513 × 44 ## gbifID datasetKey occurrenceID kingdom phylum class order family genus ## * \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1146738051 258c9ce5-1b… INBO:VIS:007… Animal… Arthr… Mala… Deca… Pseud… Erio… ## 2 1146738044 258c9ce5-1b… INBO:VIS:007… Animal… Arthr… Mala… Deca… Pseud… Erio… ## 3 1146738029 258c9ce5-1b… INBO:VIS:007… Animal… Arthr… Mala… Deca… Pseud… Erio… ## 4 1146738026 258c9ce5-1b… INBO:VIS:007… Animal… Arthr… Mala… Deca… Pseud… Erio… ## 5 1146737979 258c9ce5-1b… INBO:VIS:007… Animal… Arthr… Mala… Deca… Pseud… Erio… ## 6 1146737949 258c9ce5-1b… INBO:VIS:007… Animal… Arthr… Mala… Deca… Pseud… Erio… ## 7 1146737902 258c9ce5-1b… INBO:VIS:007… Animal… Arthr… Mala… Deca… Pseud… Erio… ## 8 1146737751 258c9ce5-1b… INBO:VIS:007… Animal… Arthr… Mala… Deca… Pseud… Erio… ## 9 1146737736 258c9ce5-1b… INBO:VIS:007… Animal… Arthr… Mala… Deca… Pseud… Erio… ## 10 1146737717 258c9ce5-1b… INBO:VIS:007… Animal… Arthr… Mala… Deca… Pseud… Erio… ## # … with 503 more rows, and 35 more variables: species \u0026lt;chr\u0026gt;, ## # infraspecificEpithet \u0026lt;lgl\u0026gt;, taxonRank \u0026lt;chr\u0026gt;, scientificName \u0026lt;chr\u0026gt;, ## # countryCode \u0026lt;chr\u0026gt;, locality \u0026lt;lgl\u0026gt;, publishingOrgKey \u0026lt;chr\u0026gt;, ## # coordinateUncertaintyInMeters \u0026lt;dbl\u0026gt;, coordinatePrecision \u0026lt;lgl\u0026gt;, ## # elevation \u0026lt;lgl\u0026gt;, elevationAccuracy \u0026lt;lgl\u0026gt;, depth \u0026lt;lgl\u0026gt;, depthAccuracy \u0026lt;lgl\u0026gt;, ## # eventDate \u0026lt;dttm\u0026gt;, day \u0026lt;int\u0026gt;, month \u0026lt;int\u0026gt;, year \u0026lt;int\u0026gt;, taxonKey \u0026lt;int\u0026gt;, ## # speciesKey \u0026lt;int\u0026gt;, basisOfRecord \u0026lt;chr\u0026gt;, institutionCode \u0026lt;chr\u0026gt;, …  Now we are ready to make the spatial overlay. This is done with the aid of sf::st_join. The default join type is st_intersects. This will result in the same spatial overlay as sp::over (see next section). We join the information from the grid to the points through a left join. See the DE-9IM topological model for explanations about all possible spatial joins.\nNote that with st_intersects points on a polygon boundary and points corresponding to a polygon vertex are considered to be inside the polygon. In the case where points are joined with polygons, st_intersects and st_covered_by will give the same result. The join type st_within can be used if we want to join only when points are completely within (excluding polygon boundary).\ninvasive_be10grid_sf \u0026lt;- invasive_spatial %\u0026gt;% st_join(be10grid, join = st_intersects, left = TRUE) %\u0026gt;% select(-eoforigin, -noforigin) # exclude the EofOrigin NofOrigin columns Looking at selected columns of the resulting object:\ninvasive_be10grid_sf %\u0026gt;% select(species, eventDate, cellcode, geometry) ## Simple feature collection with 513 features and 3 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 35125.7 ymin: 188235.3 xmax: 153743.2 ymax: 220135 ## Projected CRS: Belge 1972 / Belgian Lambert 72 ## # A tibble: 513 × 4 ## species eventDate cellcode geometry ## \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;POINT [m]\u0026gt; ## 1 Eriocheir sinensis 2008-09-17 00:00:00 10kmE392N312 (143788.9 201487.1) ## 2 Eriocheir sinensis 2008-06-03 00:00:00 10kmE392N312 (143788.9 201487.1) ## 3 Eriocheir sinensis 2008-03-20 00:00:00 10kmE389N311 (114822.9 188235.3) ## 4 Eriocheir sinensis 2008-07-03 00:00:00 10kmE393N311 (153743.2 191634.8) ## 5 Eriocheir sinensis 2008-09-17 00:00:00 10kmE392N312 (143788.9 201487.1) ## 6 Eriocheir sinensis 2008-04-10 00:00:00 10kmE392N312 (147138.3 199035.5) ## 7 Eriocheir sinensis 2008-03-13 00:00:00 10kmE381N314 (35125.7 205808.5) ## 8 Eriocheir sinensis 2008-03-19 00:00:00 10kmE391N312 (136197.7 197300.8) ## 9 Eriocheir sinensis 2008-03-19 00:00:00 10kmE391N312 (136197.7 197300.8) ## 10 Eriocheir sinensis 2008-10-28 00:00:00 10kmE389N311 (121593.8 190203.2) ## # … with 503 more rows  Point in polygon with the sp package General note: migration to the more actively developed sf package is currently advised by the sp maintainer. The sp package is still maintained in order to support the newest versions of the GDAL and PROJ backends.\nInstead of sf objects (= data.frames or tibbles with a geometry list-column), the sp package works with Spatial spatial data classes (which has many derived spatial data classes for points, polygons, …).\nFirst, we need to convert the data.frame with point locations to a SpatialPointsDataFrame. We also need to ensure that the coordinate reference system (CRS) for both the point locations and the grid is the same. The data from GBIF are in WGS84 format.\ncrs_wgs84 \u0026lt;- CRS(SRS_string = \u0026#34;EPSG:4326\u0026#34;) coord \u0026lt;- invasive_species %\u0026gt;% select(decimalLongitude, decimalLatitude) invasive_spatial \u0026lt;- SpatialPointsDataFrame(coord, data = invasive_species, proj4string = crs_wgs84) The sp package has no native methods to read the Belgium 10 km x 10 km grid, but we can use rgdal::readOGR to connect with the sqlite/spatialite database and extract the Belgium 10 km x 10 km grid as a SpatialPolygonsDataFrame. Apart from the 10 km x 10 km grid, the database also contains 1 km x 1 km and 100 km x 100 km grids as raster or vector files.\nbe10grid \u0026lt;- readOGR(dsn = file.path(tempdir(), \u0026#34;Belgium.sqlite\u0026#34;), layer = \u0026#34;be_10km\u0026#34;) ## Warning in OGRSpatialRef(dsn, layer, morphFromESRI = morphFromESRI, dumpSRS ## = dumpSRS, : Discarded datum European_Terrestrial_Reference_System_1989 in ## Proj4 definition: +proj=laea +lat_0=52 +lon_0=10 +x_0=4321000 +y_0=3210000 ## +ellps=GRS80 +units=m +no_defs ## OGR data source with driver: SQLite ## Source: \u0026quot;/private/var/folders/c2/7_7qg3r93993m4kk3kjqgj2n76h_tn/T/RtmpYC9cBo/Belgium.sqlite\u0026quot;, layer: \u0026quot;be_10km\u0026quot; ## with 580 features ## It has 3 fields  Note the warning: it is because some PROJ.4 information, i.e. the string to represent the coordinate reference system, is not supported anymore in the current geospatial GDAL and PROJ libraries (the background workhorses for spatial R packages). The spatialite database from the EEA website (with the 10 km x 10 km grid) still uses the older PROJ.4 string . Because the rgdal package is still backwards compatible, we should not (yet) worry about this: rgdal does the translation for the newer GDAL 3 and PROJ \u0026gt;= 6. Do know that, instead of PROJ.4 strings, the WKT2 string is now used in R to better represent coordinate reference systems (so it would best be incorporated in the EEA data source). Just compare these:\n# PROJ.4 string = old; used by PROJ 4 proj4string(be10grid) # or: be10grid@proj4string ## Warning in proj4string(be10grid): CRS object has comment, which is lost in ## output ## [1] \u0026quot;+proj=laea +lat_0=52 +lon_0=10 +x_0=4321000 +y_0=3210000 +ellps=GRS80 +units=m +no_defs\u0026quot;  # WKT2 string = new and much better; used by PROJ \u0026gt;= 6 wkt(be10grid) %\u0026gt;% cat # or: be10grid@proj4string %\u0026gt;% comment %\u0026gt;% cat ## PROJCRS[\u0026quot;ETRS89-extended / LAEA Europe\u0026quot;, ## BASEGEOGCRS[\u0026quot;ETRS89\u0026quot;, ## DATUM[\u0026quot;European Terrestrial Reference System 1989\u0026quot;, ## ELLIPSOID[\u0026quot;GRS 1980\u0026quot;,6378137,298.257222101, ## LENGTHUNIT[\u0026quot;metre\u0026quot;,1]]], ## PRIMEM[\u0026quot;Greenwich\u0026quot;,0, ## ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433]], ## ID[\u0026quot;EPSG\u0026quot;,4258]], ## CONVERSION[\u0026quot;Europe Equal Area 2001\u0026quot;, ## METHOD[\u0026quot;Lambert Azimuthal Equal Area\u0026quot;, ## ID[\u0026quot;EPSG\u0026quot;,9820]], ## PARAMETER[\u0026quot;Latitude of natural origin\u0026quot;,52, ## ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433], ## ID[\u0026quot;EPSG\u0026quot;,8801]], ## PARAMETER[\u0026quot;Longitude of natural origin\u0026quot;,10, ## ANGLEUNIT[\u0026quot;degree\u0026quot;,0.0174532925199433], ## ID[\u0026quot;EPSG\u0026quot;,8802]], ## PARAMETER[\u0026quot;False easting\u0026quot;,4321000, ## LENGTHUNIT[\u0026quot;metre\u0026quot;,1], ## ID[\u0026quot;EPSG\u0026quot;,8806]], ## PARAMETER[\u0026quot;False northing\u0026quot;,3210000, ## LENGTHUNIT[\u0026quot;metre\u0026quot;,1], ## ID[\u0026quot;EPSG\u0026quot;,8807]]], ## CS[Cartesian,2], ## AXIS[\u0026quot;northing (Y)\u0026quot;,north, ## ORDER[1], ## LENGTHUNIT[\u0026quot;metre\u0026quot;,1]], ## AXIS[\u0026quot;easting (X)\u0026quot;,east, ## ORDER[2], ## LENGTHUNIT[\u0026quot;metre\u0026quot;,1]], ## USAGE[ ## SCOPE[\u0026quot;Statistical analysis.\u0026quot;], ## AREA[\u0026quot;Europe - European Union (EU) countries and candidates. Europe - onshore and offshore: Albania; Andorra; Austria; Belgium; Bosnia and Herzegovina; Bulgaria; Croatia; Cyprus; Czechia; Denmark; Estonia; Faroe Islands; Finland; France; Germany; Gibraltar; Greece; Hungary; Iceland; Ireland; Italy; Kosovo; Latvia; Liechtenstein; Lithuania; Luxembourg; Malta; Monaco; Montenegro; Netherlands; North Macedonia; Norway including Svalbard and Jan Mayen; Poland; Portugal including Madeira and Azores; Romania; San Marino; Serbia; Slovakia; Slovenia; Spain including Canary Islands; Sweden; Switzerland; Turkey; United Kingdom (UK) including Channel Islands and Isle of Man; Vatican City State.\u0026quot;], ## BBOX[24.6,-35.58,84.17,44.83]], ## ID[\u0026quot;EPSG\u0026quot;,3035]]  We transform the 10 km x 10 km grid to the same CRS system:\nbe10grid \u0026lt;- spTransform(be10grid, crs_wgs84) Now we are ready to spatially join (overlay) the SpatialPointsDataFrame wth the 10 km x 10 km grid. This can be done using sp::over. The first two arguments of the function give, respectively, the geometry (locations) of the queries, and the layer from which the geometries or attributes are queried. See ?sp::over. In this case, when x = “SpatialPoints” and y = “SpatialPolygonsDataFrame”, it returns a data.frame of the second argument with row entries corresponding to the first argument.\ninvasive_be10grid \u0026lt;- over(x = invasive_spatial, y = be10grid) invasive_species_be10grid \u0026lt;- bind_cols(invasive_species, invasive_be10grid) To see what the result looks like, we can select the most relevant variables and print it (first ten rows).\ninvasive_species_be10grid %\u0026gt;% select(species, starts_with(\u0026#34;decimal\u0026#34;), eventDate, cellcode) %\u0026gt;% head(10) ## # A tibble: 10 × 5 ## species decimalLatitude decimalLongitude eventDate cellcode ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt; ## 1 Eriocheir si… 51.1 4.28 2008-09-17 00:00:00 10kmE392N… ## 2 Eriocheir si… 51.1 4.28 2008-06-03 00:00:00 10kmE392N… ## 3 Eriocheir si… 51.0 3.87 2008-03-20 00:00:00 10kmE389N… ## 4 Eriocheir si… 51.0 4.42 2008-07-03 00:00:00 10kmE393N… ## 5 Eriocheir si… 51.1 4.28 2008-09-17 00:00:00 10kmE392N… ## 6 Eriocheir si… 51.1 4.33 2008-04-10 00:00:00 10kmE392N… ## 7 Eriocheir si… 51.2 2.73 2008-03-13 00:00:00 10kmE381N… ## 8 Eriocheir si… 51.1 4.17 2008-03-19 00:00:00 10kmE391N… ## 9 Eriocheir si… 51.1 4.17 2008-03-19 00:00:00 10kmE391N… ## 10 Eriocheir si… 51.0 3.96 2008-10-28 00:00:00 10kmE389N…  What have we done? To wrap this up, we make a map that shows what we have done. We will use the results obtained with the sf package.\nFirst, we need to transform invasive_be10grid_sf back to WGS84 (the background maps in leaflet are in WGS84) (be10grid is already in WGS84 format).\ninvasive_be10grid_sf \u0026lt;- invasive_be10grid_sf %\u0026gt;% st_transform(crs = 4326) Zooming in on the point markers and hovering over a marker will show the reference grid identifier for the grid cell as it is joined to spatial points object invasive_be10grid. Clicking in a grid cell will bring up a popup showing the reference grid identifier for the grid cell as it is named in be10grid.\nleaflet(be10grid) %\u0026gt;% addTiles() %\u0026gt;% addPolygons(popup = ~cellcode) %\u0026gt;% addMarkers(data = invasive_be10grid_sf, label = ~cellcode) Note: run the code to see the interactive map.\n","href":"/tutorials/spatial_point_in_polygon/","title":"How to make spatial joins (point in polygon)?"},{"content":"During this workshop you learn how to turn a regular Rmarkdown file into a bookdown document using the INBO corporate identity. A lot of tips and trics use plain bookdown. So you can use them with other bookdown output formats.\nThe slides are available on the INBOmd examples website.\nThe source code is avalaible in the INBOmd example GitHub repository.\n","href":"/tutorials/r_inbomd/","title":"Applying the INBO corporate identity to bookdown documents"},{"content":"","href":"/tags/e-book/","title":"e-book"},{"content":"See the vignette/tutorial at https://inbo.github.io/n2khab/articles/v030_GRTSmh.html\n","href":"/tutorials/vignette_n2khab_v030_grtsmh/","title":"n2khab: Using the GRTS data sources"},{"content":"","href":"/authors/adokter/","title":"adokter"},{"content":"","href":"/tags/biorad/","title":"bioRad"},{"content":"See the vignette/tutorial at https://adokter.github.io/bioRad/articles/bioRad.html\n","href":"/tutorials/vignette_biorad/","title":"bioRad: Introduction to bioRad"},{"content":"See the vignette/tutorial at https://inbo.github.io/n2khab/articles/v010_reference_lists.html\n","href":"/tutorials/vignette_n2khab_v010_reference_lists/","title":"n2khab: Reference lists and using them"},{"content":"","href":"/tags/dplyr/","title":"dplyr"},{"content":"","href":"/authors/ivyjansen/","title":"ivyjansen"},{"content":"","href":"/authors/pieterverschelde/","title":"pieterverschelde"},{"content":"Doel van de cursus Hoe maak je van je ingezamelde gegevens een reproduceerbare analyse, visualisatie en rapportage, gebruik makend van de software R en Rstudio.\n Rstudio kunnen gebruiken (Les 1a) Commando\u0026rsquo;s uitvoeren vanuit een script (Les 1b) Externe databestanden inlezen in R (Les 2a) Gegevens visualiseren (ggplot2) (Les 2b) Data manipuleren in een gewenste vorm (dplyr) (Les 3) Reproduceerbaar analyserapport maken (Rmarkdown) + algemene vragen (Les 4)  Bovenstaande topics worden gecombineerd in een opleiding van 4 workshops. Deze opleiding is bedoeld voor mensen die nog nooit met R gewerkt hebben. In de workshops wordt het materiaal in de handleidingen al doende uitgelegd, en afgewisseld met oefeningen. Na elke workshop wordt er altijd een huistaak aangeboden, die op vrijwillige basis ingediend kan worden, en van feedback voorzien.\nR en RStudio Introductie tot R en Rstudio\n Uitleg van alle vensters in Rstudio, en overlopen van de user installation instructions Werken met projecten Packages installeren en laden Coding basics in R Vectoren en dataframes  Handleiding Code Huistaak\nInlezen van gegevens Introductie tot het inlezen van externe databestanden\n readr readxl googlesheets4  Handleiding Data Code Oefening\nggplot2 Introductie tot het maken van grafieken met ggplot2\n Basis syntax Geoms Aesthetics Facets Titels Plot bewaren  Handleiding Data Code Oefening Huistaak\ndplyr Introductie tot data manipulatie met dplyr\n piping filter, arrange, mutate, select group_by, summarise Tidy data: gather, spread  Handleiding Data Code Oefening Huistaak\nRmarkdown Introductie tot het maken van een reproduceerbaar document met Rmarkdown\n Markdown syntax Code chunks Chunk opties en globale opties YAML header Tabellen  Handleiding Code Oefening Figuur voor oefening Huistaak\n","href":"/tutorials/r_beginners/","title":"R voor beginners"},{"content":"These workshops are a follow-up of the course on \u0026ldquo;Spatial, temporal and spatial-temporal models using R-INLA\u0026rdquo; by Alain Zuur and Elena Ieno (Highland Statistics Ltd.). The main goal is the get people using R-INLA with their own data in a workshop setting so they can tap into the knowledge of others. The workshops are not a copy of the Highstat course but elaborate certain topics. We also introduce the inlatools and inlabru.\nHackMD, a place to share your code related to these workshops.\nWorkshop 1 Fitting models with only fixed effects, random intercepts and first order random walk.\n slides source code and data  Workshop 2 inlabru versus INLA for fixed effects, random intercepts and random walks.\n slides source code and data  Workshop 3 Fitting models with spatial correlation\n slides source code and data  Literature  Bachl, F. et al, 2019 inlabru: an R package for Bayesian spatial modelling from ecological survey data https://doi.org/10.1111/2041-210X.13168 Blangiardo, M. and Cameletti, M. 2015 Spatial and Spatio-temporal Bayesian Models with R-INLA https://sites.google.com/a/r-inla.org/stbook/ ISBN: 978-1-118-32655-8 Gómez-Rubio, V. 2019 Bayesian inference with INLA and R-INLA https://becarioprecario.bitbucket.io/inla-gitbook/index.html Krainski, E. et al 2018 Advanced Spatial Modeling with Stochastic Partial Differential Equations Using R and INLA https://becarioprecario.bitbucket.io/spde-gitbook/index.html Wang, X. et al 2018 Bayesian Regression Modeling with INLA ISBN: 978-1-498-72725-9 Zuur, A. et al 2017 Beginner\u0026rsquo;s Guide to Spatial, Temporal and Spatial-Temporal Ecological Data Analysis with R-INLA http://highstat.com/index.php/beginner-s-guide-to-regression-models-with-spatial-and-temporal-correlation ISBN: 978-0-957-17419-1 Zuur, A. and Ieno, E. 2018 Spatial, Temporal and Spatial-Temporal Ecological Data Analysis with R-INLA. Volume II: GAM and Zero-Inflated Models http://highstat.com/index.php/beginner-s-guide-to-regression-models-with-spatial-and-temporal-correlation ISBN: 978-0-957-17414-6  ","href":"/tutorials/r_inla/","title":"INLA workshops"},{"content":"","href":"/tags/inlabru/","title":"inlabru"},{"content":"At INBO, people do write code and require version control. As git is not the most straightforward environment to work with for non-IT specialists, we try to define our own sub-ecosystem with relevant practices and an achievable workflow.\nTo introduce the concept of version control with Git and Github, a separate git course website is available here.\nThe git course provides an introduction on main terminology of Git based on 5 important tasks:\n Tell the story of your project Travel back in time Experiment with changes Backup your work Collaborate on projects  The hands-on session git with Rstudio is part of the course. It can also be used as a reference to the individual steps when using git.\nWhen looking for a quick day-to-day workflow to use git(hub) with Rstudio, check the Rstudio workflow.\n","href":"/tutorials/git_introduction/","title":"Git(hub) introduction"},{"content":"","href":"/tags/inlatools/","title":"inlatools"},{"content":"See the vignette/tutorial at https://inlatools.netlify.app/articles/distribution.html\n","href":"/tutorials/vignette_inlatools_distribution/","title":"inlatools: Checking the distribution and dispersion of a model"},{"content":"See the vignette/tutorial at https://inlatools.netlify.app/articles/rwprior.html\n","href":"/tutorials/vignette_inlatools_rwprior/","title":"inlatools: Getting a grasp on the random walk hyperparameter"},{"content":"","href":"/tags/vmm/","title":"vmm"},{"content":"OPGELET: deze databank is alleen raadpleegbaar voor INBO-medewerkers, niet voor externen. Externen kunnen gebruik maken van het VMM-geoloket waterkwaliteit.\nHet INBO krijgt jaarlijks een (gedeeltelijke) kopie van de waterkwaliteitsdatabank van de Vlaamse Milieumaatschappij (VMM). De kopie omvat fysicochemische metingen en kwaliteitsindexen uit het VMM-meetnet oppervlaktewaters. Deze en andere waterkwaliteitsgevens zijn rechtstreeks opvraagbaar via het VMM-geoloket waterkwaliteit, maar de lokale kopie laat een veel vlottere raadpleging toe wanneer analyse van een groter aantal meetpunten gewenst is. De databank wordt ± jaarlijks geactualiseerd.\nMeer info over de aard van de gegevens en hoe ze kunnen geraadpleegd worden is te vinden op het intranet van het INBO: https://sites.google.com/a/inbo.be/intranet/home/ondersteuning/IT-en-data/datasystemen/vmm-oppervlaktewaters\n","href":"/tutorials/vmm_surface_waters_quality_data/","title":"VMM surface waters - quality data"},{"content":"","href":"/authors/nicolasnoe/","title":"nicolasnoe"},{"content":"See the vignette/tutorial at https://inlatools.netlify.app/articles/prior.html\n","href":"/tutorials/vignette_inlatools_prior/","title":"inlatools: Setting a prior for the random intercept variance and fixed effects"},{"content":"See the vignette/tutorial at https://inbo.github.io/inborutils/articles/data_loaders_examples.html\n","href":"/tutorials/vignette_inborutils_data_loaders_examples/","title":"inborutils: (Down)load data from external partners"},{"content":"See the vignette/tutorial at https://inbo.github.io/INBOmd/articles/introduction.html\n","href":"/tutorials/vignette_inbomd_introduction/","title":"INBOmd: Introduction to INBOmd"},{"content":"Introduction R code can become elaborate and consequently unclear or difficult to navigate. Yet, it is possible to introduce headers and navigate through them.\nCreating sections manually To create a header of a section, different methods can be applied. Any comment line which includes at least four trailing dashes (-), equal signs (=), or hash tags (#) automatically creates a code section.\n# 1. Header 1 #### # 2. Header 2 ---- # 3. Header 3 ==== On the right side of the code editor, nex to the buttons to run your code, a button with horizontal lines can be found. When you click it, the headers will be visible. As such, the structure of your code is visible and allows more easily to navigate through it.\nAnother way of navigation is via the button with the name of the selected header On the bottom of the code editor.\nCreating sections automatically It is also possible to add sections automatically by clicking on the tab Code and select Insert Section…\nDrop down Note there is a drop down button next to each header, allowing to collapse or expand your code. Yet, there are shortcuts to this:\n Collapse — Alt+L Expand — Shift+Alt+L Collapse All — Alt+O Expand All — Shift+Alt+O  An example Now we will illustrate its use with an example of an analysis.\nRun tidyverse package\nlibrary(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.5 ✓ dplyr 1.0.7 ## ✓ tidyr 1.1.4 ✓ stringr 1.4.0 ## ✓ readr 2.0.2 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag()  Now different manipulations will be performed on the dataset. To make navigation through the different manipulations more straightforward, we add sections.\n# 1. Plot hindfoot length over weight per year ---- surveys \u0026lt;- read_csv(\u0026#34;../data/20180222_surveys.csv\u0026#34;) %\u0026gt;% filter(!is.na(weight), # remove missing weight !is.na(hindfoot_length), # remove missing hindfoot_length !is.na(sex)) # remove missing sex ggplot(surveys, aes(x = weight, y = hindfoot_length)) + geom_point(aes(colour = species_id), alpha = 0.5) + ylab(\u0026#34;hindfoot length\u0026#34;) + scale_x_log10() + scale_color_discrete() + theme_dark() + facet_wrap(~year) # 2. Create a heatmap of the population growth in Ghent and its districts ---- tidy_bevolking \u0026lt;- read_csv(\u0026#34;../data/20180522_gent_groeiperwijk_tidy.csv\u0026#34;) ggplot(tidy_bevolking, aes(x = year, y = wijk)) + geom_tile(aes(fill = growth), color = \u0026#34;red\u0026#34;) + # fill = colour of content/pane; color = colour of edge # scale_fill_gradient(low = \u0026#34;white\u0026#34;, high = \u0026#34;steelblue\u0026#34;) + scale_fill_distiller(type = \u0026#34;div\u0026#34;) + theme(axis.title.x=element_blank(), axis.title.y=element_blank()) # 3. Place two plots in one window ---- install.packages(\u0026#34;cowplot\u0026#34;) devtools::install_github(\u0026#34;inbo/INBOtheme\u0026#34;) # install inbo theme library(cowplot) library(INBOtheme) weight_scatter \u0026lt;- ggplot(surveys, aes(x = weight, y = hindfoot_length)) + geom_point() + ylab(\u0026#34;hindfoot length\u0026#34;) weight_density \u0026lt;- ggplot(surveys, aes(x = weight, y = ..density..) ) + # the \u0026#39;..\u0026#39; refers to internal calculations of the density geom_histogram() + geom_density() # two plots in one window plot_grid(weight_scatter, weight_density, labels = c(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;)) ","href":"/tutorials/r_script_sections/","title":"Headers and navigation in R code"},{"content":"","href":"/authors/jeroenspeybroeck/","title":"jeroenspeybroeck"},{"content":"General note: migration to the more actively developed sf package is currently advised by the sp maintainer. The sp package, used in this tutorial, is still maintained in order to support the newest versions of the GDAL and PROJ backends.\nIntroduction The required packages are leaflet and sp.\nlibrary(leaflet) library(sp) Dummy data Let’s create a dummy data.frame to play around, i.e. the three locations of INBO:\nnames \u0026lt;- c(\u0026#34;VAC HT\u0026#34;, \u0026#34;Geraardsbergen\u0026#34;, \u0026#34;Linkebeek\u0026#34;) lat \u0026lt;- c(50.865664, 50.760201, 50.767950) lon \u0026lt;- c(4.349944, 3.874300, 4.333044) data \u0026lt;- data.frame(names, lat, lon) We created three points:\nplot(data$lon, data$lat) Creating a map We need to convert the data.frame to a SpatialPointsDataFrame:\ncrs_wgs84 \u0026lt;- CRS(SRS_string = \u0026#34;EPSG:4326\u0026#34;) pts \u0026lt;- SpatialPointsDataFrame(data[c(\u0026#34;lon\u0026#34;,\u0026#34;lat\u0026#34;)], data[!(names(data) %in% c(\u0026#34;lon\u0026#34;,\u0026#34;lat\u0026#34;))], proj4string = crs_wgs84) The leaflet package is ideal to create a basic interactive map:\nlibrary(leaflet) leaf_map \u0026lt;- leaflet(pts) %\u0026gt;% addTiles(group = \u0026#34;OSM\u0026#34;) %\u0026gt;% addCircleMarkers() leaf_map Note: run the code to see the interactive map.\nNice, no?!\nMore information is provided at the leaflet information website!\n","href":"/tutorials/spatial_create_leaflet_map/","title":"Let's create an interactive map!"},{"content":"","href":"/authors/lienreyserhove/","title":"lienreyserhove"},{"content":"","href":"/authors/pieterjanverhelst/","title":"pieterjanverhelst"},{"content":"","href":"/tags/tidyverse/","title":"tidyverse"},{"content":"library(dplyr) How to use piping in R Normally, you would do this:\nhead(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1  However, with piping, this would look different:\nmtcars %\u0026gt;% head() ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1  You may wonder, what\u0026rsquo;s the point? If you need to apply multiple functions on one dataframe, piping saves you a lot of typing, and makes for tidy R code. An example:\nmtcars %\u0026gt;% mutate(dec = mpg/10) %\u0026gt;% select(mpg, dec, am) %\u0026gt;% filter(am == \u0026#34;1\u0026#34;) ## mpg dec am ## 1 21.0 2.10 1 ## 2 21.0 2.10 1 ## 3 22.8 2.28 1 ## 4 32.4 3.24 1 ## 5 30.4 3.04 1 ## 6 33.9 3.39 1 ## 7 27.3 2.73 1 ## 8 26.0 2.60 1 ## 9 30.4 3.04 1 ## 10 15.8 1.58 1 ## 11 19.7 1.97 1 ## 12 15.0 1.50 1 ## 13 21.4 2.14 1  What did we do:\n We created a new column \u0026lsquo;dec\u0026rsquo; using mutate(). This column \u0026lsquo;dec\u0026rsquo; consists of the values of column mpg divided by 10. We selected the columns \u0026lsquo;mpg\u0026rsquo;, \u0026lsquo;dec\u0026rsquo; and \u0026lsquo;am\u0026rsquo; using select(). We filtered for the value \u0026lsquo;1\u0026rsquo; in the column \u0026lsquo;am\u0026rsquo; using filter().  And all of this in just one step! Now what? We have created a new column, but this column is not part of our dataframe yet! We could do this:\nmtcars \u0026lt;- mtcars %\u0026gt;% mutate(dec = mpg/10) OR\u0026hellip; we could do this!\nlibrary(magrittr) mtcars %\u0026lt;\u0026gt;% mutate(dec = mpg/10) Soooo easy! This has been our first introduction to piping. There is however much more to learn! That is why you should definitely go to this link.\n","href":"/tutorials/r_tidyverse_piping/","title":"Using `%\u003e%` pipes in R"},{"content":"","href":"/authors/yasmineverzelen/","title":"yasmineverzelen"},{"content":"Insync is a thirth party tool that synchronises files with Google Drive. It has some nice features which are still not available in the sync tools provided by Google. For the remaining of this tutorial, \u0026ldquo;GoogleDrive\u0026rdquo; refers to the sync tools provided by Google.\nThe problem with GoogleDrive GoogleDrive doesn\u0026rsquo;t work well in combination with RStudio projects or Git projects. We\u0026rsquo;ll illustrate the problem with RStudio. RStudio has a performant auto save functionality, which limits data loss after an unexpected crash. As soon as the user changes a few characters in a script, the auto save kicks in. This functionality stores the backup information into a hidden subdirectory of the project (.Rproj.user). It writes very often to the files in this subdirectory.\nGoogleDrive is constantly monitoring the synchronised directory for new, changed or deleted files. As soon as it detects such file, it will lock the file, synchronise the file and unlock the file. The locking of the file pervents that changes are made to the file while it is being synchronised, because this would mess up the synchronisation. GoogleDrive synchronises all files within a synced directory, including those created by the RStudio auto save function. But as this function writes very often to those files, it often ends in trying to write to a file which still is locked by GoogleDrive. This results in a \u0026ldquo;cannot save to file\u0026rdquo; dialog box in RStudio, which has to be dismissed by the user. This happens so often that it becomes frustrating for the user.\nHow Insync solves this problem Insync is also constantly monitoring all files in the synchronised directories. Unlike GoogleDrive, Insync has an option to ignore directories or files when synchronising. When Insync is set to ignore .Rproj.user, the files within .Rproj.user are no longer synchronised and thus never locked, causing no problem with the RStudio auto save function.\nWait a minute, so these files will be no longer be available through the GoogleDrive website? Isn\u0026rsquo;t that a problem? Indeed, they will not be available. And no, that is not a problem. Only your temporary changes are no longer synchronised. When you save your script file in RStudio, you are saving a file to a location which is not on the ignore list and will thus be synchronised. But after saving, this file will be locked during sync? Yes, but the time between two consecutive manual saves of a script is a lot larger that the time required to sync the script. So the file will be unlocked by the next time you save the file.\nHow to set the ignore list in Insync First of all, it is important to do this prior to syncing files to your computer. Once a file or directory has been synced between the computer and the cloud, Insync will keep syncing it. Even when a file or directory is afterward added to the ignore list.\nSet up  Open the Insync app Click on your avatar Choose ignore list Add the search pattern for the files/folders to ignore into the form fieldand click on the circled \u0026lsquo;+\u0026rsquo;  The default action is to exclude all matching files and directories (including their files and subdirectories) from syncing (\u0026ldquo;do not upload or download\u0026rdquo;). Local files will remain only local and files in the cloud will remain only in the cloud. You can change this behaviour via the drop down menu of the pattern. Other options are \u0026ldquo;do not upload\u0026rdquo;, \u0026ldquo;do not download\u0026rdquo; or \u0026ldquo;remove from this list\u0026rdquo;.\nWe recommend to add following patterns:\n .rproj.user *.git *.rcheck *_cache *_files _site  FAQ  Can I use the same local folder when switching from GoogleDrive to Insync  It is safer you use a different folder.   I\u0026rsquo;ve already synced an RStudio project with Insync without setting the ignore list  Create a new RStudio project in a different folder and copy your data an script to this new RStudio project    ","href":"/installation/user/user_install_insync/","title":"Insync installation"},{"content":"","href":"/tags/ci/","title":"ci"},{"content":"","href":"/tags/development/","title":"development"},{"content":"Set up continuous integration with Wercker There are 2 major steps to set up continuous integration:\n create a wercker.yml file in the package add the application (package) to Wercker.com  To be able to add a package to Wercker, one must have administrator rights on the package repository on Github.\nThe Wercker test environment can only be set up if the file wercker.yml is commited to the repository, but Wercker is triggered to start checking when the application is added to wercker.com (giving an error if wercker.yml is not commited yet).\nwercker.yml Add a file \u0026ldquo;wercker.yml\u0026rdquo; in the root of the package with:\n box: reference to a package with a Docker image that is used as a test environment. If no specific version is specified, only the last master version is used. Which Docker image to use?  inbobmk/rstable which is an image with stable versions of R and a large number of packages (see the README). Most of the packages which are often used at INBO are available. The version of the packages is roughly fixed to the date on which the R version in the Docker image was upgraded. rocker/verse (https://hub.docker.com/r/rocker/verse/) which has the R, devtools and all tidyverse packages. The latest version of the image contains the latest version of the packages.   build:  different steps to pass  inbobmk/r-check: runs R CMD check but assumes that all dependencies are installed. Use this in combination with inbobmk/rstable in case you want to check your package against a stable set of packages. jimhester/r-check: installs all missing dependencies on the fly and then runs R CMD check. This will install the latest version of the dependencies. inbobmk/r-coverage: checks which lines in the code are covered by unit tests and which are not. See our page on code coverage for more details. This assumes that the covr package is installed. jimhester/r-coverage: installs covr and runs the code coverage inbobmk/r-lint: this check the style of your code. Good coding style is like using correct punctuation. You can manage without it, but it sure makes things easier to read. It assumes the lintr package is installed. jimhester/r-lint: installs the lintr package and checks the style of the code   steps are run following their order in the yaml file the execution will stop when a step fails if not all packages are available in the docker image, code to install packages has to be added as a first step   after a build pipe, one can also add a deploy-pipe  An example of a simple wercker.yml-file:\nbox: inbobmk/rstable build: steps: - script: code: Rscript -e \u0026quot;install.packages(c('DT','plotly'))\u0026quot; - inbobmk/r-check - inbobmk/r-coverage - inbobmk/r-lint  Wercker.com To add the application to www.wercker.com:\n log in on the website (easiest is to log in via github) and create a username click on the \u0026ldquo;+\u0026rdquo; button on the right top and choose \u0026ldquo;add application\u0026rdquo; select your username (next) select the repository of your package (next) choose the recommended option (next) you could choose to make the results publicly available and \u0026ldquo;create\u0026rdquo;  After creation, one can under \u0026ldquo;options\u0026rdquo;:\n pick a color for the package (useful when adding more than one package) read information on Webhook (ensures communication between github, Wercker and other services) status badge: markdown-code that allows you to add Wercker-results to your own code (e.g. copy this link to the README-file)  Hitting the avatar on the top right and choosing \u0026ldquo;Settings\u0026rdquo; allows to adjust if and when to receive email notifications.\n","href":"/tutorials/development_wercker/","title":"Wercker"},{"content":"  McElreath (2015): Statistical Rethinking is an introduction to applied Bayesian data analysis, aimed at PhD students and researchers in the natural and social sciences. This audience has had some calculus and linear algebra, and one or two joyless undergraduate courses in statistics. I’ve been teaching applied statistics to this audience for about a decade now, and this book has evolved from that experience. The book teaches generalized linear multilevel modeling (GLMMs) from a Bayesian perspective, relying on a simple logical interpretation of Bayesian probability and maximum entropy. The book covers the basics of regression through multilevel models, as well as touching on measurement error, missing data, and Gaussian process models for spatial and network autocorrelation. This is not a traditional mathematical statistics book. Instead the approach is computational, using complete R code examples, aimed at developing skilled and skeptical scientists. Theory is explained through simulation exercises, using R code. And modeling examples are fully worked, with R code displayed within the main text. Mathematical depth is given in optional {“}overthinking{”} boxes throughout.\n  Kass et al. (2016): The authors propose a set of 10 simple rules for effective statistical practice\n  Quinn \u0026amp; Keough (2002): An essential textbook for any student or researcher in biology needing to design experiments, sample programs or analyse the resulting data. The text begins with a revision of estimation and hypothesis testing methods, covering both classical and Bayesian philosophies, before advancing to the analysis of linear and generalized linear models. Topics covered include linear and logistic regression, simple and complex ANOVA models (for factorial, nested, block, split-plot and repeated measures and covariance designs), and log-linear models. Multivariate techniques, including classification and ordination, are then introduced. Special emphasis is placed on checking assumptions, exploratory data analysis and presentation of results. The main analyses are illustrated with many examples from published papers and there is an extensive reference list to both the statistical and biological literature. The book is supported by a website that provides all data sets, questions for each chapter and links to software.\n  James et al. (2013): An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform. Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.\n  Emden (2008): The typical biology student is “hardwired” to be wary of any tasks involving the application of mathematics and statistical analyses, but the plain fact is much of biology requires interpretation of experimental data through the use of statistical methods. This unique textbook aims to demystify statistical formulae for the average biology student. Written in a lively and engaging style, Statistics for Terrified Biologists draws on the author’s 30 years of lecturing experience. One of the foremost entomologists of his generation, van Emden has an extensive track record for successfully teaching statistical methods to even the most guarded of biology students. For the first time basic methods are presented using straightforward, jargon-free language. Students are taught to use simple formulae accurately to interpret what is being measured with each test and statistic, while at the same time learning to recognize overall patterns and guiding principles. Complemented by simple illustrations and useful case studies, this is an ideal statistics resource tool for undergraduate biology and environmental science students who lack confidence in their mathematical abilities.\n  Agresti (2002): The use of statistical methods for categorical data has increased dramatically, particularly for applications in the biomedical and social sciences. Responding to new developments in the field as well as to the needs of a new generation of professionals and students, this new edition of the classic Categorical Data Analysis offers a comprehensive introduction to the most important methods for categorical data analysis. Designed for statisticians and biostatisticians as well as scientists and graduate students practicing statistics, Categorical Data Analysis, Second Edition summarizes the latest methods for univariate and correlated multivariate categorical responses. Readers will find a unified generalized linear models approach that connects logistic regression and Poisson and negative binomial regression for discrete data with normal regression for continuous data.\n  van Belle (2008): This book contains chapters titled:\n Begin with a Basic Formula for Sample Size–Lehr’s Equation Calculating Sample Size Using the Coefficient of Variation Ignore the Finite Population Correction in Calculating Sample Size for a Survey The Range of the Observations Provides Bounds for the Standard Deviation * Do not Formulate a Study Solely in Terms of Effect Size Overlapping Confidence Intervals do not Imply Nonsignificance Sample Size Calculation for the Poisson Distribution Sample Size Calculation for Poisson Distribution with Background Rate Sample Size Calculation for the Binomial Distribution When Unequal Sample Sizes Matter; When They Don’t * Determining Sample Size when there are Different Costs Associated with the Two Samples Use the Rule of Threes for 95% Upper Bounds when there Have Been No Events Sample Size Calculations Should be Based on the Way the Data will be Analyzed    Grolemund \u0026amp; Wickham (2016): This is the website for {“}R for Data Science{”}. This book will teach you how to do data science with R: You’ll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you’ll learn how to clean data and draw plots—and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You’ll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You’ll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.\n  Baddeley et al. (2015): Spatial Point Patterns: Methodology and Applications with R shows scientific researchers and applied statisticians from a wide range of fields how to analyze their spatial point pattern data. Making the techniques accessible to non-mathematicians, the authors draw on their 25 years of software development experiences, methodological research, and broad scientific collaborations to deliver a book that clearly and succinctly explains concepts and addresses real scientific questions. Practical Advice on Data Analysis and Guidance on the Validity and Applicability of Methods The first part of the book gives an introduction to R software, advice about collecting data, information about handling and manipulating data, and an accessible introduction to the basic concepts of point processes. The second part presents tools for exploratory data analysis, including non-parametric estimation of intensity, correlation, and spacing properties. The third part discusses model-fitting and statistical inference for point patterns. The final part describes point patterns with additional {“}structure,{”} such as complicated marks, space-time observations, three- and higher-dimensional spaces, replicated observations, and point patterns constrained to a network of lines. Easily Analyze Your Own Data Throughout the book, the authors use their spatstat package, which is free, open-source code written in the R language. This package provides a wide range of capabilities for spatial point pattern data, from basic data handling to advanced analytic tools. The book focuses on practical needs from the user’s perspective, offering answers to the most frequently asked questions in each chapter.\n  Hobbs \u0026amp; Hooten (2015): Bayesian modeling has become an indispensable tool for ecological research because it is uniquely suited to deal with complexity in a statistically coherent way. This textbook provides a comprehensive and accessible introduction to the latest Bayesian methods—in language ecologists can understand. Unlike other books on the subject, this one emphasizes the principles behind the computations, giving ecologists a big-picture understanding of how to implement this powerful statistical approach. Bayesian Models is an essential primer for non-statisticians. It begins with a definition of probability and develops a step-by-step sequence of connected ideas, including basic distribution theory, network diagrams, hierarchical models, Markov chain Monte Carlo, and inference from single and multiple models. This unique book places less emphasis on computer coding, favoring instead a concise presentation of the mathematical statistics needed to understand how and why Bayesian analysis works. It also explains how to write out properly formulated hierarchical Bayesian models and use them in computing, research papers, and proposals. This primer enables ecologists to understand the statistical principles behind Bayesian modeling and apply them to research, teaching, policy, and management.\n Presents the mathematical and statistical foundations of Bayesian modeling in language accessible to non-statisticians Covers basic distribution theory, network diagrams, hierarchical models, Markov chain Monte Carlo, and more - Deemphasizes computer coding in favor of basic principles Explains how to write out properly factored statistical expressions representing Bayesian models    Zuur et al. (2017): In Volume I we explain how to apply linear regression models, generalised linear models (GLM), and generalised linear mixed-effects models (GLMM) to spatial, temporal, and spatial-temporal data. The models that will be employed use the Gaussian and gamma distributions for continuous data, the Poisson and negative binomial distributions for count data, the Bernoulli distribution for absence–presence data, and the binomial distribution for proportional data.In Volume II we apply zero-inflated models and generalised additive (mixed-effects) models to spatial and spatial-temporal data. We also discuss models with more exotic distributions like the generalised Poisson distribution to deal with underdispersion and the beta distribution to analyse proportional data.\n  Zuur et al. (2010):\n While teaching statistics to ecologists, the lead authors of this paper have noticed common statistical problems. If a randomsample of theirwork (including scientific papers) produced before doing these courses were selected, half would probably contain violations of the underlying assumptions of the statistical techniquesemployed. Some violations have little impact on the results or ecological conclusions; yet others increase type I or type II errors, potentially resulting in wrong ecological conclusions. Most of these violations can be avoided by applying better data exploration. These problems are especially trouble- somein applied ecology, wheremanagement and policy decisions are often at stake. Here, we provide a protocol for data exploration; discuss current tools to detect outliers, heterogeneity of variance, collinearity, dependence of observations, problems with interactions, double zeros in multivariate analysis, zero inflation in generalized linear modelling, and the correct type of relationships between dependent and independent variables; and provide advice on how to address these problems when they arise. We also address misconceptions about normality, and provide advice on data transformations. Data exploration avoids type I and type II errors, among other problems, thereby reducing the chance ofmaking wrong ecological conclusions and poor recommendations. It is therefore essential for good quality management and policy based on statistical analyses. Key-words:    Kelleher \u0026amp; Wagener (2011): Our ability to visualize scientific data has evolved significantly over the last 40 years. However, this advancement does not necessarily alleviate many common pitfalls in visualization for scientific journals, which can inhibit the ability of readers to effectively understand the information presented. To address this issue within the context of visualizing environmental data, we list ten guidelines for effective data visualization in scientific publications. These guidelines support the primary objective of data visualization, i.e. to effectively convey information. We believe that this small set of guidelines based on a review of key visualization literature can help researchers improve the communication of their results using effective visualization. Enhancement of environmental data visualization will further improve research presentation and communication within and across disciplines.\n  Lohr (2010): Sharon L. Lohr’s SAMPLING: DESIGN AND ANALYSIS, 2ND EDITION, provides a modern introduction to the field of survey sampling intended for a wide audience of statistics students. Practical and authoritative, the book is listed as a standard reference for training on real-world survey problems by a number of prominent surveying organizations. Lohr concentrates on the statistical aspects of taking and analyzing a sample, incorporating a multitude of applications from a variety of disciplines. The text gives guidance on how to tell when a sample is valid or not, and how to design and analyze many different forms of sample surveys. Recent research on theoretical and applied aspects of sampling is included, as well as optional technology instructions for using statistical software with survey data.\n  Zuur et al. (2009): Building on the successful Analysing Ecological Data (Zuur et al., 2007), the authors now provide an expanded introduction to using regression and its extensions in analysing ecological data. As with the earlier book, real data sets from postgraduate ecological studies or research projects are used throughout. The first part of the book is a largely non-mathematical introduction to linear mixed effects modelling, GLM and GAM, zero inflated models, GEE, GLMM and GAMM. The second part provides ten case studies that range from koalas to deep sea research. These chapters provide an invaluable insight into analysing complex ecological datasets, including comparisons of different approaches to the same problem. By matching ecological questions and data structure to a case study, these chapters provide an excellent starting point to analysing your own data. Data and R code from all chapters are available from www.highstat.com.\n  Zuur \u0026amp; Ieno (2016):\n Scientific investigation is of value only insofar as relevant results are obtained and communicated, a task that requires organizing, evaluating, analysing and unambiguously communicating the significance of data. In this context, working with ecological data, reflecting the complexities and interactions of the natural world, can be a challenge. Recent innovations for statistical analysis ofmultifaceted interrelated datamake obtaining more accu- rate andmeaningful results possible, but key decisions of the analyses to use, and which components to present in a scientific paper or report, may be overwhelming. We offer a 10-step protocol to streamline analysis of data thatwill enhance understanding of the data, the statistical models and the results, and optimize communication with the reader with respect to both the procedure and the outcomes. The protocol takes the investigator from study design and organization of data (formulating relevant questions, visualizing data collection, data exploration, identifying dependency), through conducting analysis (presenting, fitting and validating the model) and presenting output (numerically and visually), to extending themodel via simulation. Each step includes procedures to clarify aspects of the data that affect statistical analysis, as well as guidelines for written presentation. Steps are illustrated with examples using data from the literature. Following this protocol will reduce the organization, analysis and presentation ofwhatmay be an overwhelming information avalanche into sequential and, more to the point, manageable, steps. It provides guidelines for selecting optimal statistical tools to assess data relevance and significance, for choosing aspects of the analysis to include in a published report and for clearly communicating information.    Gelman \u0026amp; Hill (2007): Data Analysis Using Regression and Multilevel/Hierarchical Models is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors’ own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout.\n  Lindenmayer \u0026amp; Likens (2010): Long-term monitoring programs are fundamental to understanding the natural environment and effectively tackling major environmental problems. Yet they are often done very poorly and ineffectively. Effective Ecological Monitoring describes what makes successful and unsuccessful long-term monitoring programs. Short and to the point, it illustrates key aspects with case studies and examples. It is based on the collective experience of running long-term research and monitoring programs of the two authors – experience which spans more than 70 years. The book first outlines why long-term monitoring is important, then discusses why long-term monitoring programs often fail. The authors then highlight what makes good and effective monitoring. These good and bad aspects of long-term monitoring programs are further illustrated in the fourth chapter of the book. The final chapter sums up the future of long-term monitoring programs and how to make them better, more effective and better targeted.\n  Bolker (2008): Ecological Models and Data in R is the first truly practical introduction to modern statistical methods for ecology. In step-by-step detail, the book teaches ecology graduate students and researchers everything they need to know in order to use maximum likelihood, information-theoretic, and Bayesian techniques to analyze their own data using the programming language R. Drawing on extensive experience teaching these techniques to graduate students in ecology, Benjamin Bolker shows how to choose among and construct statistical models for data, estimate their parameters and confidence limits, and interpret the results. The book also covers statistical frameworks, the philosophy of statistical modeling, and critical mathematical functions and probability distributions. It requires no programming background–only basic calculus and statistics.\n Practical, beginner-friendly introduction to modern statistical techniques for ecology using the programming language R Step-by-step instructions for fitting models to messy, real-world data Balanced view of different statistical approaches Wide coverage of techniques – from simple (distribution fitting) to complex (state-space modeling) Techniques for data manipulation and graphical display Companion Web site with data and R code for all examples    Bibliography Agresti A. (2002). Categorical Data Analysis (Second Edition). John Wiley \u0026amp; Sons, Inc.\nBaddeley A., Rubak E. \u0026amp; Turner R. (2015). Spatial Point Patterns: Methodology and Applications with R. Chapman; Hall/CRC, Boca Raton.\nBolker B.M. (2008). Ecological Models and Data in R. Princeton University Press, Princeton, NJ.\nEmden H. van (2008). Statistics for Terrified Biologists. Blackwell Publishing.\nGelman A. \u0026amp; Hill J. (2007). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press, Cambridge. URL: http://www.loc.gov/catdir/enhancements/fy0668/2006040566-t.html.\nGrolemund G. \u0026amp; Wickham H. (2016). R for Data Science. URL: http://r4ds.had.co.nz/.\nHobbs N.T. \u0026amp; Hooten M.B. (2015). Bayesian Models: A Statistical Primer for Ecologists. Princeton University Press.\nJames G., Witten D., Hastie T. \u0026amp; Tibshirani R. (2013). An Introduction to Statistical Learning with Applications in R. Springer.\nKass R.E., Caffo B.S., Davidian M., Meng X.-L., Yu B. \u0026amp; Reid N. (2016). Ten Simple Rules for Effective Statistical Practice. PLOS Computational Biology 12 (6): e1004961. URL: http://dx.plos.org/10.1371/journal.pcbi.1004961. DOI: 10.1371/journal.pcbi.1004961.\nKelleher C. \u0026amp; Wagener T. (2011). Ten guidelines for effective data visualization in scientific publications. Environmental Modelling \u0026amp; Software 26 (6): 822–827. URL: https://www.sciencedirect.com/science/article/pii/S1364815210003270. DOI: 10.1016/J.ENVSOFT.2010.12.006.\nLindenmayer D. \u0026amp; Likens G.E. (2010). Effective ecological monitoring. Earthscan, London, UK.\nLohr S.L. (2010). Sampling: Design and Analysis, Second Edi. ed. Brooks/Cole.\nMcElreath R. (2015). Statistical rethinking : a Bayesian course with examples in R and Stan. Chapman; Hall/CRC, Boca Raton.\nQuinn G. \u0026amp; Keough M. (2002). Experimental design and data analysis for biologists. Cambridge University Press. URL: http://www.cambridge.org.\nvan Belle G. (2008). Statistical Rules of Thumb: Second Edition. John Wiley \u0026amp; Sons, Inc. DOI: 10.1002/9780470377963.\nZuur A.F. \u0026amp; Ieno E.N. (2016). A protocol for conducting and presenting results of regression-type analyses. Methods in Ecology and Evolution 7 (6): 636–645. URL: http://doi.wiley.com/10.1111/2041-210X.12577. DOI: 10.1111/2041-210X.12577.\nZuur A.F., Ieno E.N. \u0026amp; Elphick C.S. (2010). A protocol for data exploration to avoid common statistical problems. Methods in Ecology and Evolution 1 (9999): 3–14.\nZuur A.F., Ieno E.N. \u0026amp; Smith G.M. (2007). Analysing ecological data. Springer Verlag.\nZuur A.F., Ieno E.N., Anatoly, A \u0026amp; Saveliev (2017). Beginner’s guide to spatial, temporal, and spatial-temporal ecological data analysis with R-INLA. Highland Statistics Ltd. URL: http://www.highstat.com/Books/BGS/SpatialTemp/Zuuretal2017_TOCOnline.pdf.\nZuur A.F., Ieno E.N., Walker N.J., Saveliev A.A. \u0026amp; Smith G.M. (2009). Mixed effects models and extensions in ecology with R. Springer.\n","href":"/articles/statistics/","title":"Books and articles on statistics"},{"content":"This page lists selected literature and online resources. Some are related to existing tutorial pages, while others are not. They are supposed to be of high interest to this site’s users.\nSeveral of the resources were added based on an inspiring talk by Julia Lowndes at the SAFRED conference, Brussels, 27 Feb 2018.\nSharing data  Ellis \u0026amp; Leek (2017): guidelines for providing data to a scientist / statistician: provide raw data, format consistently, include metadata \u0026amp; preprocessing steps Wilkinson et al. (2016): the FAIR data principles: date are to be findable, accessible, interoperable and reusable Culina et al. (2018): overview of online data infrastructures and considerations to be made Perkel (2016): data repository sites like github and others  Communicating  Smith et al. (2016): recommendations of the FORCE11 Software Citation Working Group  Bibliography Culina A., Baglioni M., Crowther T.W., Visser M.E., Woutersen-Windhouwer S. \u0026amp; Manghi P. (2018). Navigating the unfolding open data landscape in ecology and evolution. Nature Ecology \u0026amp; Evolution 2 (3): 420–426. https://doi.org/10.1038/s41559-017-0458-2.\nEllis S.E. \u0026amp; Leek J.T. (2017). How to share data for collaboration. PeerJ Preprints 5: e3139v5. https://doi.org/10.7287/peerj.preprints.3139v5.\nPerkel J. (2016). Democratic databases: Science on GitHub. Nature News 538 (7623): 127. https://doi.org/10.1038/538127a.\nSmith A.M., Katz D.S. \u0026amp; Niemeyer K.E. (2016). Software citation principles. PeerJ Computer Science 2: e86. https://doi.org/10.7717/peerj-cs.86.\nWilkinson M.D., Dumontier M., Aalbersberg I.J., Appleton G., Axton M., Baak A., Blomberg N., Boiten J.-W., Silva Santos L.B. da, Bourne P.E., Bouwman J., Brookes A.J., Clark T., Crosas M., Dillo I., Dumon O., Edmunds S., Evelo C.T., Finkers R., Gonzalez-Beltran A., Gray A.J.G., Groth P., Goble C., Grethe J.S., Heringa J., Hoen P.A.C. ’t, Hooft R., Kuhn T., Kok R., Kok J., Lusher S.J., Martone M.E., Mons A., Packer A.L., Persson B., Rocca-Serra P., Roos M., Schaik R. van, Sansone S.-A., Schultes E., Sengstag T., Slater T., Strawn G., Swertz M.A., Thompson M., Lei J. van der, Mulligen E. van, Velterop J., Waagmeester A., Wittenburg P., Wolstencroft K., Zhao J. \u0026amp; Mons B. (2016). The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data 3: 160018. http://dx.doi.org/10.1038/sdata.2016.18.\n","href":"/articles/communicating/","title":"Sharing and communicating"},{"content":"Real life datasources seldom provide data in exactly the format you need for the analysis. Hence most of the time you need to manipulate the data after reading it into R. There are several ways to do this, each with their pros and cons. We highly recommend the tidyverse collection of packages. The command library(tidyverse) will actually load the following packages: ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr and forecats.\nWhere to find good information on these packages:  official tidyverse website the R for data science book (R4DS) by Garrett Grolemund and Hadley Wickham. Note that this book is freely available online. A printed version is available at the INBO library. video tutorials:  Data wrangling with R and RStudio: a good introduction on dplyr and tidyr by Garrett Grolemund dplyr tutorial at useR!2014 by Hadley Wickham (video part 1 and part 2) tidyverse, visualization, and manipulation basics: a high-level overview of tidyverse by Garrett Grolemund   Data Transformation Cheat Sheet: a two page document which covers the most important function for dplyr  ","href":"/tutorials/r_tidyverse_info/","title":"Data wrangling with tidyverse"},{"content":"See the vignette/tutorial at https://inbo.github.io/inborutils/articles/guess_projection.html\n","href":"/tutorials/vignette_inborutils_guess_projection/","title":"inborutils: Check coordinate system"},{"content":"See the vignette/tutorial at https://docs.ropensci.org/wateRinfo/articles/define_date_periods.html\n","href":"/tutorials/vignette_waterinfo_define_date_periods/","title":"wateRinfo: Define the date period to download"},{"content":"See the vignette/tutorial at https://docs.ropensci.org/wateRinfo/articles/download_timeseries_batch.html\n","href":"/tutorials/vignette_waterinfo_download_timeseries_batch/","title":"wateRinfo: Download time series from multiple stations/variables"},{"content":"Scope This style guide is a recommendation for all R code written for the Research Institute for Nature and Forest (INBO). The goal of this style guide is twofold. First of all applying the guidelines will result in readable code. Secondly, it is much easier to work together on code when everyone is using the same guidelines. It is likely that applying these guidelines will have consequences on the current style used by many R users at INBO. Therefore this style guide should be applied within reason.\n Don’t apply the style guide to existing code. R users are free to apply the style guide to new personal R code. Using the style guide is highly recommended for new or revised R code intended to be distributed and used by other R users. The style guide is mandatory for new or revised R packages distributed by INBO.  Please note that the RStudio editor has some handy features that automatically highlights errors against the code style in a non intrusive way. RStudio hints in this document are the instructions to activate these diagnostics in RStudio.\nSyntax RStudio hint: Tools \u0026gt; Global options \u0026gt; Code \u0026gt; Diagnostics: Check everything\nGeneral  lines should not exceed 80 characters  split the command over multiple lines if the command is longer than 80 characters RStudio hint: Tools \u0026gt; Global options \u0026gt; Code \u0026gt; Display: Check Show margin and set Margin column to 80   object names should be meaningful object names should not exceed 20 characters object names should be lowercase use _ to separate words in object names  function names with a single dot are allowed   use double quotes (\u0026quot;) around characters and not single quotes (') don’t add commented code  use version control if you want to keep old versions of code    # Good example_text \u0026lt;- example_function( first_argument = \u0026#34;Some text\u0026#34;, second_argument = \u0026#34;More text\u0026#34; ) # Bad some.really.long.dot.separated.name \u0026lt;- MyCoolFunction(FirstArgument = \u0026#39;Some text\u0026#39;, second.argument = \u0026#39;More text\u0026#39;) Whitespace naturallanguagesusewhitespaceandpunctuationtomaketextsmorereadableprogramminglanguagesarenoexceptiontothisrule  Natural languages use whitespace and punctuation to make texts more readable. Programming languages are no exception to this rule.\n don’t use tabs, use two spaces instead  RStudio hint: Tools \u0026gt; Global options \u0026gt; Code \u0026gt; Editing: Check Insert spaces for tab and set Tab width to 2   no space before a comma, one space after a comma one space before and after an infix operator (+, -, *, /, ^, \u0026amp;, |, %%, %/%, %*%, %in%, …) no spaces a the end of a line  RStudio hint: Tools \u0026gt; Global options \u0026gt; Code \u0026gt; Saving: Check Strip trailing horizontal whitespace when saving   end the script file with a single blank line  RStudio hint: Tools \u0026gt; Global options \u0026gt; Code \u0026gt; Saving: Check Ensure that source files end with newline    Assignments  only create an object when you will use it later on always use \u0026lt;- for assignment always put the new variable on the left and never use -\u0026gt; use = only for passing arguments in a function at least one space before and at least one space after \u0026lt;- and =  use multiple spaces if it improves readability    # Good x \u0026lt;- data.frame(z = 1:10) summary(x) # Bad x=data.frame(z\u0026lt;-1:10) 5 -\u0026gt; a # Improved readability example a \u0026lt;- 5 ab \u0026lt;- 10 abc \u0026lt;- 7 d \u0026lt;- 245 Brackets R uses three types of brackets: round (...), square [...] and curly {...}.\n no spaces after opening a bracket no spaces before closing a bracket no spaces before opening a bracket except:  one space with control flow functions (if, else, for, while)   no spaces after closing a bracket except:  one space with control flow functions (if, else, for, while)   { should not start on a newline and is always the end of a line apply indentation when splitting long text inside brackets over multiple lines  # Good y \u0026lt;- seq(0, 2) if (max(y) \u0026lt;= 10) { x \u0026lt;- 1 } else { x \u0026lt;- 2 } sapply( y, function(x){ return(x) } ) # Bad y \u0026lt;- seq (0, 2 ) if( max( y ) \u0026lt;= 10 ) { x \u0026lt;- 1 } else { x \u0026lt;- 2 } sapply( y , function ( x ) { return(x)}) Special cases and exceptions  selecting rows with square brackets df[selection, ]  this results in two conflicting rules  a single space after a comma no space before a bracket   solution in case of a short command: add # nolint after the command  df[selection, ] # nolint   solution in case of a long command: split the command over several lines    # Good relevant_subset \u0026lt;- original_dataframe[ original_dataframe$x \u0026gt; some_value | original_dataframe$y \u0026lt; some_other_value, ] # Recommended dplyr alternative relevant_subset \u0026lt;- original_dataframe %\u0026gt;% filter(x \u0026gt; some_value | y \u0026lt; some_other_value) # Bad relevant_subset \u0026lt;- original_dataframe[original_dataframe$x \u0026gt; some_value | original_dataframe$y \u0026lt; some_other_value, ] # nolint  a really long text  text shorter than 80 characters but passed the 80 character limit due to the indentitation  solution: remove all indentation   text longer than 80 characters  solution: add # nolint at the end of the line     functions from other packages with names that don’t comply with this style guide  solution: add # nolint at the end of the line    Important notice\nAdding # nolint at the end of a line excludes that line from the automatic checks for coding styles. Therefore use it only when you have no other options.\nFunctions  always explicitly mention return in functions  # Good sum \u0026lt;- function(x, y) { z \u0026lt;- x + y return(z) } sum \u0026lt;- function(x, y) { return(x + y) } # Bad sum \u0026lt;- function(x, y) { x + y } Validating syntax The code below validate the syntax for an R file, an RMarkdown file or an R package.\nRStudio hint: Running this code within RStudio will open a Markers pane, indicating the filename, line number and the kind of syntax error that occurs. Double clicking on the error will open the file at the correct location, making it easy to rectify the problem.\nExtra hint: start correcting for the last lines and work your way forward. This leaves the line numbers of the errors intact until you solve them.\nAn example to clarify this. Suppose you have an error at line 10 and an error at line 100. Both errors are lines should not be more than 80 characters, so to solve them we have to split the lines over multiple lines.\nLet say that we start with solving line 10 by splitting it over four lines. So the old line 10 becomes the new lines 10 to 13. Hence the old line 11 becomes the new line 14, and the old line 100 becomes the new line 103. When we now click on the marker for line 100, RStudio will go the current line 100 which is the old line 97. So you end up looking for an error at the wrong position.\nStarting at the back solved this issue. In the same example we would start by solving line 100. Let’s assume we split this over two lines. So old line 100 because new line 100 and 101. Old line 101 becomes new line 102 but more importantly all line numbers before 100 are unchanged. So clicking on the marker for line 10 will take you the current line 10 which is the old line 10.\n# validate a single file lintr::lint(filename = \u0026#34;file.R\u0026#34;) lintr::lint(filename = \u0026#34;file.Rmd\u0026#34;) # validate a package lintr::lint_package(path = \u0026#34;.\u0026#34;) Documentation Functions  Add documentation above each function with Roxygen markup Add inline comments where relevant  Required Roxygen tags #\u0026#39; @title Title of the helpfile #\u0026#39; @description Description of the function in the helpfile #\u0026#39; @param define a parameter #\u0026#39; @export is the function exported by the package NAMESPACE #\u0026#39; @importFrom import a function from another package Optional Roxygen tags #\u0026#39; @seealso link to other functions #\u0026#39; @section section title #\u0026#39; @alias other name for the topic #\u0026#39; @keywords a set of standardised keywords. See file.path(R.home(\u0026#34;doc\u0026#34;), \u0026#34;KEYWORDS\u0026#34;) #\u0026#39; @inheritParams inherit the definition of parameters from another function #\u0026#39; @examples a working example of the function #\u0026#39; @return a description of the output from the function See http://r-pkgs.had.co.nz/man.html#roxygen-comments for more information on Roxygen\nINBO extra requirements for package DESCRIPTION  license: MIT or GPL-3? In case of MIT a LICENSE file should be added and License: MIT to the DESCRIPTION. In case of GPL-3 it is sufficient to add License: GPL-3 to the DESCRIPTION list of authors in Authors@R format  INBO is listed as copyright holder one or more roles are atributed to each person  cre: package maintainer (only one person) aut: main author (at least one person) ctb: contributor (if relevant) cph: copyright holder (must be INBO)      Authors@R: c(person(“Els”, “Lommelen”, email = “els.lommelen@inbo.be”, role = c(“aut”, “cre”)), person(“Thierry”, “Onkelinx”, email = “thierry.onkelinx@inbo.be”, role = “aut”), person(“Anja”, “Leyman”, email = “anja.leyman@inbo.be”, role = “ctb”), person(family = “Research Institute for Nature and Forest (INBO)”, email = “info@inbo.be”, role = “cph”))\nHow-to’s  Add one or more how-to’s to a package Add them as RMarkdown vignettes  File structure R Package Functions  all generic R functions should be distributed as an R package use devtools::create() to start a new pacakge  RStudio hint: File \u0026gt; New project \u0026gt; New directory \u0026gt; R Package: Type the name in Package name   keep source files compact  create a separate file for each function, with the file name equal to the function name. This makes it easy to find the correct source file. exception: very short auxilary functions with related functionality  related functions can be bundled into one R script file name is either equal to the most important function or describes the related functionality     split large functions into several subfunctions  Scripts  place scripts in the inst folder  the scripts will be available for the user after installing the package the location of the scripts can be found with system.file(\u0026quot;script-name.R\u0026quot;, package = \u0026quot;yourpackage\u0026quot;) use a relevant folder structure when adding lots of files to inst    Unit tests  use the testthat package for unit tests  use devtools::use_testthat() to setup the test infrastructure   all unit tests are stored in tests/testthat all files should have either a test_ or helper_ prefix  files with helper_ prefix contain auxiliary function for the tests but no tests   the test files will be run in alphabetical order  setting the order of the files is easy by adding 3 letters to the prefix (eg. test_aaa_, test_baa, test_zzz_) 3 letters offers quite some flexibility to insert new files at the correct location without having to rename at lot of files. If the first file is test_aaa_ and the second test_baa, they you can 675 files between the two.   unit test files can be larger than source files a unit test file can contain tests for several functions in case the functions are strongly related (e.g. subfunctions) and reuse test cases each package should contain the unit for coding style as listed below  store this in a file tests/testthat/test_zzz_coding_style.R add this file to .gitignore  the coding style will be tested separately when using continuous integration      if (requireNamespace(\u0026#34;lintr\u0026#34;, quietly = TRUE)) { context(\u0026#34;lints\u0026#34;) test_that(\u0026#34;Package Style\u0026#34;, { lintr::expect_lint_free() }) } Data To make data available for users, they can be stored in a package in 3 different file types:\n plain text file (.txt, .csv,…): use if  your package is under version control and the data often change you want to keep track on the changes using version control consider to keep row and column order fixed   binary file (.Rdata or .rda): use if  your dataset is large and data do not change between different versions you are not interested in keeping track on the exact changes you want to keep the exact format of the data (e.g. factors with levels) (possible in plain text with git2rdata)   code (.r) generating a table: use if the data can easily be generated with code  Data can be stored in 3 places:\n to make data available for loading and parsing examples, store them in the folder inst/extdata. Access this data with system.file(\u0026quot;xyz\u0026quot;, package = \u0026quot;abc\u0026quot;). Possible for all data types. to make data available to package users, store them in the data folder. Access this data with data(xyz). Possible only for data formats that can be handled by data(). Binary .rda-files can be stored by using usethis::use_data(xyz). to keep data internal for use by your functions, store them in the file R/Sysdata.rda by using usethis::use_data(xyz, internal = TRUE). Access this data with abc::xyz.  (In the above examples, xyz are data and abc is the package in which they are stored.)\nAdd scripts for generating these data in the folder data-raw and create this folder by using usethis::use_data_raw() (ignores folder during build).\nR script  group a long set of commands with similar functionality into a dedicated function  e.g. prepare_data(), do_analysis(), create_figure(), …   place the user defined functions in a separate file which you source() into the main script  it is better to use the same file structure as an R package consider writing a simple package in case you have a lot of functions    RMarkdown  each chunk has only one output (figure, table, summary, …) don’t mix (heavy) calculations and output in the same chunk: this is more interesting for caching the results give chunks a relevant name: this make debugging easier and file name of figures and Bookdown label will be based on the chunk name avoid writing code that generates Markdown  use (parametrised) child documents instead   use the bookdown version for long reports: this makes it easy to split a long report into several child documents  Recommended packages Data import  readr: import text files readxl: import Excel files googlesheets: import Google Sheets DBI: connect to databases PostgreSQL, SQLite, MySQL, Oracle, … RODBC: connect to databases SQL Server, Access  Data manipulation \u0026amp; transformation  dplyr:  subsetting observations subsetting variables changing variables aggregation combining dataframes   tidyr:  changing a dataframe from wide to long format and vice versa nesting and unnesting dataframes splitting a single variable into multiple variables    Graphics  ggplot2:all static graphics, charts and plots INBOtheme: INBO corporate identity for ggplot2 graphics  Quality control  lintr: checking coding style testthat: writing unit tests covr: check which part of the code is not covered by unit tests  ","href":"/tutorials/styleguide_r_code/","title":"Styleguide R code"},{"content":"Na de eerste installatie  Start Rstudio Kies in het menu Tools -\u0026gt; Global Options In het tabblad General  Pas de Default working directory aan naar de locatie waar je R versie staat (C:/R/R-4.x.y) 1 Restore .RData into workspace at startup: uitvinken Save workspace to.RData on exit: Never   In het tabblad Code  Subtab Editing  Insert spaces for tab: aanvinken Tab width: 2 Soft-wrap R source files: aanvinken   Subtab Saving  Default text encoding: UTF-8   Subtab Diagnostics  Alles aanvinken     In het tabblad Appearance  Stel in naar eigen smaak   In het tabblad Packages  CRAN mirror: wijzigen naar Global (CDN) - RStudio   In het tabblad Sweave  Weave Rnw files using: knitr Typeset LaTeX into PDF using: XeLaTex   Klik op OK en herstart RStudio  Configuratie van RStudio na een upgrade van R  Start RStudio Kies in het menu Tools -\u0026gt; Global Options Indien niet de laatste versie vermeld staat bij R version: klik op Change om het aan te passen. Klik op OK als je een waarschuwing krijgt dat je RStudio moet herstarten. Wijzig de default working directory in C:/R/R-4.x.y 1 Klik op OK Herstart RStudio    x en y verwijzen naar het versienummer. Dus bij R-4.1.0 is x = 1 en y = 0. De working directory is in dat geval C:/R/R-4.1.0\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","href":"/installation/user/user_install_rstudio/","title":"Rstudio installation"},{"content":"Na de eerste installatie Bij een nieuwe R installatie hoeft de gebruiker geen bijkomende stappen te ondernemen.\nNa elke upgrade Voer onderstaande instructies uit telkens een nieuwe R versie geïnstalleerd werd. Je kan dit, indien gewenst, ook frequenter uitvoeren.\nWindows  Zorg dat de computer verbonden is met het internet. Zorg dat er geen enkele R sessie actief is op de computer. Start R x64 x.y.z via het menu Start \u0026gt; Alle programma\u0026rsquo;s \u0026gt; R. x.y.z verwijst naar de versie die geïnstalleerd werd. Tik het commando update.packages(ask = FALSE, checkBuilt = TRUE) gevolgd door enter. Er zullen nu packages waarvan een nieuwe versie beschikbaar is gedownload en geïnstalleerd worden. Dit duurt een hele poos, afhankelijk van het aantal te upgraden packages. Wacht tot de installatie volledig afgelopen is. Tik vervolgens het commando q() gevolgd door enter. R zal nu afgesloten worden.  Linux  Zorg dat de computer verbonden is met het internet. Zorg dat er geen enkele R sessie actief is op de computer. Open een terminal venster (Ctrl + Alt + T). Voer het commando Rscript -e 'update.packages(ask = FALSE, checkBuilt = TRUE)' uit. Er zullen nu packages waarvan een nieuwe versie beschikbaar is gedownload en geïnstalleerd worden. Dit duurt een hele poos, afhankelijk van het aantal te upgraden packages. Wacht tot de installatie volledig afgelopen is. Sluit de terminal met exit.  ","href":"/installation/user/user_install_r/","title":"R language installation"},{"content":"Introduction You notice you have done something wrong on your branch. No worries, erroneous commits can be undone or reverted.\nFirst, check your history of commits to see which commits are the faulty ones.\ngit log --oneline Example output\nb7119f2 Continue doing crazy things 872fa7e Try something crazy a1e8fb5 Make some important changes to hello.py 435b61d Create hello.py 9773e52 Initial import Git checkout The git checkout command serves three distinct functions: checking out files, checking out commits, and checking out branches. In this part, only the first two configurations are addressed. Checking out a commit makes the entire working directory match that commit. This can be used to view an old state of your project without altering your current state in any way. Checking out a file lets you see an old version of that particular file, leaving the rest of your working directory untouched. (This will put you in a detached HEAD state.)\nYou can use git checkout to view the “Make some import changes to hello.py” commit as follows:\ngit checkout a1e8fb5 This makes your working directory match the exact state of the a1e8fb5 commit. You can look at files, and even edit files without worrying about losing the current state of the project. Nothing you do in here will be saved in your repository. Checking out an old commit is a read-only operation. It’s impossible to harm your repository while viewing an old revision. To continue developing, you need to get back to the “current” state of your project (assuming your master branch is the head of the project):\ngit checkout master If you’re only interested in a single file, you can also use git checkout to fetch an old version of it. For example, if you only wanted to see the hello.py file from the old commit, you could use the following command:\ngit checkout a1e8fb5 hello.py Remember, unlike checking out a commit, this does affect the current state of your project. The old file revision will show up as a “Change to be committed,” giving you the opportunity to revert back to the previous version of the file. If you decide you don’t want to keep the old version, you can check out the most recent version with the following:\ngit checkout HEAD hello.py This concludes the part on checking your previous commits. In the following part, some methods of rollback to a previous state will be elucidated.\ngit revert The git revert command undoes a committed snapshot. But, instead of removing the commit from the project history, it figures out how to undo the changes introduced by the commit and appends a new commit with the resulting content. This prevents Git from losing history, which is important for the integrity of your revision history and for reliable collaboration.\nUsage:\ngit revert \u0026lt;commit\u0026gt; Generate a new commit that undoes all of the changes introduced in , then apply it to the current branch.\nReverting should be used when you want to remove an entire commit from your project history. This can be useful, for example, if you’re tracking down a bug and find that it was introduced by a single commit. Instead of manually going in, fixing it, and committing a new snapshot, you can use git revert to automatically do all of this for you.\nIt\u0026rsquo;s important to understand that git revert undoes a single commit—it does not “revert” back to the previous state of a project by removing all subsequent commits. In Git, this is actually called a reset, not a revert.\nReverting has two important advantages over resetting. First, it doesn’t change the project history, which makes it a “safe” operation for commits that have already been published to a shared repository. For details about why altering shared history is dangerous, please see the git reset page.\nSecond, git revert is able to target an individual commit at an arbitrary point in the history, whereas git reset can only work backwards from the current commit. For example, if you wanted to undo an old commit with git reset, you would have to remove all of the commits that occurred after the target commit, remove it, then re-commit all of the subsequent commits. Needless to say, this is not an elegant undo solution.\nExample\nThe following example is a simple demonstration of git revert. It commits a snapshot, then immediately undoes it with a revert.\n// edit some tracked files // commit a snapshot git commit -m 'make some changes that will be undone' // revert the commit we have just created git revert HEAD This can be visualized as the following:\nNote that the 4th commit is still in the project history after the revert. Instead of deleting it, git revert added a new commit to undo its changes. As a result, the 3rd and 5th commits represent the exact same code base, and the 4th commit is still in our history just in case we want to go back to it down the road.\ngit reset If git revert is a “safe” way to undo changes, you can think of git reset as the dangerous method. When you undo with git reset(and the commits are no longer referenced by any ref or the reflog), there is no way to retrieve the original copy—it is a permanent undo. Care must be taken when using this tool, as it’s one of the only Git commands that has the potential to lose your work.\nLike git checkout, git reset is a versatile command with many configurations. It can be used to remove committed snapshots, although it’s more often used to undo changes in the staging area and the working directory. In either case, it should only be used to undo local changes—you should never reset snapshots that have been shared with other developers.\nUsage:\ngit reset \u0026lt;file\u0026gt; Remove the specified file from the staging area, but leave the working directory unchanged. This unstages a file without overwriting any changes.\ngit reset Reset the staging area to match the most recent commit, but leave the working directory unchanged. This unstages all files without overwriting any changes, giving you the opportunity to re-build the staged snapshot from scratch.\ngit reset --hard Reset the staging area and the working directory to match the most recent commit. In addition to unstaging changes, the \u0026ndash;hard flag tells Git to overwrite all changes in the working directory, too. Put another way: this obliterates all uncommitted changes, so make sure you really want to throw away your local developments before using it.\ngit reset \u0026lt;commit\u0026gt; Move the current branch tip backward to , reset the staging area to match, but leave the working directory alone. All changes made since  will reside in the working directory, which lets you re-commit the project history using cleaner, more atomic snapshots.\ngit reset --hard \u0026lt;commit\u0026gt; Move the current branch tip backward to  and reset both the staging area and the working directory to match. This obliterates not only the uncommitted changes, but all commits after , as well.\nDiscussion\nAll of the above invocations are used to remove changes from a repository. Without the \u0026ndash;hard flag, git reset is a way to clean up a repository by unstaging changes or uncommitting a series of snapshots and re-building them from scratch. The \u0026ndash;hard flag comes in handy when an experiment has gone horribly wrong and you need a clean slate to work with.\nWhereas reverting is designed to safely undo a public commit, git reset is designed to undo local changes. Because of their distinct goals, the two commands are implemented differently: resetting completely removes a changeset, whereas reverting maintains the original changeset and uses a new commit to apply the undo.\nDon’t Reset Public History\nYou should never use git reset  when any snapshots after  have been pushed to a public repository. After publishing a commit, you have to assume that other developers are reliant upon it.\nRemoving a commit that other team members have continued developing poses serious problems for collaboration. When they try to sync up with your repository, it will look like a chunk of the project history abruptly disappeared. The sequence below demonstrates what happens when you try to reset a public commit. The origin/master branch is the central repository’s version of your local master branch.\nAs soon as you add new commits after the reset, Git will think that your local history has diverged from origin/master, and the merge commit required to synchronize your repositories is likely to confuse and frustrate your team.\nThe point is, make sure that you’re using git reset  on a local experiment that went wrong—not on published changes. If you need to fix a public commit, the git revert command was designed specifically for this purpose.\nExamples: unstaging a file The git reset command is frequently encountered while preparing the staged snapshot. The next example assumes you have two files called hello.py and main.py that you’ve already added to the repository.\n# Edit both hello.py and main.py # Stage everything in the current directory git add . # Realize that the changes in hello.py and main.py # should be committed in different snapshots # Unstage main.py git reset main.py # Commit only hello.py git commit -m \u0026quot;Make some changes to hello.py\u0026quot; # Commit main.py in a separate snapshot git add main.py git commit -m \u0026quot;Edit main.py\u0026quot; As you can see, git reset helps you keep your commits highly-focused by letting you unstage changes that aren’t related to the next commit.\nremoving local commits The next example shows a more advanced use case. It demonstrates what happens when you’ve been working on a new experiment for a while, but decide to completely throw it away after committing a few snapshots.\n# Create a new file called `foo.py` and add some code to it # Commit it to the project history git add foo.py git commit -m \u0026quot;Start developing a crazy feature\u0026quot; # Edit `foo.py` again and change some other tracked files, too # Commit another snapshot git commit -a -m \u0026quot;Continue my crazy feature\u0026quot; # Decide to scrap the feature and remove the associated commits git reset --hard HEAD~2 The git reset HEAD~2 command moves the current branch backward by two commits, effectively removing the two snapshots we just created from the project history. Remember that this kind of reset should only be used on unpublished commits. Never perform the above operation if you’ve already pushed your commits to a shared repository.\nUsage statement The content of this Rmarkdown tutorial is a transformation from this source and is licensed under a Creative Commons Attribution 2.5 Australia License.\n","href":"/tutorials/git_undo_commit/","title":"Undo a git commit"},{"content":"When working off line, two Git tasks cannot be performed: fetching/pulling updates from the server, and pushing changes to the server. All other commands still work.\nOne can commit changes, branch off, revert and reset changes, the same as when there exists an internet connection.\nExample workflow: start offline mode\nwhile(notBored): commit changes add files branch off new features end offline mode\nupdate master branch\ngit fetch origin push changes to the server\ngit push \u0026lt;branch-name\u0026gt; it is possible, that during your down-time, a pull request got accepted in that case, perform the following steps\ngit fetch origin git checkout \u0026lt;branch-name\u0026gt; git rebase master when necessary: solve merge conflicts, and rebase again.\nYour feature branch can now be pushed to the server, and a pull request can be made\n","href":"/tutorials/git_no_internet/","title":"Git without internet"},{"content":"BEFORE I START WORKING   STEP 1: Update the master branch on my PC to make sure it is aligned with the remote master\ngit fetch origin git checkout master git merge --ff-only origin/master   STEP 2: Choose your option:\n  OPTION 2A: I already have a branch I want to continue working on:\nSwitch to existing topic branch:\ngit checkout name_existing_branch git fetch origin git rebase origin/master   OPTION 2B: I\u0026rsquo;ll make a new branch to work with: Create a new topic branch from master(!):\ngit checkout master git checkout -b name_new_branch     WHILE EDITING  STEP 3.x: adapt in tex, code,\u0026hellip; (multiple times)   New files added\ngit add .   Adaptation\ngit commit -am \u0026quot;clear and understandable message about edits\u0026quot;     EDITS ON BRANCH READY   STEP 4: Pull request to add your changes to the current master. Choose your option:\n  OPTION 2A CHOSEN:\ngit push origin name_existing_branch   OPTION 2B CHOSEN:\ngit push origin name_new_branch     STEP 5: Code review!\nGo to your repo on Github.com and click the create pull request block. You and collaborators can make comments about the edits and review the code.\nIf everything is ok, click the Merge pull request, followed by confirm merge. (all online actions on GitHub). Delete the online branch, since obsolete.\nYou\u0026rsquo;re work is now tracked and added to master! Congratulations.\nIf the code can\u0026rsquo;t be merged automatically (provided by a message online), go to STEP 6.\n  PULL REQUEST CANNOT BE MERGED BY GITHUB   STEP 6: master has changed and there are conflicts: update your working branch with rebase\ngit checkout name_existing_branch git fetch origin git rebase origin/master # fix conflicts local git add file_with_conflict git rebase --continue git push -f origin name_existing_branch   ","href":"/tutorials/git_workflow/","title":"Git workflow using the command line"},{"content":"The list below contains all R related software which should be installed on the computers of useRs at INBO. Note that the installation process of most software requires administrator rights.\n","href":"/installation/administrator/","title":"Administrator installation notes"},{"content":"Here a some pages which describe the steps that the users stil needs to do after an installation or upgrade.\n","href":"/installation/user/","title":"User installation notes"},{"content":"Fix merge conflict with a pull request You have made some changes to a feature branch. Make a pull request on the server. The standard case of automatic merge is not possible. Push your latest changes from the feature branch to the server.\nLocally on your computer:\ngit fetch origin Rebase your feature branch with your master\ngit rebase origin/master Git will now state that there are merge conflicts. These will look like this:\n\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD:\u0026lt;some git nonsense\u0026gt; This part is from a version of this file ===== This is from another version of a file \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; blahdeblahdeblah:\u0026lt;some more git nonsense\u0026gt; The \u0026laquo;\u0026laquo;\u0026lt;, ===== and \u0026raquo;\u0026raquo;\u0026gt; markers show which lines were changed simultaneously. In order to remove the conflict, choose which line you want to keep (first or second), remove the other line and the markers, and finally commit the result.\nAdd your files that you fixed.\ngit add \u0026lt;fixed files\u0026gt; Continue with your rebase\ngit rebase --continue If more troubles occur, fix them, add them, and do a git rebase --continue\nForce push your branch to the server. (force because you changed the commit)\ngit push -f origin branchname On the server, you can now automatically close your Pull Request.\n","href":"/tutorials/git_conflict/","title":"Handle conflicts"},{"content":"De installatiebestanden voor de stabiele versies zijn beschikbaar via http://www.rstudio.com/products/rstudio/download/. De preview versie is beschikbaar via https://www.rstudio.com/products/rstudio/download/preview/\nWindows Nieuwe installatie en upgrade van RStudio RStudio upgraden doe je door de nieuwe versie te installeren over de oude.\n Zorg dat eerst R geïnstalleerd is. Voer het 64-bit installatiebestand uit. Klik Ja. Welkom bij de installatie: klik op volgende. Geef de doelmap en klik op volgende. Je mag de standaard gebruiken. Klik op installeren. Klik op voltooien. Kopieer het bestand rstudio-prefs.json naar de verborgen map AppData/Roaming/RStudio in de persoonlijke map van de gebruiker (C:/users/username).  RStudio mag niet met admininstratorrechten gestart worden. Anders worden een aantal R packages met administrator rechten geïnstalleerd waardoor de gebruiker ze niet meer kan updaten.\nTest de configuratie door RStudio te starten als een gewone gebruiker.\nAfwijkingen t.o.v. default installatie  Geen  ","href":"/installation/administrator/admin_install_rstudio/","title":"RStudio Desktop installation"},{"content":"Windows Installatie en upgrade De installatiebestanden zijn beschikbaar via cloud.r-project.org. Kies de 64-bit versie van de installer.\nBij een upgrade dient eerst de vorige versie te worden verwijderd.\n Klik Ja. Kies C:\\rtools40 (standaard) als installatiemap en klik Next. Laat alles aangevinkt en klik Next. Klik Install. Klik Finish. Wijzing PATH naar PATH=\u0026quot;${RTOOLS40_HOME}\\usr\\bin;${PATH}\u0026quot;. Start R en controleer of Sys.which(\u0026quot;make\u0026quot;) verwijst naar \u0026quot;C:\\\\rtools40\\\\usr\\\\bin\\\\make.exe\u0026quot;.  Afwijkingen t.o.v. default installatie  Wijzig de systeemvariabele PATH naar PATH=\u0026quot;${RTOOLS40_HOME}\\usr\\bin;${PATH}\u0026quot;  ","href":"/installation/administrator/admin_install_rtools/","title":"Rtools installation"},{"content":"","href":"/tags/windows/","title":"windows"},{"content":"Windows Pandoc wordt automatisch geïnstalleerd als je RStudio installeert.\nUbuntu  Kijk op https://github.com/jgm/pandoc/releases wat de laatste versie is. Pas het versienummer in onderstaande code aan en voer ze uit in een terminalvenster  wget https://github.com/jgm/pandoc/releases/download/1.19/pandoc-1.19-1-amd64.deb sudo dpkg -i pandoc-1.19-1-amd64.deb rm pandoc-1.19-1-amd64.deb ","href":"/installation/administrator/admin_install_pandoc/","title":"Pandoc installation"},{"content":"R CMD check has a large set of generic quality tests on a package. It is impossible to create generic tests that check the content of the package. E.g. does each function return sensible results. However, R CMD check does run a set unit tests. These are small pieces of code written by the package developer which test the output of a specific function under specific circumstances. We highly recommend the testthat framework for writing unit tests.\nCombining code coverage and Wercker A useful tool to visualise the coverage of the package by unit tests, is codecov. It can be added to the Wercker application by:\n login to http://www.codecov.io (via GitHub) and copy the token add it to the tab Environment on http://www.wercker.com/: Key = CODECOV_TOKEN, Value = (paste the token) and tick \u0026lsquo;Protected\u0026rsquo; to prevent it from being viewed. This makes it secure.  Note that it only makes sense when the wercker.yaml has a inbobmk/r-coverage or jimhester/r-coverage step.\n","href":"/tutorials/development_codecov/","title":"Code coverage"},{"content":"","href":"/authors/bartaelterman/","title":"bartaelterman"},{"content":"Intro R is known to have difficulties handling large data files. Here we will explore some tips that make working with such files in R less painfull.\ntl;dr  If you can comfortably work with the entire file in memory, but reading the file is rather slow, consider using the data.table package and read the file with its fread function. If your file does not comfortably fit in memory:  Use sqldf if you have to stick to csv files. Use a SQLite database and query it using either SQL queries or dplyr. Convert your csv file to a sqlite database in order to query    Downloading the example files While you can directly test this tutorial on your own large data files, we will use bird tracking data from the LifeWatch bird tracking network for the examples. We have made two versions of some tracking data available for download: a .csv file (text data) and a .db file (sqlite data). Both contain processed log files; for more information on the processing, see the BirdTrackingEtl package.\ncsv.name \u0026lt;- \u0026#34;2016-04-20-processed-logs-big-file-example.csv\u0026#34; db.name \u0026lt;- \u0026#34;2016-04-20-processed-logs-big-file-example.db\u0026#34; The evaluation of the next code chunk is ignored by default as the downloading and unzipping of the files results in more than 3 GB of data. If you do want to download the files yourself and test the other chunks, run the code and download the csv and sqlite examples. Make sure you have the R.utils package available (for unzipping the downloaded files). If not, use the command install.packages(\u0026quot;R.utils\u0026quot;) in your R console to download the package.\nlibrary(\u0026#34;R.utils\u0026#34;) # download the CSV file example csv.url \u0026lt;- paste(\u0026#34;https://s3-eu-west-1.amazonaws.com/lw-birdtracking-data/\u0026#34;, csv.name, \u0026#34;.gz\u0026#34;, sep = \u0026#34;\u0026#34;) if (!file.exists(csv.name)) { download.file(csv.url, destfile = paste0(csv.name, \u0026#34;.gz\u0026#34;)) gunzip(paste0(csv.name, \u0026#34;.gz\u0026#34;)) } # download the sqlite database example db.url \u0026lt;- paste(\u0026#34;https://s3-eu-west-1.amazonaws.com/lw-birdtracking-data/\u0026#34;, db.name, \u0026#34;.gz\u0026#34;, sep = \u0026#34;\u0026#34;) if (!file.exists(db.name)) { download.file(db.url, destfile = paste0(db.name, \u0026#34;.gz\u0026#34;)) gunzip(paste0(db.name, \u0026#34;.gz\u0026#34;)) } Loading a large dataset: use fread() or functions from readr instead of read.xxx(). library(\u0026#34;data.table\u0026#34;) library(\u0026#34;readr\u0026#34;) If you really need to read an entire csv in memory, by default, R users use the read.table method or variations thereof (such as read.csv). However, fread from the data.table package is a lot faster. Furthermore, the readr package also provides more optimized reading functions (read_csv, read_delim,…). Let’s measure the time to read in the data using these three different methods.\nread.table.timing \u0026lt;- system.time(read.table(csv.name, header = TRUE, sep = \u0026#34;,\u0026#34;)) readr.timing \u0026lt;- system.time(read_delim(csv.name, \u0026#34;,\u0026#34;, col_names = TRUE)) data.table.timing \u0026lt;- system.time(allData \u0026lt;- fread(csv.name, showProgress = FALSE)) data \u0026lt;- data.frame(method = c(\u0026#39;read.table\u0026#39;, \u0026#39;readr\u0026#39;, \u0026#39;fread\u0026#39;), timing = c(read.table.timing[3], readr.timing[3], data.table.timing[3])) data ## method timing ## 1 read.table 183.732 ## 2 readr 3.625 ## 3 fread 12.564  fread and read_delim are indeed much faster then the default read.table. However, the result of fread is a data.table and the result of read_delim is a tibble. Both are not a data.frame. The data.table package describes the data.table object as a more performant replacement for the data.frame. This means that selecting, filtering and aggregating data is much faster on a data.table compared to the standard data.frame but it requires you to use a slightly different syntax. A tibble is very similar to a data.frame, but provides more convenience when printing or subsetting the data table.\nYou can find the data.table package on CRAN. A good place to learn this package are the package vignettes. The introduction to data.table should be enough to get started. The readr package is also on CRAN. It belongs to a suite of R packages aiming to improve data manipulation in R, called tidyverse. More examples and explanation about readr is provided on the readr website.\nData files that don’t fit in memory If you are not able to read in the data file, because it does not fit in memory (or because R becomes too slow when you load the entire dataset), you will need to limit the amount of data that will actually be stored in memory. There are a couple of options which we will investigate:\n limit the number of lines you are trying to read for some exploratory analysis. Once you are happy with the analysis you want to run on the entire dataset, move to another machine. limit the number of columns you are reading to reduce the memory required to store the data. limit both the number of rows and the number of columns using sqldf. stream the data.  1. Limit the number of lines you read (fread) Limiting the number of lines you read is easy. Just use the nrows and/or skip option (available to both read.table and fread). skip can be used to skip a number of rows, but you can also pass a string to this parameter causing fread to only start reading lines from the first line matching that string. Let’s say we only want to start reading lines after we find a line matching the pattern 2015-06-12 15:14:39. We can do that like this:\nsprintf(\u0026#34;Number of lines in full data set: %s\u0026#34;, nrow(allData)) ## [1] \u0026quot;Number of lines in full data set: 3761058\u0026quot;  subSet \u0026lt;- fread(csv.name, skip = \u0026#34;2015-06-12 15:14:39\u0026#34;, showProgress = FALSE) sprintf(\u0026#34;Number of lines in data set with skipped lines: %s\u0026#34;, nrow(subSet)) ## [1] \u0026quot;Number of lines in data set with skipped lines: 9998\u0026quot;  Skipping rows this way is obviously not giving you the entire dataset, so this strategy is only useful for doing exploratory analysis on a subset of your data. Note that also read_delim provides a n_max argument to limit the number of lines to read. If you want to explore the whole dataset, limiting the number of columns you read can be a more useful strategy.\n2. Limit the number of columns you read (fread) If you only need 4 columns of the 21 columns present in the file, you can tell fread to only select those 4. This can have a major impact on the memory footprint of your data. The option you need for this is: select. With this, you can specify a number of columns to keep. The opposite - specifying the columns you want to drop - can be accomplished with the drop option.\nfourColumns = fread(csv.name, select = c(\u0026#34;device_info_serial\u0026#34;, \u0026#34;date_time\u0026#34;, \u0026#34;latitude\u0026#34;, \u0026#34;longitude\u0026#34;), showProgress = FALSE) sprintf(\u0026#34;Size of total data in memory: %s MB\u0026#34;, utils::object.size(allData)/1000000) ## [1] \u0026quot;Size of total data in memory: 1173.480728 MB\u0026quot;  sprintf(\u0026#34;Size of only four columns in memory: %s MB\u0026#34;, utils::object.size(fourColumns)/1000000) ## [1] \u0026quot;Size of only four columns in memory: 105.311936 MB\u0026quot;  The difference might not be as large as you would expect. R objects claim more memory than needed to store the data alone, as they keep pointers, and other object attributes. But still, the difference could save you.\n3. Limiting both the number of rows and the number of columns using sqldf The sqldf package allows you to run SQL-like queries on a file, resulting in only a selection of the file being read. It allows you to limit both the number of lines and the number of rows at the same time. In the background, this actually creates a sqlite database on the fly to execute the query. Consider using the package when starting from a csv file, but the actual strategy boils down to making a sqlite database file of your data. See this section below to learn how to interact with those and create a SQlite database from a CSV-file.\n4. Streaming data Short: streaming a file in R is a bad idea. If you are interested why, read the rest of this section.\nStreaming a file means reading it line by line and only keeping the lines you need or do stuff with the lines while you read through the file. It turns out that R is really not very efficient in streaming files. The main reason is the memory allocation process that has difficulties with a constantly growing object (which can be a dataframe containing only the selected lines).\nIn the next code block, we will read parts of our data file once using the freadfunction, and once line by line. You’ll see the performance issue with the streaming solution.\nlibrary(ggplot2) allowedDevices = c(753, 801, 852) minDate = strptime(\u0026#39;1/3/2014\u0026#39;,format = \u0026#39;%d/%m/%Y\u0026#39;) maxDate = strptime(\u0026#39;1/10/2014\u0026#39;,format = \u0026#39;%d/%m/%Y\u0026#39;) streamFile \u0026lt;- function(limit) { con \u0026lt;- file(csv.name, open = \u0026#34;r\u0026#34;) selectedRecords \u0026lt;- list() i \u0026lt;- 0 file.streaming.timing \u0026lt;- system.time( while (i \u0026lt; limit) { oneLine \u0026lt;- readLines(con, n = 1, warn = FALSE) vec = (strsplit(oneLine, \u0026#34;,\u0026#34;)) selectedRecords \u0026lt;- c(selectedRecords, vec) i \u0026lt;- i + 1 } ) close(con) return(file.streaming.timing[[3]]) } freadFile \u0026lt;- function(limit) { file.fread.timing = system.time( d \u0026lt;- fread(csv.name, showProgress = FALSE, nrows = limit) ) return(file.fread.timing[[3]]) } maxLines \u0026lt;- c(5000, 10000, 15000, 20000, 25000, 30000) streamingTimes \u0026lt;- sapply(maxLines, streamFile) freadTimes \u0026lt;- sapply(maxLines, freadFile) data \u0026lt;- data.frame(n = maxLines, streaming = streamingTimes, fread = freadTimes) pdata \u0026lt;- melt(data, id = c(\u0026#34;n\u0026#34;)) colnames(pdata) \u0026lt;- c(\u0026#34;n\u0026#34;, \u0026#34;algorithm\u0026#34;, \u0026#34;execTime\u0026#34;) qplot(n, execTime, data = pdata, color = algorithm, xlab = \u0026#34;number of lines read\u0026#34;, ylab = \u0026#34;execution time (s)\u0026#34;) The database file strategy Working with SQLite databases SQLite databases are single file databases meaning you can simply download them, store them in a folder or share them with colleagues. Similar to a csv. They are however more powerful than csv’s because of two important features:\n Support for SQL: this allows you to execute intelligent filters on your data, similar to the sqldf package or database environments you are familiar with. That way, you can reduce the amount of data that’s stored in memory by filtering out rows or columns. Indexes: SQLite databases contain indexes. An index is something like an ordered version of a column. When enabled on a column, you can search through the column much faster. We will demonstrate this below.  We have downloaded a second file 2016-04-20-processed-logs-big-file-example.db that contains the same data as the 2016-04-20-processed-logs-big-file-example.csv file, but as a sqlite database. Furthermore, the database file contains indexes which will dramatically drop the time needed to perform search queries. If you do not have a SQLite database containing your data, you can first convert your csv into a SQlite as described further in this tutorial.\nLet’s first connect to the database and list the available tables.\nlibrary(RSQLite) db \u0026lt;- dbConnect(SQLite(), dbname = db.name) # show the tables in this database dbListTables(db) ## [1] \u0026quot;SpatialIndex\u0026quot; \u0026quot;geom_cols_ref_sys\u0026quot; ## [3] \u0026quot;geometry_columns\u0026quot; \u0026quot;geometry_columns_auth\u0026quot; ## [5] \u0026quot;geometry_columns_field_infos\u0026quot; \u0026quot;geometry_columns_statistics\u0026quot; ## [7] \u0026quot;geometry_columns_time\u0026quot; \u0026quot;processed_logs\u0026quot; ## [9] \u0026quot;spatial_ref_sys\u0026quot; \u0026quot;spatialite_history\u0026quot; ## [11] \u0026quot;sql_statements_log\u0026quot; \u0026quot;sqlite_sequence\u0026quot; ## [13] \u0026quot;vector_layers\u0026quot; \u0026quot;vector_layers_auth\u0026quot; ## [15] \u0026quot;vector_layers_field_infos\u0026quot; \u0026quot;vector_layers_statistics\u0026quot; ## [17] \u0026quot;views_geometry_columns\u0026quot; \u0026quot;views_geometry_columns_auth\u0026quot; ## [19] \u0026quot;views_geometry_columns_field_infos\u0026quot; \u0026quot;views_geometry_columns_statistics\u0026quot; ## [21] \u0026quot;virts_geometry_columns\u0026quot; \u0026quot;virts_geometry_columns_auth\u0026quot; ## [23] \u0026quot;virts_geometry_columns_field_infos\u0026quot; \u0026quot;virts_geometry_columns_statistics\u0026quot;  Let’s try to select rows where the device id matches a given value (e.g. 860), and the date time is between two given timestamps. For our analysis, we only need date_time, latitude, longitude and altitude so we will only select those.\nsqlTiming \u0026lt;- system.time(data \u0026lt;- dbGetQuery(conn = db, \u0026#34;SELECT date_time, latitude, longitude, altitude FROM processed_logs WHERE device_info_serial = 860 AND date_time \u0026lt; \u0026#39;2014-07-01\u0026#39; AND date_time \u0026gt; \u0026#39;2014-03-01\u0026#39;\u0026#34; )) print(sqlTiming[3]) ## elapsed ## 40.405  This provides a convenient and fast way to request subsets of data from our large data file. We could do the same analysis for each of the serial numbers, each time only loading that subset of the data. As an example, consider the calculation of the average altitude over the specified period for each of the bird serial identifiers in the list serial_id_list. By using a for loop, the calculation is done for each of the birds separately and the amount of data loaded into memory at the same time is lower:\nserial_id_list \u0026lt;- c(853, 860, 783) print(\u0026#34;Average altitude between 2014-03-01 and 2014-07-01:\u0026#34;) ## [1] \u0026quot;Average altitude between 2014-03-01 and 2014-07-01:\u0026quot;  for (serialid in serial_id_list) { data \u0026lt;- dbGetQuery(conn = db, sprintf(\u0026#34;SELECT date_time, latitude, longitude, altitude FROM processed_logs WHERE device_info_serial = %d AND date_time \u0026lt; \u0026#39;2014-07-01\u0026#39; AND date_time \u0026gt; \u0026#39;2014-03-01\u0026#39;\u0026#34;, serialid)) print(sprintf(\u0026#34;serialID %d: %f\u0026#34;, serialid, mean(data$altitude))) } ## [1] \u0026quot;serialID 853: NA\u0026quot; ## [1] \u0026quot;serialID 860: 23.550518\u0026quot; ## [1] \u0026quot;serialID 783: 14.900030\u0026quot;  Remark that we use the sprintf function to dynamically replace the serial id in the sqlite query we will execute. For each loop, the %d is replaced by the value of the serial id of the respective loop. Read the manual of the sprintf function for more information and options.\nInteracting with SQLite databases using dplyr If you’re not comfortable with writing queries in SQL, R has a great alternative: dplyr. dplyr can connect to a SQLite database and you can perform the same operations on it that you would do on a dataframe. However, dplyr will translate your commands to SQL, allowing you to take advantage of the indexes in the SQLite database.\nlibrary(dplyr) my_db \u0026lt;- src_sqlite(db.name, create = FALSE) bird_tracking \u0026lt;- tbl(my_db, \u0026#34;processed_logs\u0026#34;) results \u0026lt;- bird_tracking %\u0026gt;% filter(device_info_serial == 860) %\u0026gt;% select(date_time, latitude, longitude, altitude) %\u0026gt;% filter(date_time \u0026lt; \u0026#34;2014-07-01\u0026#34;) %\u0026gt;% filter(date_time \u0026gt; \u0026#34;2014-03-01\u0026#34;) head(results) ## # Source: lazy query [?? x 4] ## # Database: sqlite 3.36.0 ## # [/Users/peter_desmet/Coding/Repositories/inbo/tutorials/content/tutorials/r_large_data_files_handling/2016-04-20-processed-logs-big-file-example.db] ## date_time latitude longitude altitude ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2014-03-10 12:43:37 44.0 -7.59 626 ## 2 2014-03-10 12:58:32 44.1 -7.67 405 ## 3 2014-03-10 13:13:52 44.1 -7.69 326 ## 4 2014-03-10 13:28:57 44.1 -7.71 250 ## 5 2014-03-10 13:43:54 44.1 -7.72 174 ## 6 2014-03-10 13:59:06 44.1 -7.73 23  Dplyr provides the ability to perform queries as above without the need to know SQL. If you want to learn more about how to use dplyr with a SQLite database, head over to this vignette.\nCreate a SQLite database from a CSV file In the case you have a CSV file available and you would like to query the data using SQL queries or with dplyr as shown in the previous sections, you can decide to convert the data to a SQlite database. The conversion will require some time, but once available, it provides the opportunity to query the data using SQL queries or with dplyr as shown in the previous sections. Moreover, you can easily add additional tables with related information to combine the data with.\nIf you already loaded the CSV file into memory, the creation of a SQLITE database is very straighforward and can be achieved in two steps:\ndb \u0026lt;- dbConnect(SQLite(), dbname = \u0026#34;example.sqlite\u0026#34;) dbWriteTable(db, \u0026#34;birdtracks\u0026#34;, allData) dbDisconnect(db) The first command creates a new database when the file example.sqlite does not exist already. The command dbWriteTable writes the table in the database. Hence, we can rerun the query from the previous section, but now on the newly created SQlite database, with the single created table birdtracks:\nmy_db \u0026lt;- src_sqlite(\u0026#34;example.sqlite\u0026#34;, create = FALSE) bird_tracking \u0026lt;- tbl(my_db, \u0026#34;birdtracks\u0026#34;) results \u0026lt;- bird_tracking %\u0026gt;% filter(device_info_serial == 860) %\u0026gt;% select(date_time, latitude, longitude, altitude) %\u0026gt;% filter(date_time \u0026lt; \u0026#34;2014-07-01\u0026#34;) %\u0026gt;% filter(date_time \u0026gt; \u0026#34;2014-03-01\u0026#34;) head(results) ## # Source: lazy query [?? x 4] ## # Database: sqlite 3.36.0 ## # [/Users/peter_desmet/Coding/Repositories/inbo/tutorials/content/tutorials/r_large_data_files_handling/example.sqlite] ## # … with 4 variables: date_time \u0026lt;dbl\u0026gt;, latitude \u0026lt;dbl\u0026gt;, longitude \u0026lt;dbl\u0026gt;, ## # altitude \u0026lt;int\u0026gt;  However, when working with really large CSV files, you do not want to load the entire file into memory first (this is the whole point of this tutorial). An alternative strategy is to load the data from the CSV file in chunks (small sections) and write them step by step to the SQlite database.\nThis can be implemented by reading the CSV file in small sections (let’s say 50000 lines each time) and move all sections to a given table in a sqlite database. As this is a recurrent task, we will provide the transformation in a custom written function, called csv_to_sqlite. The function is available within the inborutils package. Check the function documentation online or by typing ?csv_to_sqlite after installing and loading the inborutils package. As SQlite does not natively support date and datetime representations, the function converts those columns to an appropriate string representation before copying the dates to sqlite. To check for the date handling, the lubridate package is used.\nAs an example, let’s convert the processed bird logs csv file to a sqlite database, called example.sqlite as a table birdtracks. Using the default values for the preprocessing number of lines and the chunk size, the conversion is as follows:\nlibrary(inborutils) sqlite_file \u0026lt;- \u0026#34;example2.sqlite\u0026#34; table_name \u0026lt;- \u0026#34;birdtracks\u0026#34; inborutils::csv_to_sqlite(csv_file = csv.name, sqlite_file, table_name, pre_process_size = 1000, chunk_size = 50000, show_progress_bar = FALSE) Hence, this approach will work for large files as well and is an ideal first step when doing this kind of analysis. Once performed, the SQlite database is available to query, similar to the previous examples:\nmy_db \u0026lt;- src_sqlite(\u0026#34;example2.sqlite\u0026#34;, create = FALSE) ## Warning: `src_sqlite()` was deprecated in dplyr 1.0.0. ## Please use `tbl()` directly with a database connection ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.  bird_tracking \u0026lt;- tbl(my_db, \u0026#34;birdtracks\u0026#34;) results \u0026lt;- bird_tracking %\u0026gt;% filter(device_info_serial == 860) %\u0026gt;% select(date_time, latitude, longitude, altitude) %\u0026gt;% filter(date_time \u0026lt; \u0026#34;2014-07-01\u0026#34;) %\u0026gt;% filter(date_time \u0026gt; \u0026#34;2014-03-01\u0026#34;) head(results) ## # Source: lazy query [?? x 4] ## # Database: sqlite 3.36.0 ## # [/Users/peter_desmet/Coding/Repositories/inbo/tutorials/content/tutorials/r_large_data_files_handling/example2.sqlite] ## date_time latitude longitude altitude ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2014-03-10 12:43:37 44.0 -7.59 626 ## 2 2014-03-10 12:58:32 44.1 -7.67 405 ## 3 2014-03-10 13:13:52 44.1 -7.69 326 ## 4 2014-03-10 13:28:57 44.1 -7.71 250 ## 5 2014-03-10 13:43:54 44.1 -7.72 174 ## 6 2014-03-10 13:59:06 44.1 -7.73 23  Remark that the dates are properly handled, by making sure the date representation inside SQlite is the converted string version.\n","href":"/tutorials/r_large_data_files_handling/","title":"Reading large data files in R"},{"content":"Connection to INBO database Here we provide the approach using the package DBI, which is also used by RStudio.[1]. This package enables the link between R and the (remote) database. After installation of the needed packages (install.packages(c(\u0026quot;DBI\u0026quot;, \u0026quot;glue\u0026quot;, \u0026quot;tidyverse\u0026quot;))), the packages can be loaded:\nlibrary(DBI) library(glue) library(tidyverse) To create a database connection, different approaches are available as explained in the next section:\n Use the dedicated connect_inbo_dbase function in the inbodb package Setup the required parameters yourself Reuse existing registered connections on your computer  Inbodb function To support the connection to INBO databases, a dedicated function is available in the inbodb package, called connect_inbo_dbase. The function provides support to connect to INBO databases and uses the Connections pane in the Rstudio interface:\nTo use the connect_inbo_dbase function, make sure to install the inbodb package, following the installation instructions. After a successfull installation, load the library and create a database connection:\nlibrary(inbodb) my_connection \u0026lt;- connect_inbo_dbase(\u0026#34;D0021_00_userFlora\u0026#34;) Once the connection is successfully established, the database can be queried.\nRemark for Linux users: When working in Linux, this setup (using Trusted connection) requires an active kerberos session. More information about the setup and functionality is to be found in the tutorial on kerberos installation.\nSetting up the connection yourself In case you want to setup the connection yourself (e.g. errors using inbodb), you can do so by providing the necessary parameters:\nmy_connection \u0026lt;- DBI::dbConnect(odbc::odbc(), driver = \u0026#34;ODBC Driver 17 for SQL Server\u0026#34;, server = \u0026#34;\u0026lt;server hostname or IP\u0026gt;\u0026#34;, port = 1433, database = \u0026#34;D0021_00_userFlora\u0026#34;, # or your database of interest trusted_connection = \u0026#34;Yes\u0026#34;) The most important parameters are server and database, the others should normally be kept as such (except for maybe driver). For server, keep in mind that there is more than one server: database names starting with M, S or W use another server than those starting with D. The database name is the name of the database (if you can’t remember the name, connect with a database you do know, e.g. D0021_00_userFlora and you’ll see an overview of the existing databases on that server after connecting.) The driver version depends on your computer and can be looked up using odbc::odbcListDrivers().\nUse existing MSAccess connection name When you query data from a SQL database that is already accessible using MSAccess, such a database is also accessible from R. For Windows users, the most important element is to know the so-called DSN (i.e. a registered Data Source Name). Actually, it is just the name of the database as it is known by your computer (and MS Access). The easiest way to check the DSN is to check the registered ODBC connections in the administrator tools menu.\nFor Dutch-speaking Windows 7 users:\n\u0026gt; Kies in het Configuratiescherm van Windows de optie Systeembeheer \u0026gt; Gegevensbronnen (ODBC). De optie Systeembeheer verschijnt in de categorie Systeem en onderhoud.  You should see a list similar to the list underneath, with the names of the available DSN names enlisted:\nAn alternative way to check the DSN name of a database already working on with Access, is to check the DSN inside MS Access (in dutch, check menu item Koppelingsbeheer):\nFor example, the DSN name UserFlora or Cydonia-prd can be used to query these databases and extract data from it with similar queries to the one used in MSAccess. First of all, the connection with the database need to be established, by using the odbcConnect function, providing the DSN name as argument:\nFor Windows users:\nmy_connection \u0026lt;- odbcConnect(\u0026#34;UserFlora\u0026#34;) Once the connection is successfully established, the database can be queried.\nGet a complete table from the database The function dbReadTable can be used to load an entire table from a database. For example, to extract the tblTaxon table from the flora database:\nrel_taxa \u0026lt;- dbReadTable(my_connection, \u0026#34;relTaxonTaxonGroep\u0026#34;) head(rel_taxa) %\u0026gt;% knitr::kable()    ID TaxonGroepID TaxonID     1 4 1   2 4 2   3 4 3   4 4 4   5 4 5   6 4 6    The connection my_connection, made earlier, is used as the first argument. The table name is the second argument.\nRemark: If you have no idea about the size of the table you’re trying to load from the database, this could be rather tricky and cumbersome. Hence, it is probably better to only extract a portion of the table using a query.\nExecute a query to the database The function dbGetQuery provides more flexibilty as it can be used to try any SQL-query on the database. A complete introduction to the SQL language is out of scope here. We will focus on the application and the reusage of a query.\nmeting \u0026lt;- dbGetQuery(my_connection, paste(\u0026#34;SELECT TOP 10 * FROM dbo.tblMeting\u0026#34;, \u0026#34;WHERE COR_X IS NOT NULL\u0026#34;)) head(meting) %\u0026gt;% knitr::kable()    ID WaarnemingID TaxonID MetingStatusCode Cor_X Cor_Y CommentaarTaxon CommentaarHabitat CREATION_DATE CREATION_USER UPDATE_DATE UPDATE_USER     2 21748 3909 GDGA 109948 185379 NA NA NA NA NA NA   14 45523 3909 GDGA 127708 179454 NA NA NA NA NA NA   15 124394 3909 GDGA 109424 192152 NA NA NA NA NA NA   23 38561 3909 GDGA 128290 179297 NA NA NA NA NA NA   24 126500 3909 GDGA 98714 178373 NA NA NA NA NA NA   173 73725 3909 GDGA 102612 189891 NA NA NA NA NA NA    Create and use query templates When you regularly use similar queries, with some minimal alterations, you do not want to copy/paste each time the entire query. It is prone to errors and you’re script will become verbose. It is advisable to create query templates, that can be used within the dbGetQuery function.\nConsider the execution of the following query. We are interested in those records with valid X and Y coordinates for the measurement, based on a given dutch name:\nsubset_meting \u0026lt;- dbGetQuery(my_connection, \u0026#34;SELECT meet.COR_X , meet.Cor_Y , meet.MetingStatusCode , tax.NaamNederlands , tax.NaamWetenschappelijk , waar.IFBLHokID FROM tblMeting AS meet LEFT JOIN tblTaxon AS tax ON tax.ID = meet.TaxonID LEFT JOIN tblWaarneming AS waar ON waar.ID = meet.WaarnemingID WHERE meet.Cor_X IS NOT NULL AND meet.Cor_X != 0 AND tax.NaamNederlands LIKE \u0026#39;Wilde hyacint\u0026#39;\u0026#34;) head(subset_meting) %\u0026gt;% knitr::kable()    COR_X Cor_Y MetingStatusCode NaamNederlands NaamWetenschappelijk IFBLHokID     88720 208327 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 11195   24106 199925 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 9601   103111 190915 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 6990   118123 183942 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 5672   106107 182343 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 5217   105765 180785 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 4830    If we need this query regularly, but each time using a different tax.NaamNederlands (only the name changes), it is worthwhile to invest some time in the creation of a small custom function that uses this query as a template. Let’s create a function flora_records_on_dutch_name that takes a valid database connection and a given Dutch name and returns the relevant subset of the data for this query:\nflora_records_on_dutch_name \u0026lt;- function(dbase_connection, dutch_name) { dbGetQuery(dbase_connection, glue_sql( \u0026#34;SELECT meet.Cor_X , meet.COr_Y , meet.MetingStatusCode , tax.NaamNederlands , tax.NaamWetenschappelijk , waar.IFBLHokID FROM dbo.tblMeting meet LEFT JOIN dbo.tblTaxon AS tax ON tax.ID = meet.TaxonID LEFT JOIN dbo.tblWaarneming AS waar ON waar.ID = meet.WaarnemingID WHERE meet.Cor_X IS NOT NULL AND meet.Cor_X != 0 AND tax.NaamNederlands LIKE {dutch_name}\u0026#34;, dutch_name = dutch_name, .con = dbase_connection)) } Hence, instead of copy-pasting the whole query each time (which could be error-prone), we can reuse the function for different names:\nhyacint \u0026lt;- flora_records_on_dutch_name(my_connection, \u0026#34;Wilde hyacint\u0026#34;) head(hyacint) %\u0026gt;% knitr::kable()    Cor_X COr_Y MetingStatusCode NaamNederlands NaamWetenschappelijk IFBLHokID     88720 208327 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 11195   24106 199925 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 9601   103111 190915 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 6990   118123 183942 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 5672   106107 182343 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 5217   105765 180785 GDGA Wilde hyacint Hyacinthoides non-scripta (L.) Chouard ex Rothm. 4830    bosanemoon \u0026lt;- flora_records_on_dutch_name(my_connection, \u0026#34;Bosanemoon\u0026#34;) head(bosanemoon) %\u0026gt;% knitr::kable()    Cor_X COr_Y MetingStatusCode NaamNederlands NaamWetenschappelijk IFBLHokID     119247 204936 GDGA Bosanemoon Anemone nemorosa L. 10234   73658 199081 GDGA Bosanemoon Anemone nemorosa L. 8823   72752 199010 GDGA Bosanemoon Anemone nemorosa L. 8824   72921 198828 GDGA Bosanemoon Anemone nemorosa L. 8824   72874 198735 GDGA Bosanemoon Anemone nemorosa L. 8824   72887 198660 GDGA Bosanemoon Anemone nemorosa L. 8824    Remark: Do not forget to close your connection when finished.\ndbDisconnect(my_connection) The glue_sql function In order to accomplish the re-usage of a query for different input names (dutch_name), the glue_sql function is used from the glue package. The glue_sql function (and the more general glue function) provides the ability to combine text and variable values in a single charactor string (i.e. the query to execute). For each variable name required in the query (any part of your query you want to have interchangeable), a representation in the query is given by the variable name you use in R, put in between curly brackets. For example, if you have the dutch_name variable in R, you can use it inside the query as {dutch_name}:\ndutch_name \u0026lt;- \u0026#39;Jan\u0026#39; an_integer \u0026lt;- 3 a_float \u0026lt;- 2.8 glue(\u0026#39;This prints a combination of a name: {dutch_name}, an integer: {an_integer} and a float value: {a_float}\u0026#39;) ## This prints a combination of a name: Jan, an integer: 3 and a float value: 2.8  Whereas the glue function is a general function for strings, the glue_sql function is specifically created to setup queries to databases. More information is provided here and here.\n[1] Formerly, connections were made using the package RODBC\n","href":"/tutorials/r_database_access/","title":"Read data from INBO databases (SQL Server) with R"},{"content":"Windows  Download gitconfig. Bewaar het bestand als .gitconfig (bestandsnaam start met een punt) in C:/users/username. Open .gitconfig (rechtsklikken op het bestand en openen met bijvoorbeeld Notepad++) en vul jouw naam en e-mail aan. Sla het bestand op.  Ubuntu Doe dezelfde stappen als onder Windows. Alleen bewaar je het bestand als ~/.gitconfig.\n","href":"/installation/user/user_install_git/","title":"Git configuration"},{"content":"Windows De installatiebestanden zijn beschikbaar via http://git-scm.com/downloads\n Installeer eerst notepad++. Voer het installatiebestand uit en klik Ja. Welkom bij de installatie: klik op Next. Aanvaard de licentievoorwaarden door Next te klikken. Installeer git in de voorgestelde standaard directory. Gebruik de standaard componenten door Next te klikken. Klik Next om de menu map in te stellen. Kies Notepad++ als editor. Kies use Git from the command line en klik Next. Use OpenSSL library en klik Next. Kies Checkout Windows-style, commit Unix-style line endings en klik Next Kies use Windows' default console window en klik Next Kies Default en klik Next. Gebruik de standaard door Next te klikken. Gebruik de standaard door Install te klikken. Vink alles uit en klik op Next.  Afwijkingen t.o.v. default installatie  Notepad++ als editor use Windows' default console window  Ubuntu sudo apt-get update sudo apt-get install git ","href":"/installation/administrator/admin_install_git/","title":"Git installation"},{"content":"","href":"/search/","title":"Search"}]
