
@book{stodden_implementing_2014,
  address = {Boca Raton, FL},
  title = {Implementing reproducible research},
  isbn = {978-1-4665-6159-5 1-4665-6159-9},
  language = {English},
  publisher = {CRC Press},
  author = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D},
  year = {2014},
  keywords = {bmkFAS_3OntwerpAnalyse, bmkFAS_4OntwerpRapportering, bmkRAP_QAQC, bmkRAP_Writing, bmkSOF_Programming, bmkSOF_R, CAT_ICT_SoftwareAndProgramming, CAT_ResearchStrategyAndTechniques, ESSENTIAL_reading_Floris, Floris_ANNOTED, FlorisVanderhaeghe, THEORY},
  annote = {Computational reproducibility and (experiment) replicability; the three parts are TOOLS, PRACTICES AND GUIDELINES, PLATFORMS},
  file = {Stodden_etal_2014_Implementing_reproducible_research.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/S/Stodden_etal_2014_Implementing_reproducible_research.pdf:application/pdf;Stodden_et_al_2014_Implementing_reproducible_research.zip:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/S/Stodden_et_al_2014_Implementing_reproducible_research.zip:application/zip}
}

@article{wickham_tidy_2014,
  title = {Tidy {Data}},
  volume = {59},
  number = {10},
  journal = {Journal of Statistical Software},
  author = {Wickham, Hadley},
  year = {2014},
  keywords = {bmkSOF_Databanken, bmkSOF_R, CAT_ICT_Database, CAT_ICT_SoftwareAndProgramming, FlorisVanderhaeghe, THEORY},
  annote = {How to make tidy data},
  file = {Wickham_2014_JStatSoft.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/W/Wickham_2014_JStatSoft.pdf:application/pdf}
}

@book{gunther_learn_2014,
  title = {Learn {Version} {Control} with {Git}: {A} step-by-step course for the complete beginner},
  volume = {1},
  author = {Günther, Tobias},
  isbn = {978-1520786506},
  month = apr,
  year = {2014},
  url = {https://www.git-tower.com/learn/git/ebook/en/command-line/introduction},
  publisher = {fournova Software GmbH},
  keywords = {bmkADM_BibVanBMK, bmkSOF_Andere, bmkSOF_Programming, bmkSOF_R, CAT_ICT_SoftwareAndProgramming, ESSENTIAL_reading_Floris, Floris_EBOOK, THEORY}
}

@article{stodden_best_2014,
  title = {Best {Practices} for {Computational} {Science}: {Software} {Infrastructure} and {Environments} for {Reproducible} and {Extensible} {Research}},
  volume = {2},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are ©, ® or ™ of their respective owners. No challenge to any owner’s rights is intended or should be inferred.},
  issn = {2049-9647},
  shorttitle = {Best {Practices} for {Computational} {Science}},
  url = {http://openresearchsoftware.metajnl.com/articles/10.5334/jors.ay/},
  doi = {10.5334/jors.ay},
  abstract = {The goal of this article is to coalesce a discussion around best practices for scholarly research that utilizes computational methods, by providing a formalized set of best practice recommendations to guide computational scientists and other stakeholders wishing to disseminate reproducible research, facilitate innovation by enabling data and code re-use, and enable broader communication of the output of computational scientific research. Scholarly dissemination and communication standards are changing to reflect the increasingly computational nature of scholarly research, primarily to include the sharing of the data and code associated with published results. We also present these Best Practices as a living, evolving, and changing document at http://wiki.stodden.net/Best_Practices.},
  language = {en},
  number = {1},
  urldate = {2016-02-26},
  journal = {Journal of Open Research Software},
  author = {Stodden, Victoria and Miguez, Sheila},
  month = jul,
  year = {2014},
  keywords = {archiving, best practices, bmkSOF_Andere, bmkSOF_R, CAT_ICT_SoftwareAndProgramming, code sharing, computational science, data sharing, ESSENTIAL_reading_Floris, open science, reproducible research, scientific method, THEORY, wiki},
  annote = {A formalized set of best practice recommendations for reproducible research},
  file = {Stodden_Miguez_2014_JOpenResSoftw.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/S/Stodden_Miguez_2014_JOpenResSoftw.pdf:application/pdf}
}

@article{wilson_good_2016,
  title = {Good {Enough} {Practices} in {Scientific} {Computing}},
  url = {http://arxiv.org/abs/1609.00037},
  urldate = {2016-09-06},
  journal = {PLOS Submission},
  author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
  year = {2016},
  keywords = {bmkSOF_Andere, bmkSOF_Programming, bmkSOF_R, CAT_ICT_SoftwareAndProgramming, CAT_ResearchStrategyAndTechniques, ESSENTIAL_reading_Floris, THEORY},
  annote = {Computing tools and techniques for reproducible science},
  file = {Wilson_etal_2016_PLOS.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/W/Wilson_etal_2016_PLOS.pdf:application/pdf}
}

@article{wilson_good_2017,
  title = {Good enough practices in scientific computing},
  volume = {13},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510},
  doi = {10.1371/journal.pcbi.1005510},
  abstract = {Author summary Computers are now essential in all branches of science, but most researchers are never taught the equivalent of basic lab skills for research computing. As a result, data can get lost, analyses can take much longer than necessary, and researchers are limited in how effectively they can work with software and data. Computing workflows need to follow the same practices as lab projects and notebooks, with organized data, documented steps, and the project structured for reproducibility, but researchers new to computing often don't know where to start. This paper presents a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill. These practices, which encompass data management, programming, collaborating with colleagues, organizing projects, tracking work, and writing manuscripts, are drawn from a wide variety of published sources from our daily lives and from our work with volunteer organizations that have delivered workshops to over 11,000 people since 2010.},
  number = {6},
  urldate = {2017-08-24},
  journal = {PLOS Computational Biology},
  author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
  month = jun,
  year = {2017},
  keywords = {bmkANA_Andere, bmkFAS_3OntwerpAnalyse, bmkFAS_4OntwerpRapportering, bmkRAP_Writing, bmkSOF_Andere, bmkSOF_Databanken, bmkSOF_Programming, bmkSOF_R, CAT_ICT_Database, CAT_ICT_SoftwareAndProgramming, CAT_ModellingAndStatistics, CAT_Publishing, CAT_ResearchStrategyAndTechniques, Computer software, Control systems, Data management, Data processing, ESSENTIAL_reading_Floris, Programming languages, reproducibility, Software tools, Source code, THEORY},
  pages = {e1005510},
  annote = {Set of good computing practices that every researcher can adopt},
  file = {Wilson_etal_2017_PLOSComputBiol.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/W/Wilson_etal_2017_PLOSComputBiol.pdf:application/pdf}
}

@article{lowndes_our_2017,
  title = {Our path to better science in less time using open data science tools},
  volume = {1},
  copyright = {2017 Nature Publishing Group},
  issn = {2397-334X},
  url = {https://www.nature.com/articles/s41559-017-0160},
  doi = {10.1038/s41559-017-0160},
  abstract = {{\textless}p{\textgreater}Reproducibility starts with having a transparent and streamlined workflow. Here, the authors describe how they achieved this using open data tools for the collaborative Ocean Health Index project.{\textless}/p{\textgreater}},
  language = {en},
  number = {6},
  urldate = {2017-08-24},
  journal = {Nature Ecology \& Evolution},
  author = {Lowndes, Julia S. Stewart and Best, Benjamin D. and Scarborough, Courtney and Afflerbach, Jamie C. and Frazier, Melanie R. and O’Hara, Casey C. and Jiang, Ning and Halpern, Benjamin S.},
  month = may,
  year = {2017},
  keywords = {bmkANA_Andere, bmkFAS_3OntwerpAnalyse, bmkFAS_4OntwerpRapportering, bmkRAP_Writing, bmkSOF_Andere, bmkSOF_Databanken, bmkSOF_Programming, bmkSOF_R, CAT_ICT_Database, CAT_ICT_SoftwareAndProgramming, CAT_ModellingAndStatistics, CAT_Publishing, CAT_ResearchStrategyAndTechniques, ESSENTIAL_reading_Floris, THEORY},
  pages = {s41559--017--0160--017},
  annote = {Learning open data science tools},
  file = {Lowndes_etal_2017_NatureEcolEvol.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/L/Lowndes_etal_2017_NatureEcolEvol.pdf:application/pdf}
}

@article{baker_is_2016,
  title = {Is there a reproducibility crisis?},
  volume = {533},
  url = {http://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970},
  doi = {10.1038/533452a},
  abstract = {Survey sheds light on the ‘crisis’ rocking research.},
  number = {7604},
  urldate = {2017-09-25},
  journal = {Nature},
  author = {Baker, Monya},
  month = may,
  year = {2016},
  keywords = {bmkCTX_BeleidsgerichteMeetnetten, bmkCTX_Onderzoekscontext, bmkFAS_2OntwerpObservatie, bmkFAS_3OntwerpAnalyse, bmkSOF_Andere, CAT_ICT_Database, CAT_ICT_SoftwareAndProgramming, CAT_ResearchStrategyAndTechniques, ESSENTIAL_reading_Floris, THEORY},
  pages = {452--454},
  annote = {Results from a Nature survey on research reproducibility},
  file = {Baker_2016_Nature.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/B/Baker_2016_Nature.pdf:application/pdf}
}

@article{begley_institutions_2015,
  title = {Institutions must do their part for reproducibility},
  volume = {525},
  shorttitle = {Robust research},
  url = {http://www.nature.com/news/robust-research-institutions-must-do-their-part-for-reproducibility-1.18259},
  doi = {10.1038/525025a},
  abstract = {Tie funding to verified good institutional practice, and robust science will shoot up the agenda, say C. Glenn Begley, Alastair M.},
  number = {7567},
  urldate = {2017-09-25},
  journal = {Nature},
  author = {Begley, C. Glenn and Buchan, Alastair M. and Dirnagl, Ulrich},
  month = sep,
  year = {2015},
  keywords = {bmkCTX_BeleidsgerichteMeetnetten, bmkCTX_Onderzoekscontext, bmkFAS_2OntwerpObservatie, bmkFAS_3OntwerpAnalyse, bmkRAP_QAQC, bmkSOF_Andere, CAT_ICT_Database, CAT_ICT_SoftwareAndProgramming, CAT_ResearchStrategyAndTechniques, ESSENTIAL_reading_Floris, THEORY},
  pages = {25--27},
  annote = {Current irreproducibility and good institutional practice},
  file = {Begley_etal_2015_Nature.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/B/Begley_etal_2015_Nature.pdf:application/pdf}
}

@article{collaboration_estimating_2015,
  title = {Estimating the reproducibility of psychological science},
  volume = {349},
  copyright = {Copyright © 2015, American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  url = {http://science.sciencemag.org/content/349/6251/aac4716},
  doi = {10.1126/science.aac4716},
  abstract = {Empirically analyzing empirical evidence
One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.
Science, this issue 10.1126/science.aac4716
Structured Abstract
INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.
RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.
RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {\textless} .05). Thirty-six percent of replications had significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. {\textless}img class="fragment-image" src="https://d2ufo47lrtsv5s.cloudfront.net/content/sci/349/6251/aac4716/F1.medium.gif"/{\textgreater} Download high-res image Open in new tab Download Powerpoint Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.
Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.
A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.},
  language = {en},
  number = {6251},
  urldate = {2017-09-25},
  journal = {Science},
  author = {Collaboration, Open Science},
  month = aug,
  year = {2015},
  pmid = {26315443},
  keywords = {bmkFAS_2OntwerpObservatie, bmkFAS_3OntwerpAnalyse, CAT_ResearchStrategyAndTechniques, ESSENTIAL_reading_Floris, THEORY},
  pages = {aac4716},
  annote = {Results of reproducing 100 published experiments}
}

@incollection{ibanez_practicing_2014,
  address = {Boca Raton, FL},
  title = {Practicing open science},
  isbn = {978-1-4665-6159-5 1-4665-6159-9},
  language = {English},
  booktitle = {Implementing reproducible research},
  publisher = {CRC Press},
  author = {Ibanez, Luis and Schroeder, William J. and Hanwell, Marcus D.},
  editor = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D},
  year = {2014},
  keywords = {bmkFAS_3OntwerpAnalyse, bmkFAS_4OntwerpRapportering, bmkRAP_QAQC, bmkRAP_Writing, bmkSOF_Programming, bmkSOF_R, CAT_ICT_SoftwareAndProgramming, CAT_ResearchStrategyAndTechniques, ESSENTIAL_reading_Floris, Floris_ANNOTED, FlorisVanderhaeghe, THEORY},
  annote = {Reproducible science: vision, routine practices, collaboration, literate computing},
  file = {Ibanez_etal_2014_Implementing_reproducible_research.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/I/Ibanez_etal_2014_Implementing_reproducible_research.pdf:application/pdf}
}

@article{wilkinson_fair_2016,
  title = {The {FAIR} {Guiding} {Principles} for scientific data management and stewardship},
  volume = {3},
  url = {http://dx.doi.org/10.1038/sdata.2016.18},
  abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders—representing academia, industry, funding agencies, and scholarly publishers—have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
  journal = {Scientific Data},
  author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J.G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and ’t Hoen, Peter A.C and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
  month = mar,
  year = {2016},
  keywords = {bmkANA_Andere, bmkFAS_3OntwerpAnalyse, bmkFAS_4OntwerpRapportering, bmkRAP_Writing, bmkSOF_Andere, bmkSOF_Databanken, bmkSOF_Programming, bmkSOF_R, CAT_ICT_Database, CAT_ICT_SoftwareAndProgramming, CAT_ModellingAndStatistics, CAT_Publishing, CAT_ResearchStrategyAndTechniques, ESSENTIAL_reading_Floris, THEORY},
  pages = {160018},
  file = {Wilkinson_etal_2016_SciData.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/W/Wilkinson_etal_2016_SciData.pdf:application/pdf}
}

@article{smith_software_2016,
  title = {Software citation principles},
  volume = {2},
  issn = {2376-5992},
  url = {https://peerj.com/articles/cs-86},
  doi = {10.7717/peerj-cs.86},
  abstract = {Software is a critical part of modern research and yet there is little support across the scholarly ecosystem for its acknowledgement and citation. Inspired by the activities of the FORCE11 working group focused on data citation, this document summarizes the recommendations of the FORCE11 Software Citation Working Group and its activities between June 2015 and April 2016. Based on a review of existing community practices, the goal of the working group was to produce a consolidated set of citation principles that may encourage broad adoption of a consistent policy for software citation across disciplines and venues. Our work is presented here as a set of software citation principles, a discussion of the motivations for developing the principles, reviews of existing community practice, and a discussion of the requirements these principles would place upon different stakeholders. Working examples and possible technical solutions for how these principles can be implemented will be discussed in a separate paper.},
  language = {en},
  urldate = {2018-02-19},
  journal = {PeerJ Computer Science},
  author = {Smith, Arfon M. and Katz, Daniel S. and Niemeyer, Kyle E.},
  month = sep,
  year = {2016},
  keywords = {bmkFAS_4OntwerpRapportering, bmkRAP_Writing, bmkSOF_Andere, bmkSOF_Programming, bmkSOF_R, CAT_ICT_Database, CAT_ICT_SoftwareAndProgramming, CAT_Publishing, CAT_ResearchStrategyAndTechniques, ESSENTIAL_reading_Floris, THEORY},
  pages = {e86},
  annote = {Recommendations of the FORCE11 Software Citation Working Group},
  file = {Smith_etal_2016_PeerJCompSci.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/S/Smith_etal_2016_PeerJCompSci.pdf:application/pdf}
}

@article{culina_navigating_2018,
  title = {Navigating the unfolding open data landscape in ecology and evolution},
  volume = {2},
  copyright = {2018 The Author(s)},
  issn = {2397-334X},
  url = {https://www.nature.com/articles/s41559-017-0458-2},
  doi = {10.1038/s41559-017-0458-2},
  abstract = {Open data is increasing rapidly, but data sets may be scattered among many repositories. Here, the authors present an overview of the open data landscape in ecology and evolutionary biology, and highlight key points to consider when reusing data.},
  language = {en},
  number = {3},
  urldate = {2018-02-28},
  journal = {Nature Ecology \& Evolution},
  author = {Culina, Antica and Baglioni, Miriam and Crowther, Tom W. and Visser, Marcel E. and Woutersen-Windhouwer, Saskia and Manghi, Paolo},
  month = mar,
  year = {2018},
  keywords = {bmkCTX_Onderzoekscontext, bmkSOF_Andere, bmkSOF_Databanken, CAT_ICT_Database, CAT_ICT_SoftwareAndProgramming, ESSENTIAL_reading_Floris},
  pages = {420--426},
  annote = {Overview of online data infrastructures and considerations to be made},
  file = {Culina_etal_2018_NatureEcolEvol.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/C/Culina_etal_2018_NatureEcolEvol.pdf:application/pdf}
}

@article{hampton_tao_2015,
  title = {The {Tao} of open science for ecology},
  volume = {6},
  issn = {2150-8925},
  url = {http://onlinelibrary.wiley.com/doi/10.1890/ES14-00402.1/abstract},
  doi = {10.1890/ES14-00402.1},
  abstract = {The field of ecology is poised to take advantage of emerging technologies that facilitate the gathering, analyzing, and sharing of data, methods, and results. The concept of transparency at all stages of the research process, coupled with free and open access to data, code, and papers, constitutes “open science.” Despite the many benefits of an open approach to science, a number of barriers to entry exist that may prevent researchers from embracing openness in their own work. Here we describe several key shifts in mindset that underpin the transition to more open science. These shifts in mindset include thinking about data stewardship rather than data ownership, embracing transparency throughout the data life-cycle and project duration, and accepting critique in public. Though foreign and perhaps frightening at first, these changes in thinking stand to benefit the field of ecology by fostering collegiality and broadening access to data and findings. We present an overview of tools and best practices that can enable these shifts in mindset at each stage of the research process, including tools to support data management planning and reproducible analyses, strategies for soliciting constructive feedback throughout the research process, and methods of broadening access to final research products.},
  language = {en},
  number = {7},
  journal = {Ecosphere},
  author = {Hampton, Stephanie E. and Anderson, Sean S. and Bagby, Sarah C. and Gries, Corinna and Han, Xueying and Hart, Edmund M. and Jones, Matthew B. and Lenhardt, W. Christopher and MacDonald, Andrew and Michener, William K. and Mudge, Joe and Pourmokhtarian, Afshin and Schildhauer, Mark P. and Woo, Kara H. and Zimmerman, Naupaka},
  month = jul,
  year = {2015},
  keywords = {bmkFAS_2OntwerpObservatie, bmkFAS_3OntwerpAnalyse, bmkFAS_4OntwerpRapportering, bmkSOF_Programming, CAT_ICT_SoftwareAndProgramming, CAT_ResearchStrategyAndTechniques, Data management, Ecology, ESSENTIAL_reading_Floris, open access, open science, reproducible research, THEORY},
  pages = {1--13},
  annote = {Workflows, tools, obstacles and needed mindshifts for open science},
  file = {Hampton_etal_2015_Ecosphere.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/H/Hampton_etal_2015_Ecosphere.pdf:application/pdf}
}

@article{stevens_building_2018,
  title = {Building a local community of practice in scientific programming for {Life} {Scientists}},
  copyright = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  url = {https://www.biorxiv.org/content/early/2018/02/15/265421},
  doi = {10.1101/265421},
  abstract = {For most experimental biologists, handling the avalanche of data generated is similar to self-learn how to drive. Although that might be doable, it is preferable and safer to learn good practices. One way to achieve this is to build local communities of practice by bringing together scientists that perform code-intensive research to spread know-how and good practices. Here, we indicate important challenges and issues that stand in the way of establishing these local communities of practice. For a given researcher working for an academic institution, their capacity to conduct data-intensive research will be arbitrarily relying on the presence of well-trained bioinformaticians in their neighborhood. In this paper, we propose a model to build a local community of practice for scientific programmers. First, Software/Data Carpentry (SWC) programming workshops designed for researchers new to computational biology can be organized. However, while they provide an immediate solution for learning, more regular long-term assistance is also needed. Researchers need persisting, local support to continue learning and to solve programming issues that hamper their research progress. The solution we describe here is to implement a study group where researchers can meet-up and help each other in a "safe-learning atmosphere". Based on our experience, we describe two examples of building local communities of practice: one in the Netherlands at the Amsterdam Science Park and one in the United States at the University of Wisconsin-Madison. The current challenge is to make these local communities self-sustainable despite the high turnover of researchers at any institution and the lack of academic reward (e.g. publication). Here, we present some lessons learned from our experience. We believe that our local communities of practice will prove useful for other scientists that want to set up similar structures of researchers involved in scientific programming and data science.},
  language = {en},
  urldate = {2018-02-28},
  journal = {bioRxiv},
  author = {Stevens, Sarah L. R. and Kuzak, Mateusz and Martinez, Carlos and Moser, Aurelia and Bleeker, Petra M. and Galland, Marc},
  month = feb,
  year = {2018},
  keywords = {bmkSOF_Programming, CAT_ICT_SoftwareAndProgramming, CAT_ResearchStrategyAndTechniques, THEORY},
  pages = {265421},
  annote = {Local community of practice for scientific programming: why, how (with scheme), challenges},
  file = {Stevens_etal_2018_bioRxiv.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/S/Stevens_etal_2018_bioRxiv.pdf:application/pdf}
}

@article{hampton_skills_2017,
  title = {Skills and {Knowledge} for {Data}-{Intensive} {Environmental} {Research}},
  volume = {67},
  issn = {0006-3568},
  url = {https://academic.oup.com/bioscience/article/67/6/546/3784601},
  doi = {10.1093/biosci/bix025},
  abstract = {The scale and magnitude of complex and pressing environmental issues lend urgency to the need for integrative and reproducible analysis and synthesis, facilitated by data-intensive research approaches. However, the recent pace of technological change has been such that appropriate skills to accomplish data-intensive research are lacking among environmental scientists, who more than ever need greater access to training and mentorship in computational skills. Here, we provide a roadmap for raising data competencies of current and next-generation environmental researchers by describing the concepts and skills needed for effectively engaging with the heterogeneous, distributed, and rapidly growing volumes of available data. We articulate five key skills: (1) data management and processing, (2) analysis, (3) software skills for science, (4) visualization, and (5) communication methods for collaboration and dissemination. We provide an overview of the current suite of training initiatives available to environmental scientists and models for closing the skill-transfer gap.},
  language = {en},
  number = {6},
  urldate = {2018-02-28},
  journal = {BioScience},
  author = {Hampton, Stephanie E. and Jones, Matthew B. and Wasser, Leah A. and Schildhauer, Mark P. and Supp, Sarah R. and Brun, Julien and Hernandez, Rebecca R. and Boettiger, Carl and Collins, Scott L. and Gross, Louis J. and Fernández, Denny S. and Budden, Amber and White, Ethan P. and Teal, Tracy K. and Labou, Stephanie G. and Aukema, Juliann E.},
  month = jun,
  year = {2017},
  keywords = {bmkANA_Andere, bmkRAP_Writing, bmkSOF_Andere, bmkSOF_Databanken, bmkSOF_Programming, CAT_ICT_Database, CAT_ICT_SoftwareAndProgramming, CAT_ModellingAndStatistics, CAT_ResearchStrategyAndTechniques, THEORY},
  pages = {546--557},
  annote = {Training approaches and needed skills in data science},
  file = {Hampton_etal_2017_BioScience.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/H/Hampton_etal_2017_BioScience.pdf:application/pdf}
}

@article{donati_information_2017,
  title = {Information management: {Data} domination},
  volume = {548},
  copyright = {© 2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {0028-0836},
  shorttitle = {Information management},
  url = {https://www.nature.com/nature/journal/v548/n7669/full/nj7669-613a.html?foxtrotcallback=true},
  doi = {10.1038/nj7669-613a},
  abstract = {Software programming, algorithm development and other technological skills can give scientists an edge in their fields.},
  language = {en},
  number = {7669},
  urldate = {2018-02-28},
  journal = {Nature},
  author = {Donati, Gaia and Woolston, Chris},
  month = aug,
  year = {2017},
  keywords = {bmkANA_Andere, bmkSOF_Andere, bmkSOF_Databanken, bmkSOF_Programming, Business, Careers, CAT_ICT_Database, CAT_ICT_SoftwareAndProgramming, CAT_ModellingAndStatistics, CAT_ResearchStrategyAndTechniques, Information technology, THEORY},
  pages = {613--614},
  annote = {How data science is becoming a large discipline},
  file = {Donati_Woolston_2017_Nature.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/D/Donati_Woolston_2017_Nature.pdf:application/pdf}
}

@article{perkel_democratic_2016,
  title = {Democratic databases: science on {GitHub}},
  volume = {538},
  shorttitle = {Democratic databases},
  url = {http://www.nature.com/news/democratic-databases-science-on-github-1.20719},
  doi = {10.1038/538127a},
  abstract = {Scientists are turning to a software–development site to share data and code.},
  language = {en},
  number = {7623},
  urldate = {2018-02-28},
  journal = {Nature News},
  author = {Perkel, Jeffrey},
  month = oct,
  year = {2016},
  keywords = {bmkFAS_2OntwerpObservatie, bmkSOF_Andere, CAT_ICT_SoftwareAndProgramming, CAT_ResearchStrategyAndTechniques, THEORY},
  pages = {127},
  annote = {Overview of git, github and some data repository sites},
  file = {Perkel_2016_NatureNews.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/P/Perkel_2016_NatureNews.pdf:application/pdf}
}

@article{kaplan_teaching_2017,
  title = {Teaching stats for data science},
  volume = {5},
  url = {https://peerj.com/preprints/3205},
  doi = {10.7287/peerj.preprints.3205v1},
  abstract = {The familiar mathematical topics of introductory statistics --- means, proportions, t-tests, normal and t distributions, chi-squared, etc. --- are a product of the first half of the 20th century. Naturally, they reflect the statistical conditions of that era: scarce, e.g.

            \&lt; 10, data originating in benchtop or agricultural experiments; algorithms communicated via algebraic formulas. Today, applied statistics relates to a different environment: software is the means of algorithmic communication, observational and "unplanned" data are interpreted for causal relationships, and data are large both in

            and the number of variables. This change in situation calls for a thorough rethinking of the topics in and approach to statistics education. This paper presents a set of ten organizing blocks for intro stats that are better suited to today's environment.},
  journal = {PeerJ Preprints},
  author = {Kaplan, Daniel T},
  year = {2017},
  keywords = {bmkANA_Andere, CAT_ModellingAndStatistics, CAT_ResearchStrategyAndTechniques, ESSENTIAL_reading_Floris, THEORY},
  pages = {e3205v1},
  annote = {Ten organizing blocks for introductory statistics teaching, in the present data science context},
  file = {Kaplan_2017_PeerJPreprints.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/K/Kaplan_2017_PeerJPreprints.pdf:application/pdf}
}

@article{bryan_excuse_2017,
  title = {Excuse me, do you have a moment to talk about version control?},
  volume = {5},
  url = {https://peerj.com/preprints/3159},
  doi = {10.7287/peerj.preprints.3159v2},
  abstract = {Data analysis, statistical research, and teaching statistics have at least one thing in common: these activities all produce many files! There are data files, source code, figures, tables, prepared reports, and much more. Most of these files evolve over the course of a project and often need to be shared with others, for reading or edits, as a project unfolds. Without explicit and structured management, project organization can easily descend into chaos, taking time away from the primary work and reducing the quality of the final product. This unhappy result can be avoided by repurposing tools and workflows from the software development world, namely, distributed version control. This article describes the use of the version control system Git and and the hosting site GitHub for statistical and data scientific workflows. Special attention is given to projects that use the statistical language R and, optionally, R Markdown documents. Supplementary materials include an annotated set of links to step-by-step tutorials, real world examples, and other useful learning resources.},
  journal = {PeerJ Preprints},
  author = {Bryan, Jennifer},
  year = {2017},
  keywords = {bmkSOF_Programming, CAT_ICT_SoftwareAndProgramming, CAT_ResearchStrategyAndTechniques, THEORY},
  pages = {e3159v2},
  annote = {Rationale, workflows and tools regarding version control for project organization},
  file = {Bryan_2017_PeerJPreprints.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/B/Bryan_2017_PeerJPreprints.pdf:application/pdf}
}

@article{cetinkaya-rundel_infrastructure_2017,
  title = {Infrastructure and tools for teaching computing throughout the statistical curriculum},
  volume = {5},
  url = {https://peerj.com/preprints/3181},
  doi = {10.7287/peerj.preprints.3181v1},
  abstract = {Modern statistics is fundamentally a computational discipline, but too often this fact is not reflected in our statistics curricula. With the rise of big data and data science it has become increasingly clear that students both want, expect, and need explicit training in this area of the discipline. Additionally, recent curricular guidelines clearly state that working with data requires extensive computing skills and that statistics students should be fluent in accessing, manipulating, analyzing, and modeling with professional statistical analysis software. Much has been written in the statistics education literature about pedagogical tools and approaches to provide a practical computational foundation for students. This article discusses the computational infrastructure and toolkit choices to allow for these pedagogical innovations while minimizing frustration and improving adoption for both our students and instructors.},
  journal = {PeerJ Preprints},
  author = {Cetinkaya-Rundel, Mine and Rundel, Colin W},
  year = {2017},
  keywords = {bmkANA_Andere, bmkSOF_Programming, CAT_ICT_SoftwareAndProgramming, CAT_ModellingAndStatistics, THEORY},
  pages = {e3181v1},
  annote = {Computational infrastructure and toolkit choices to allow for the necessary pedagogical innovations in statistics education},
  file = {Cetinkaya-Rundel_Rundel_2017_PeerJPreprints.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/C/Cetinkaya-Rundel_Rundel_2017_PeerJPreprints.pdf:application/pdf}
}

@article{ellis_how_2017,
  title = {How to share data for collaboration},
  volume = {5},
  url = {https://peerj.com/preprints/3139},
  doi = {10.7287/peerj.preprints.3139v5},
  abstract = {Within the statistics community, a number of guiding principles for sharing data have emerged; however, these principles are not always made clear to collaborators generating the data. To bridge this divide, we have established a set of guidelines for sharing data. In these, we highlight the need to provide raw data to the statistician, the importance of consistent formatting, and the necessity of including all essential experimental information and pre-processing steps carried out to the statistician. With these guidelines we hope to avoid errors and delays in data analysis.},
  journal = {PeerJ Preprints},
  author = {Ellis, Shannon E and Leek, Jeffrey T},
  year = {2017},
  keywords = {bmkSOF_Databanken, CAT_ICT_Database, CAT_ICT_SoftwareAndProgramming, CAT_ResearchStrategyAndTechniques, ESSENTIAL_reading_Floris, THEORY},
  pages = {e3139v5},
  annote = {Guidelines for providing data to a scientist: provide raw data, format consistently, include metadata \& preprocessing steps},
  file = {Ellis_Leek_2017_PeerJPreprints.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/E/Ellis_Leek_2017_PeerJPreprints.pdf:application/pdf}
}

@article{mcnamara_wrangling_2017,
  title = {Wrangling categorical data in {R}},
  volume = {5},
  url = {https://peerj.com/preprints/3163},
  doi = {10.7287/peerj.preprints.3163v2},
  abstract = {Data wrangling is a critical foundation of data science, and wrangling of categorical data is an important component of this process. However, categorical data can introduce unique issues in data wrangling, particularly in real-world settings with collaborators and periodically-updated dynamic data. This paper discusses common problems arising from categorical variable transformations in R, demonstrates the use of factors, and suggests approaches to address data wrangling challenges. For each problem, we present at least two strategies for management, one in base R and the other from the ‘tidyverse.’ We consider several motivating examples, suggest defensive coding strategies, and outline principles for data wrangling to help ensure data quality and sound analysis.},
  journal = {PeerJ Preprints},
  author = {McNamara, Amelia and Horton, Nicholas J},
  year = {2017},
  keywords = {bmkSOF_R, CAT_ICT_SoftwareAndProgramming, ESSENTIAL_reading_Floris, THEORY},
  pages = {e3163v2},
  annote = {Working with factors in R: tidyverse vs base R},
  file = {McNamara_Horton_2017_PeerJPreprints.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/M/McNamara_Horton_2017_PeerJPreprints.pdf:application/pdf}
}

@article{ross_declutter_2017,
  title = {Declutter your {R} workflow with tidy tools},
  volume = {5},
  url = {https://peerj.com/preprints/3180},
  doi = {10.7287/peerj.preprints.3180v1},
  abstract = {The R language has withstood the test of time. Forty years after it was initially developed (in the form of the S language) R is being used by millions of programmers on workflows the inventors of the language could never have imagined. Although base R packages perform well in most settings, workflows can be made more efficient by developing packages with more consistent arguments, inputs and outputs and emphasizing constantly improving code over historical code consistency. The universe of R packages known as the tidyverse, including dplyr, tidyr and others, aim to improve workflows and make data analysis as smooth as possible by applying a set of core programming principles in package development.},
  journal = {PeerJ Preprints},
  author = {Ross, Zev and Wickham, Hadley and Robinson, David},
  year = {2017},
  keywords = {bmkSOF_R, CAT_ICT_SoftwareAndProgramming, ESSENTIAL_reading_Floris, THEORY},
  pages = {e3180v1},
  annote = {Philosophy of tidyverse},
  file = {Ross_etal_2017_PeerJPreprints.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/R/Ross_etal_2017_PeerJPreprints.pdf:application/pdf}
}

@book{cooper_guide_2017,
  address = {London},
  series = {{BES} {Guides} to {Better} {Science}},
  title = {A guide to reproducible code in ecology and evolution},
  publisher = {British Ecological Society},
  editor = {Cooper, Natalie and Hsing, Pen-Yuan},
  year = {2017},
  keywords = {bmkFAS_3OntwerpAnalyse, bmkFAS_4OntwerpRapportering, bmkFAS_5Implementatie, bmkRAP_QAQC, bmkRAP_Writing, bmkSOF_Programming, bmkSOF_R, CAT_ICT_SoftwareAndProgramming, CAT_Publishing, CAT_ResearchStrategyAndTechniques, ESSENTIAL_reading_Floris, REVIEW},
  annote = {File organisation, workflow documentation, code reproducibility and readability, writing reproducible reports, version control and code archiving},
  file = {Cooper_Hsing_2017_A_guide_to_reproducible_code_in_ecology_and_evolution.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/C/Cooper_Hsing_2017_A_guide_to_reproducible_code_in_ecology_and_evolution.pdf:application/pdf}
}

@book{british_ecological_society_guide_2014,
  address = {London},
  series = {{BES} {Guides} to {Better} {Science}},
  title = {A guide to data management in ecology and evolution},
  publisher = {British Ecological Society},
  editor = {{British Ecological Society}},
  year = {2014},
  keywords = {bmkFAS_2OntwerpObservatie, bmkFAS_5Implementatie, bmkRAP_QAQC, bmkSOF_Databanken, bmkSOF_Programming, bmkSOF_R, CAT_ICT_Database, CAT_ICT_SoftwareAndProgramming, CAT_Publishing, CAT_ResearchStrategyAndTechniques, ESSENTIAL_reading_Floris, REVIEW},
  annote = {Planning the data life cycle; creating, processing, documenting, preserving, sharing \& reusing data},
  file = {British_Ecological_Society_2014_A_guide_to_data_management_in_ecology_and_evolution.pdf:/media/floris/DATA/Private/WETPRIM/ZoteroPDFs/B/British_Ecological_Society_2014_A_guide_to_data_management_in_ecology_and_evolution.pdf:application/pdf}
}

@book{McElreath2015,
abstract = {Statistical Rethinking is an introduction to applied Bayesian data analysis, aimed at PhD students and researchers in the natural and social sciences. This audience has had some calculus and linear algebra, and one or two joyless undergraduate courses in statistics. I've been teaching applied statistics to this audience for about a decade now, and this book has evolved from that experience. The book teaches generalized linear multilevel modeling (GLMMs) from a Bayesian perspective, relying on a simple logical interpretation of Bayesian probability and maximum entropy. The book covers the basics of regression through multilevel models, as well as touching on measurement error, missing data, and Gaussian process models for spatial and network autocorrelation. This is not a traditional mathematical statistics book. Instead the approach is computational, using complete R code examples, aimed at developing skilled and skeptical scientists. Theory is explained through simulation exercises, using R code. And modeling examples are fully worked, with R code displayed within the main text. Mathematical depth is given in optional "overthinking" boxes throughout.},
address = {Boca Raton},
author = {McElreath, Richard},
file = {:C\:/Users/hans_vancalster/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McElreath - 2015 - Statistical rethinking a Bayesian course with examples in R and Stan.pdf:pdf},
isbn = {9781482253443},
keywords = {bmkADM_BibVanBMK,bmkADM_TopOfStats},
mendeley-tags = {bmkADM_BibVanBMK,bmkADM_TopOfStats},
pages = {469},
publisher = {Chapman and Hall/CRC},
title = {{Statistical rethinking : a Bayesian course with examples in R and Stan}},
year = {2015}
}

@article{Kass2016,
author = {Kass, Robert E. and Caffo, Brian S. and Davidian, Marie and Meng, Xiao-Li and Yu, Bin and Reid, Nancy},
doi = {10.1371/journal.pcbi.1004961},
editor = {Lewitter, Fran},
file = {:C\:/Users/hans_vancalster/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kass et al. - 2016 - Ten Simple Rules for Effective Statistical Practice.pdf:pdf},
issn = {1553-7358},
journal = {PLOS Computational Biology},
keywords = {bmkADM_TopOfStats},
mendeley-tags = {bmkADM_TopOfStats},
month = {jun},
number = {6},
pages = {e1004961},
title = {{Ten Simple Rules for Effective Statistical Practice}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1004961},
annote = {The authors propose a set of 10 simple rules for effective statistical practice},
volume = {12},
year = {2016}
}

@book{Quinn2002,
abstract = {An essential textbook for any student or researcher in biology needing to design experiments, sample programs or analyse the resulting data. The text begins with a revision of estimation and hypothesis testing methods, covering both classical and Bayesian philosophies, before advancing to the analysis of linear and generalized linear models. Topics covered include linear and logistic regression, simple and complex ANOVA models (for factorial, nested, block, split-plot and repeated measures and covariance designs), and log-linear models. Multivariate techniques, including classification and ordination, are then introduced. Special emphasis is placed on checking assumptions, exploratory data analysis and presentation of results. The main analyses are illustrated with many examples from published papers and there is an extensive reference list to both the statistical and biological literature. The book is supported by a website that provides all data sets, questions for each chapter and links to software.},
author = {Quinn, G.P. and Keough, M.J.},
publisher = {Cambridge University Press},
keywords = {bmkADM_TopOfStats},
mendeley-tags = {bmkADM_TopOfStats},
pages = {537},
title = {{Experimental design and data analysis for biologists.}},
url = {http://www.cambridge.org},
year = {2002}
}

@book{James2013,
abstract = {An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform. Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.},
author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
file = {:C\:/Users/hans_vancalster/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/James et al. - 2013 - An Introduction to Statistical Learning with Applications in R.pdf:pdf},
isbn = {978-1-4614-7138-7},
keywords = {bmkADM_BibVanBMK,bmkADM_TopOfStats,bmkANA_Multivariate},
mendeley-tags = {bmkADM_BibVanBMK,bmkADM_TopOfStats,bmkANA_Multivariate},
pages = {426},
publisher = {Springer},
title = {{An Introduction to Statistical Learning with Applications in R}},
year = {2013}
}

@book{VanEmden2008,
abstract = {The typical biology student is “hardwired” to be wary of any tasks involving the application of mathematics and statistical analyses, but the plain fact is much of biology requires interpretation of experimental data through the use of statistical methods. This unique textbook aims to demystify statistical formulae for the average biology student. Written in a lively and engaging style, Statistics for Terrified Biologists draws on the author's 30 years of lecturing experience. One of the foremost entomologists of his generation, van Emden has an extensive track record for successfully teaching statistical methods to even the most guarded of biology students. For the first time basic methods are presented using straightforward, jargon-free language. Students are taught to use simple formulae accurately to interpret what is being measured with each test and statistic, while at the same time learning to recognize overall patterns and guiding principles. Complemented by simple illustrations and useful case studies, this is an ideal statistics resource tool for undergraduate biology and environmental science students who lack confidence in their mathematical abilities.},
author = {van Emden, Helmut},
isbn = {978-1-4051-4956-3},
keywords = {bmkADM_BibVanBMK,bmkADM_TopOfStats,bmkBooks},
mendeley-tags = {bmkADM_BibVanBMK,bmkADM_TopOfStats},
pages = {343},
publisher = {Blackwell Publishing},
title = {{Statistics for Terrified Biologists}},
year = {2008}
}

@book{Agresti2002a,
abstract = {The use of statistical methods for categorical data has increased dramatically, particularly for applications in the biomedical and social sciences. Responding to new developments in the field as well as to the needs of a new generation of professionals and students, this new edition of the classic Categorical Data Analysis offers a comprehensive introduction to the most important methods for categorical data analysis. Designed for statisticians and biostatisticians as well as scientists and graduate students practicing statistics, Categorical Data Analysis, Second Edition summarizes the latest methods for univariate and correlated multivariate categorical responses. Readers will find a unified generalized linear models approach that connects logistic regression and Poisson and negative binomial regression for discrete data with normal regression for continuous data.},
publisher = {John Wiley & Sons, Inc.},
author = {Agresti, A},
keywords = {bmkADM_BibVanBMK,bmkADM_TopOfStats},
mendeley-tags = {bmkADM_BibVanBMK,bmkADM_TopOfStats},
pages = {----},
title = {{Categorical Data Analysis (Second Edition)}},
year = {2002}
}

@book{VanBelle2008,
abstract = {This book contains chapters titled:

    * Begin with a Basic Formula for Sample Size–Lehr's Equation
    * Calculating Sample Size Using the Coefficient of Variation
    * Ignore the Finite Population Correction in Calculating Sample Size for a Survey
    * The Range of the Observations Provides Bounds for the Standard Deviation * Do not Formulate a Study Solely in Terms of Effect Size
    * Overlapping Confidence Intervals do not Imply Nonsignificance
    * Sample Size Calculation for the Poisson Distribution
    * Sample Size Calculation for Poisson Distribution with Background Rate
    * Sample Size Calculation for the Binomial Distribution
    * When Unequal Sample Sizes Matter; When They Don't * Determining Sample Size when there are Different Costs Associated with the Two Samples
    * Use the Rule of Threes for 95% Upper Bounds when there Have Been No Events
    * Sample Size Calculations Should be Based on the Way the Data will be Analyzed},
author = {{van Belle}, Gerald},
publisher = {John Wiley & Sons, Inc},
doi = {10.1002/9780470377963},
isbn = {9780470377963},
issn = {0040-1706},
keywords = {bmkADM_BibVanBMK,bmkADM_TopOfStats},
mendeley-tags = {bmkADM_BibVanBMK,bmkADM_TopOfStats},
pages = {1--272},
pmid = {1105427},
title = {{Statistical Rules of Thumb: Second Edition}},
year = {2008}
}

@misc{Grolemund2016,
abstract = {This is the website for "R for Data Science". This book will teach you how to do data science with R: You'll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you'll learn how to clean data and draw plots—and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You'll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You'll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.},
author = {Grolemund, Garrett and Wickham, Hadley},
publisher = {O'Reilly},
isbn = {978-1491910399},
keywords = {bmkADM_BibVanBMK,bmkADM_TopOfStats},
mendeley-tags = {bmkADM_BibVanBMK,bmkADM_TopOfStats},
title = {{R for Data Science}},
url = {http://r4ds.had.co.nz/},
year = {2016}
}

@book{Baddeley2015,
abstract = {Spatial Point Patterns: Methodology and Applications with R shows scientific researchers and applied statisticians from a wide range of fields how to analyze their spatial point pattern data. Making the techniques accessible to non-mathematicians, the authors draw on their 25 years of software development experiences, methodological research, and broad scientific collaborations to deliver a book that clearly and succinctly explains concepts and addresses real scientific questions. Practical Advice on Data Analysis and Guidance on the Validity and Applicability of Methods The first part of the book gives an introduction to R software, advice about collecting data, information about handling and manipulating data, and an accessible introduction to the basic concepts of point processes. The second part presents tools for exploratory data analysis, including non-parametric estimation of intensity, correlation, and spacing properties. The third part discusses model-fitting and statistical inference for point patterns. The final part describes point patterns with additional "structure," such as complicated marks, space-time observations, three- and higher-dimensional spaces, replicated observations, and point patterns constrained to a network of lines. Easily Analyze Your Own Data Throughout the book, the authors use their spatstat package, which is free, open-source code written in the R language. This package provides a wide range of capabilities for spatial point pattern data, from basic data handling to advanced analytic tools. The book focuses on practical needs from the user's perspective, offering answers to the most frequently asked questions in each chapter.},
address = {Boca Raton},
author = {Baddeley, Adrian and Rubak, Ege and Turner, Rolf},
isbn = {9781482210200},
keywords = {bmkADM_BibVanBMK,bmkADM_TopOfStats},
mendeley-tags = {bmkADM_BibVanBMK,bmkADM_TopOfStats},
publisher = {Chapman and Hall/CRC},
title = {{Spatial Point Patterns: Methodology and Applications with R}},
year = {2015}
}

@book{Hobbs2015,
abstract = {Bayesian modeling has become an indispensable tool for ecological research because it is uniquely suited to deal with complexity in a statistically coherent way. This textbook provides a comprehensive and accessible introduction to the latest Bayesian methods—in language ecologists can understand. Unlike other books on the subject, this one emphasizes the principles behind the computations, giving ecologists a big-picture understanding of how to implement this powerful statistical approach. Bayesian Models is an essential primer for non-statisticians. It begins with a definition of probability and develops a step-by-step sequence of connected ideas, including basic distribution theory, network diagrams, hierarchical models, Markov chain Monte Carlo, and inference from single and multiple models. This unique book places less emphasis on computer coding, favoring instead a concise presentation of the mathematical statistics needed to understand how and why Bayesian analysis works. It also explains how to write out properly formulated hierarchical Bayesian models and use them in computing, research papers, and proposals. This primer enables ecologists to understand the statistical principles behind Bayesian modeling and apply them to research, teaching, policy, and management.

    - Presents the mathematical and statistical foundations of Bayesian modeling in language accessible to non-statisticians
    - Covers basic distribution theory, network diagrams, hierarchical models, Markov chain Monte Carlo, and more - Deemphasizes computer coding in favor of basic principles
    - Explains how to write out properly factored statistical expressions representing Bayesian models},
author = {Hobbs, N T and Hooten, M B},
isbn = {9781400866557},
keywords = {bmkADM_BibVanBMK,bmkADM_TopOfStats},
mendeley-tags = {bmkADM_BibVanBMK,bmkADM_TopOfStats},
publisher = {Princeton University Press},
title = {{Bayesian Models: A Statistical Primer for Ecologists}},
year = {2015}
}

@book{Zuur2017,
abstract = {In Volume I we explain how to apply linear regression models, generalised linear models (GLM), and generalised linear mixed-effects models (GLMM) to spatial, temporal, and spatial-temporal data. The models that will be employed use the Gaussian and gamma distributions for continuous data, the Poisson and negative binomial distributions for count data, the Bernoulli distribution for absence–presence data, and the binomial distribution for proportional data.In Volume II we apply zero-inflated models and generalised additive (mixed-effects) models to spatial and spatial-temporal data. We also discuss models with more exotic distributions like the generalised Poisson distribution to deal with underdispersion and the beta distribution to analyse proportional data.},
author = {Zuur, Alain F and Ieno, Elena N and Anatoly and A and Saveliev},
publisher = {Highland Statistics Ltd.},
keywords = {bmkADM_BibVanBMK,bmkADM_TopOfStats},
mendeley-tags = {bmkADM_BibVanBMK,bmkADM_TopOfStats},
title = {{Beginner's guide to spatial, temporal, and spatial-temporal ecological data analysis with R-INLA.}},
url = {http://www.highstat.com/Books/BGS/SpatialTemp/Zuuretal2017_TOCOnline.pdf},
year = {2017}
}

@book{Zuur2007a,
abstract = {This book provides a practical introduction to analysing ecological data using real data sets collected as part of postgraduate ecological studies or research projects. The first part of the book gives a largely non-mathematical introduction to data exploration, univariate methods (including GAM and mixed modelling techniques), multivariate analysis, time series analysis (e.g. common trends) and spatial statistics. The second part provides 17 case studies, mainly written together with biologists who attended courses given by the first authors. The case studies include topics ranging from terrestrial ecology to marine biology. The case studies can be used as a template for your own data analysis; just try to find a case study that matches your own ecological questions and data structure, and use this as starting point for you own analysis. Data from all case studies are available from www.highstat.com. Guidance on software is provided in Chapter 2.},
author = {Zuur, A F and Ieno, E N and Smith, G M},
keywords = {bmkADM_BibVanBMK,bmkADM_TopOfStats},
mendeley-tags = {bmkADM_BibVanBMK,bmkADM_TopOfStats},
publisher = {Springer Verlag},
title = {{Analysing ecological data}},
year = {2007}
}

@article{Zuur2010,
abstract = {

    1. While teaching statistics to ecologists, the lead authors of this paper have noticed common statistical problems. If a randomsample of theirwork (including scientific papers) produced before doing these courses were selected, half would probably contain violations of the underlying assumptions of the statistical techniquesemployed.
    2. Some violations have little impact on the results or ecological conclusions; yet others increase type I or type II errors, potentially resulting in wrong ecological conclusions. Most of these violations can be avoided by applying better data exploration. These problems are especially trouble- somein applied ecology, wheremanagement and policy decisions are often at stake.
    3. Here, we provide a protocol for data exploration; discuss current tools to detect outliers, heterogeneity of variance, collinearity, dependence of observations, problems with interactions, double zeros in multivariate analysis, zero inflation in generalized linear modelling, and the correct type of relationships between dependent and independent variables; and provide advice on how to address these problems when they arise. We also address misconceptions about normality, and provide advice on data transformations.
    4. Data exploration avoids type I and type II errors, among other problems, thereby reducing the chance ofmaking wrong ecological conclusions and poor recommendations. It is therefore essential for good quality management and policy based on statistical analyses. Key-words:},
author = {Zuur, A F and Ieno, E N and Elphick, C S},
file = {:C\:/Users/hans_vancalster/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zuur, Ieno, Elphick - 2010 - A protocol for data exploration to avoid common statistical problems.pdf:pdf},
journal = {Methods in Ecology and Evolution},
keywords = {bmkADM_TopOfStats,bmkANA_EDA,bmkANA_Multivariate},
mendeley-tags = {bmkADM_TopOfStats,bmkANA_EDA,bmkANA_Multivariate},
number = {9999},
pages = {3--14},
publisher = {Blackwell Publishing Ltd},
title = {{A protocol for data exploration to avoid common statistical problems}},
volume = {1},
year = {2010}
}

@article{Kelleher2011,
abstract = {Our ability to visualize scientific data has evolved significantly over the last 40 years. However, this advancement does not necessarily alleviate many common pitfalls in visualization for scientific journals, which can inhibit the ability of readers to effectively understand the information presented. To address this issue within the context of visualizing environmental data, we list ten guidelines for effective data visualization in scientific publications. These guidelines support the primary objective of data visualization, i.e. to effectively convey information. We believe that this small set of guidelines based on a review of key visualization literature can help researchers improve the communication of their results using effective visualization. Enhancement of environmental data visualization will further improve research presentation and communication within and across disciplines.},
author = {Kelleher, Christa and Wagener, Thorsten},
doi = {10.1016/J.ENVSOFT.2010.12.006},
issn = {1364-8152},
journal = {Environmental Modelling {\&} Software},
keywords = {bmkADM_TopOfStats},
mendeley-tags = {bmkADM_TopOfStats},
month = {jun},
number = {6},
pages = {822--827},
publisher = {Elsevier},
title = {{Ten guidelines for effective data visualization in scientific publications}},
url = {https://www.sciencedirect.com/science/article/pii/S1364815210003270},
volume = {26},
year = {2011}
}

@book{Lohr2010,
abstract = {Sharon L. Lohr's SAMPLING: DESIGN AND ANALYSIS, 2ND EDITION, provides a modern introduction to the field of survey sampling intended for a wide audience of statistics students. Practical and authoritative, the book is listed as a standard reference for training on real-world survey problems by a number of prominent surveying organizations. Lohr concentrates on the statistical aspects of taking and analyzing a sample, incorporating a multitude of applications from a variety of disciplines. The text gives guidance on how to tell when a sample is valid or not, and how to design and analyze many different forms of sample surveys. Recent research on theoretical and applied aspects of sampling is included, as well as optional technology instructions for using statistical software with survey data.},
author = {Lohr, Sharon L},
edition = {Second Edi},
isbn = {0-495-11084-1},
keywords = {bmkADM_BibVanBMK,bmkADM_TopOfStats},
mendeley-tags = {bmkADM_BibVanBMK,bmkADM_TopOfStats},
pages = {596},
publisher = {Brooks/Cole},
title = {{Sampling: Design and Analysis}},
year = {2010}
}

@book{Zuur2009c,
abstract = {Building on the successful Analysing Ecological Data [@Zuur2007a], the authors now provide an expanded introduction to using regression and its extensions in analysing ecological data. As with the earlier book, real data sets from postgraduate ecological studies or research projects are used throughout. The first part of the book is a largely non-mathematical introduction to linear mixed effects modelling, GLM and GAM, zero inflated models, GEE, GLMM and GAMM. The second part provides ten case studies that range from koalas to deep sea research. These chapters provide an invaluable insight into analysing complex ecological datasets, including comparisons of different approaches to the same problem. By matching ecological questions and data structure to a case study, these chapters provide an excellent starting point to analysing your own data. Data and R code from all chapters are available from www.highstat.com.},
author = {Zuur, A F and Ieno, E N and Walker, N J and Saveliev, A A and Smith, G M},
keywords = {bmkADM_BibVanBMK,bmkADM_TopOfStats,bmkANA_MixedModels},
mendeley-tags = {bmkADM_BibVanBMK,bmkADM_TopOfStats,bmkANA_MixedModels},
publisher = {Springer},
title = {{Mixed effects models and extensions in ecology with R}},
year = {2009}
}

@article{Zuur2016,
abstract = {

    1. Scientific investigation is of value only insofar as relevant results are obtained and communicated, a task that requires organizing, evaluating, analysing and unambiguously communicating the significance of data. In this context, working with ecological data, reflecting the complexities and interactions of the natural world, can be a challenge. Recent innovations for statistical analysis ofmultifaceted interrelated datamake obtaining more accu- rate andmeaningful results possible, but key decisions of the analyses to use, and which components to present in a scientific paper or report, may be overwhelming.
    2. We offer a 10-step protocol to streamline analysis of data thatwill enhance understanding of the data, the statistical models and the results, and optimize communication with the reader with respect to both the procedure and the outcomes. The protocol takes the investigator from study design and organization of data (formulating relevant questions, visualizing data collection, data exploration, identifying dependency), through conducting analysis (presenting, fitting and validating the model) and presenting output (numerically and visually), to extending themodel via simulation. Each step includes procedures to clarify aspects of the data that affect statistical analysis, as well as guidelines for written presentation. Steps are illustrated with examples using data from the literature.
    3. Following this protocol will reduce the organization, analysis and presentation ofwhatmay be an overwhelming information avalanche into sequential and, more to the point, manageable, steps. It provides guidelines for selecting optimal statistical tools to assess data relevance and significance, for choosing aspects of the analysis to include in a published report and for clearly communicating information.},
author = {Zuur, Alain F. and Ieno, Elena N.},
doi = {10.1111/2041-210X.12577},
editor = {Freckleton, Robert},
file = {:C\:/Users/hans_vancalster/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zuur, Ieno - 2016 - A protocol for conducting and presenting results of regression-type analyses(2).pdf:pdf},
issn = {2041210X},
journal = {Methods in Ecology and Evolution},
keywords = {bmkADM_TopOfStats,effective communication,protocol,statistical analysis,visualization},
mendeley-tags = {bmkADM_TopOfStats},
month = {jun},
number = {6},
pages = {636--645},
title = {{A protocol for conducting and presenting results of regression-type analyses}},
url = {http://doi.wiley.com/10.1111/2041-210X.12577},
volume = {7},
year = {2016}
}

@book{Gelman2007b,
abstract = {Data Analysis Using Regression and Multilevel/Hierarchical Models is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout.},
author = {Gelman, Andrew. and Hill, Jennifer},
file = {:C\:/Users/hans_vancalster/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelman, Hill - 2007 - Data analysis using regression and multilevelhierarchical models.pdf:pdf},
isbn = {9780521867061 0521867061 9780521686891 052168689},
keywords = {bmkADM_BibVanBMK,bmkADM_TopOfStats,bmkANA_MixedModels},
mendeley-tags = {bmkADM_BibVanBMK,bmkADM_TopOfStats,bmkANA_MixedModels},
pages = {xxii, 625 p. :},
publisher = {Cambridge University Press, Cambridge},
title = {{Data analysis using regression and multilevel/hierarchical models}},
type = {Book},
url = {http://www.loc.gov/catdir/enhancements/fy0668/2006040566-t.html},
year = {2007}
}

@book{Lindenmayer2010d,
abstract = {Long-term monitoring programs are fundamental to understanding the natural environment and effectively tackling major environmental problems. Yet they are often done very poorly and ineffectively. Effective Ecological Monitoring describes what makes successful and unsuccessful long-term monitoring programs. Short and to the point, it illustrates key aspects with case studies and examples. It is based on the collective experience of running long-term research and monitoring programs of the two authors -- experience which spans more than 70 years. The book first outlines why long-term monitoring is important, then discusses why long-term monitoring programs often fail. The authors then highlight what makes good and effective monitoring. These good and bad aspects of long-term monitoring programs are further illustrated in the fourth chapter of the book. The final chapter sums up the future of long-term monitoring programs and how to make them better, more effective and better targeted.},
author = {Lindenmayer, David and Likens, Gene E},
file = {:C\:/Users/hans_vancalster/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lindenmayer, Likens - 2010 - Effective ecological monitoring.pdf:pdf},
keywords = {bmkADM_TopOfStats,bmkCTX_BeleidsgerichteMeetnetten},
mendeley-tags = {bmkADM_TopOfStats,bmkCTX_BeleidsgerichteMeetnetten},
publisher = {Earthscan, London, UK},
title = {{Effective ecological monitoring}},
year = {2010}
}

@book{Bolker2008a,
abstract = {Ecological Models and Data in R is the first truly practical introduction to modern statistical methods for ecology. In step-by-step detail, the book teaches ecology graduate students and researchers everything they need to know in order to use maximum likelihood, information-theoretic, and Bayesian techniques to analyze their own data using the programming language R. Drawing on extensive experience teaching these techniques to graduate students in ecology, Benjamin Bolker shows how to choose among and construct statistical models for data, estimate their parameters and confidence limits, and interpret the results. The book also covers statistical frameworks, the philosophy of statistical modeling, and critical mathematical functions and probability distributions. It requires no programming background--only basic calculus and statistics.

    - Practical, beginner-friendly introduction to modern statistical techniques for ecology using the programming language R
    - Step-by-step instructions for fitting models to messy, real-world data
    - Balanced view of different statistical approaches
    - Wide coverage of techniques -- from simple (distribution fitting) to complex (state-space modeling)
    - Techniques for data manipulation and graphical display
    - Companion Web site with data and R code for all examples},
address = {Princeton, NJ},
isbn = {0691125228},
author = {Bolker, Benjamin M},
file = {:C\:/Users/hans_vancalster/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bolker - 2008 - Ecological Models and Data in R.pdf:pdf},
keywords = {bmkADM_TopOfStats,bmkANA_MixedModels},
mendeley-tags = {bmkADM_TopOfStats,bmkANA_MixedModels},
publisher = {Princeton University Press},
title = {{Ecological Models and Data in R}},
year = {2008}
}

@book{bryan_happy_2019,
	title = {Happy {Git} and {GitHub} for the {useR}},
	url = {https://happygitwithr.com/},
	abstract = {Using Git and GitHub with R, Rstudio, and R Markdown},
	urldate = {2019-01-25},
	author = {Bryan, Jenny and {the STAT 545 TAs} and Hester, Jim},
	year = {2019},
	keywords = {CAT\_ICT\_SoftwareAndProgramming, THEORY, bmkSOF\_Andere, bmkSOF\_Programming, bmkSOF\_R, Floris\_EBOOK},
	annote = {Getting started with git and github workflows in RStudio}
}


@book{lovelace_geocomputation_2019,
  title = {Geocomputation with {{R}}},
  url = {https://geocompr.robinlovelace.net},
  author = {Lovelace, Robin and Nowosad, Jakub and Muenchow, Jannes},
  year = {2019}
}


@book{pebesma_edzer_spatial_2019,
  title = {Spatial {{Data Science}}},
  url = {https://www.r-spatial.org/book},
  author = {Pebesma, Edzer and Bivand, Roger},
  year = {2019}
}


@book{heijmans_spatial_2019,
	title = {Spatial {Data} {Science} with {R}},
	url = {https://rspatial.org/},
	urldate = {2019-11-20},
	author = {Hijmans, Robert},
	year = {2019}
}
